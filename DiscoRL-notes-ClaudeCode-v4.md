# DiscoRL Algorithm Notes

**Version: DiscoRL-notes-ClaudeCode-v4**  

**Notes:**  this doc is generated by ClaudeCode with Opus 4.5, 20260112. second version.

DiscoRL (Discovering Reinforcement Learning) 
is a meta-learning algorithm that autonomously discovers RL update rules   
by training a Meta-Network (parameterized by η) to produce targets (π̂, ŷ, ẑ)   
that guide an Agent Network (parameterized by θ) to maximize rewards across diverse environments.   

The agent produces five outputs—policy π, observation-conditioned prediction y, action-conditioned prediction z, Q-values q, and auxiliary policy p—and trains by minimizing KL-divergence between its predictions and the meta-network's targets.   
  
The meta-network uses a backward LSTM for n-step bootstrapping and a Meta-RNN that adapts across agent updates, receiving rich inputs including rewards, advantages, and prediction embeddings.   
  
Meta-optimization computes gradients by backpropagating through 20 agent update steps, enabling the discovery of novel learning semantics (y, z) that emerge automatically rather than being hand-designed like traditional value functions.   
  
The discovered rule (Disco-103)
  achieves state-of-the-art performance on Atari (IQM 13.86) and generalizes to unseen benchmarks without environment-specific tuning.


## Overview

**DiscoRL** (Discovering Reinforcement Learning) is a method for autonomously discovering RL rules through meta-learning from the cumulative experiences of a population of agents across diverse environments. The discovered rule achieves state-of-the-art performance on Atari and generalizes to unseen benchmarks.

**Paper**: "Discovering state-of-the-art reinforcement learning algorithms", Nature, Vol 648, 11 December 2025

**Authors**: Junhyuk Oh, Gregory Farquhar, Iurii Kemaev, Dan A. Calian, Matteo Hessel, Luisa Zintgraf, Satinder Singh, Hado van Hasselt, David Silver

---

## Question 1: What is the neural network architecture of the 'Agent-network'? Where is the Neural Network of the 'Agent-network' defined? How is it defined?

### Answer

The **Agent Network** (parameterized by θ) produces five types of outputs:  
1. **Policy (π)** - probability distribution over actions.  
2. **Observation-conditioned prediction (y)** - meta-learned semantics.  
3. **Action-conditioned prediction (z)** - meta-learned semantics.  
4. **Action-value function (q)** - pre-defined semantics (categorical, 601 bins).  
5. **Auxiliary policy prediction (p)** - pre-defined semantics.  

### Architecture Definition

The Agent Network is defined in the following files:

#### 1. Main Network Factory (`disco_rl/networks/nets.py:32-56`)
```python
def get_network(name: str, *args, **kwargs) -> types.PolicyNetwork:
    """Constructs a network."""
    def _get_net():
        if name == 'mlp':
            return MLP(*args, **kwargs)
```

#### 2. MLP Network Class (`disco_rl/networks/nets.py:127-158`)
```python
class MLP(MLPHeadNet):
    """Simple MLP network."""
    def __init__(self, out_spec, dense, action_spec, head_w_init_std,
                 model_out_spec, model_arch_name, model_kwargs, module_name):
        # dense=(512, 512) - torso MLP hidden sizes
```

The `_embedding_pass` method (`disco_rl/networks/nets.py:152-158`):
```python
def _embedding_pass(self, inputs, should_reset=None):
    inputs = [hk.Flatten()(x) for x in jax.tree_util.tree_leaves(inputs)]
    inputs = jnp.concatenate(inputs, axis=-1)
    return hk.nets.MLP(self._dense, name='torso')(inputs)
```

#### 3. Head Network (`disco_rl/networks/nets.py:59-124`)

The `MLPHeadNet` class defines the output heads:  
- **Flat outputs** (π, y): Linear heads from embedding (`disco_rl/networks/nets.py:94-107`).  
- **Action-conditional outputs** (z, q, p): Via action model (`disco_rl/networks/nets.py:120-124`).  

```python
def __call__(self, inputs, should_reset=None):
    torso = self._embedding_pass(inputs)
    out = self._head_pass(torso)  # Produces pi, y
    if self._model:
        root = self._model.root_embedding(torso)
        model_out = self._model.model_step(root)  # Produces z, q, aux_pi
        out.update(model_out)
    return out
```

#### 4. LSTM Action Model (`disco_rl/networks/action_models.py:36-104`)

The action-conditional model uses an LSTM inspired by MuZero/Muesli:

```python
class LSTMModel:
    """LSTM-based action-conditional model inspired by Muesli/MuZero."""

    def __init__(self, action_spec, out_spec, head_mlp_hiddens, lstm_size):
        self._lstm_size = lstm_size  # 128
        self._head_mlp_hiddens = head_mlp_hiddens  # (128,)
```

The `model_step` method (`disco_rl/networks/action_models.py:94-98`) computes outputs for all actions:
```python
def model_step(self, embedding: hk.LSTMState) -> dict[str, chex.Array]:
    transition_output = self._model_transition_all_actions(embedding)
    model_outputs = self._model_head_pass(transition_output)
    return model_outputs
```

### Output Specifications

Defined in `disco_rl/update_rules/disco.py:95-110`:

```python
def flat_output_spec(self, single_action_spec):
    return dict(
        logits=utils.get_logits_specs(single_action_spec),  # [A]
        y=types.ArraySpec((self._prediction_size,), jnp.float32),  # [600]
    )

def model_output_spec(self, single_action_spec):
    return dict(
        z=types.ArraySpec((self._prediction_size,), jnp.float32),  # [600]
        aux_pi=utils.get_logits_specs(single_action_spec),  # [A]
        q=types.ArraySpec((self._num_bins,), jnp.float32),  # [601]
    )
```

### Default Configuration (`disco_rl/agent.py:319-378`)

```python
net_settings=dict(
    name='mlp',
    net_args=dict(
        dense=(512, 512),
        model_arch_name='lstm',
        head_w_init_std=1e-2,
        model_kwargs=dict(
            head_mlp_hiddens=(128,),
            lstm_size=128,
        ),
    ),
)
```

---

## Question 2: How to train the Agent-network?

### Answer

The Agent Network is trained to minimize the distance between its predictions and targets produced by the Meta-Network.

### Training Process

#### 1. Training Loop (`disco_rl/agent.py:248-316`)

The `learner_step` method performs one training step:

```python
def learner_step(self, rng, rollout, learner_state, agent_net_state,
                 update_rule_params, is_meta_training):
    # Step 1: Unroll agent to get current predictions
    agent_out, _ = self.unroll_net(learner_state.params, agent_net_state, rollout)

    # Step 2: Apply meta-network to get targets
    meta_out, new_meta_state = self.update_rule.unroll_meta_net(
        meta_params=update_rule_params,
        params=learner_state.params,
        state=agent_net_state,
        meta_state=learner_state.meta_state,
        rollout=eta_inputs,
        hyper_params=self.settings.hyper_params.to_dict(),
        unroll_policy_fn=self._network.unroll,
        rng=rng,
        axis_name=self._batch_axis_name,
    )

    # Step 3: Compute gradient of loss
    dloss_dparams = jax.grad(self._loss, has_aux=True)
    grads, (_, last_agent_net_state, logging_dict) = dloss_dparams(
        learner_state.params,
        agent_net_state=agent_net_state,
        meta_state=learner_state.meta_state,
        rollout=rollout,
        meta_out=meta_out,
        is_meta_training=is_meta_training,
    )

    # Step 4: Average gradients across devices
    if self._batch_axis_name is not None:
        grads = jax.lax.pmean(grads, axis_name=self._batch_axis_name)

    # Step 5: Apply optimizer update
    updates, new_opt_state = self._optimizer.update(
        grads, learner_state.opt_state, learner_state.params
    )
    new_params = optax.apply_updates(learner_state.params, updates)
```

#### 2. Optimizer Configuration (`disco_rl/agent.py:101-106`)

```python
self._optimizer = optax.chain(
    optimizers.scale_by_adam_sg_denom(),  # Adam with stop-gradient on denominator
    optax.clip(max_delta=self.settings.max_abs_update),  # Gradient clipping (1.0)
    optax.scale(-self.settings.learning_rate),  # Learning rate (0.0003)
)
```

#### 3. Target Parameter Updates (`disco_rl/update_rules/disco.py:200-206`)

The target network parameters are updated via exponential moving average:

```python
coeff = hyper_params['target_params_coeff']  # 0.9
new_meta_state['target_params'] = jax.tree.map(
    lambda old, new: old * coeff + (1.0 - coeff) * new,
    meta_state['target_params'],
    params,
)
```

---

## Question 3: How to produce the predictions (π, y, z) from Agent-network? What are the inputs? What are the outputs? How to transform observations?

### Answer

### Inputs for Agent Producing Predictions

The primary input is the **observation** from the environment. The observation is processed through:

1. **Flattening**: Each observation component is flattened (`disco_rl/networks/nets.py:156-157`)
2. **Concatenation**: All flattened components are concatenated
3. **MLP Encoding**: Passed through MLP torso

```python
def _embedding_pass(self, inputs, should_reset=None):
    inputs = [hk.Flatten()(x) for x in jax.tree_util.tree_leaves(inputs)]
    inputs = jnp.concatenate(inputs, axis=-1)
    return hk.nets.MLP(self._dense, name='torso')(inputs)
```

### Output Production

#### Policy (π) - `disco_rl/networks/nets.py:94-107`
```python
def _head_pass(self, embedding):
    embedding = hk.Flatten()(embedding)
    def _infer(spec):
        output = hk.nets.MLP(
            output_sizes=(np.prod(spec.shape),),
            w_init=self._head_w_init,
            name='torso_head',
        )(embedding)
        return output.reshape((embedding.shape[0], *spec.shape))
    return jax.tree.map(_infer, self._out_spec)  # Includes 'logits' for π
```

#### Observation-conditioned prediction (y) - Same as above
- Shape: `[B, 600]` (prediction_size)
- Linear head from embedding

#### Action-conditioned prediction (z) - `disco_rl/networks/action_models.py:51-73`

```python
def _model_transition_all_actions(self, embedding):
    num_actions = utils.get_num_actions_from_spec(self._action_spec)
    batch_size = embedding.cell.shape[0]

    # Create one-hot actions for all actions
    one_hot_actions = jnp.eye(num_actions)  # [A, A]
    batched_one_hot_actions = jnp.tile(one_hot_actions, [batch_size, 1])  # [BA, A]

    # Repeat embedding for each action
    all_actions_embed = jax.tree.map(
        lambda x: jnp.repeat(x, repeats=num_actions, axis=0), embedding
    )  # [BA, H]

    # LSTM transition
    lstm_output, _ = hk.LSTM(self._lstm_size, name='action_cond')(
        batched_one_hot_actions, all_actions_embed
    )
    return lstm_output
```

### Outputs Summary

| Output | Shape | Description |
|--------|-------|-------------|
| π (logits) | [B, A] | Policy logits over actions |
| y | [B, 600] | Observation-conditioned prediction |
| z | [B, A, 600] | Action-conditioned prediction |
| q | [B, A, 601] | Categorical action-value |
| aux_pi | [B, A, A] | Auxiliary policy prediction |

### Observation Transformation

During actor step (`disco_rl/agent.py:147-173`):
```python
def actor_step(self, actor_params, rng, timestep, actor_state):
    should_reset = timestep.step_type == dm_env.StepType.LAST
    agent_outs, next_actor_state = self._network.one_step(
        actor_params, actor_state, timestep.observation, should_reset
    )
    actions = distrax.Softmax(logits=agent_outs['logits']).sample(seed=rng)
```

---

## Question 4: How to calculate loss between Agent rollouts and targets from Meta-network?

### Answer

The loss is computed in `disco_rl/update_rules/disco.py:210-294`.

### Loss Function

The total agent loss is:

$$L(\theta) = \lambda_\pi D_{KL}(\hat{\pi} || \pi) + \lambda_y D_{KL}(\hat{y} || y) + \lambda_z D_{KL}(\hat{z} || z_a) + \lambda_p L_{aux} + \lambda_q L_{value}$$

### Implementation

```python
def agent_loss(self, rollout, meta_out, hyper_params, backprop):
    # Parse agent's output (drop last timestep)
    agent_out, actions = jax.tree.map(lambda x: x[:-1], (rollout.agent_out, rollout.actions))
    logits = agent_out['logits']  # [T, B, A]
    y = agent_out['y']            # [T, B, Y]
    z = agent_out['z']
    z_a = utils.batch_lookup(agent_out['z'], actions)  # [T, B, Z]

    # Parse meta-net's output (targets)
    pi_hat = meta_out['pi']
    y_hat = meta_out['y']
    z_hat = meta_out['z']

    if not backprop:
        pi_hat, y_hat, z_hat = jax.lax.stop_gradient((pi_hat, y_hat, z_hat))

    # KL-divergence losses (using rlax.categorical_kl_divergence)
    pi_loss_per_step = rlax.categorical_kl_divergence(pi_hat, logits)
    y_loss_per_step = rlax.categorical_kl_divergence(y_hat, y)
    z_loss_per_step = rlax.categorical_kl_divergence(z_hat, z_a)

    # Auxiliary policy prediction loss
    aux_pi = rollout.agent_out['aux_pi'][:-1]  # [T, B, A, A]
    aux_pi_a = utils.batch_lookup(aux_pi, actions)  # [T, B, A]
    aux_policy_target = rollout.agent_out['logits'][1:]  # [T, B, A]
    aux_policy_loss_per_step = rlax.categorical_kl_divergence(
        jax.lax.stop_gradient(aux_policy_target), aux_pi_a
    )
    aux_policy_loss_per_step *= 1.0 - rollout.is_terminal  # Mask terminals

    # Total loss with hyperparameter weights
    total_loss_per_step = (
        hyper_params['pi_cost'] * pi_loss_per_step +      # 1.0
        hyper_params['y_cost'] * y_loss_per_step +        # 1.0
        hyper_params['z_cost'] * z_loss_per_step +        # 1.0
        hyper_params['aux_policy_cost'] * aux_policy_loss_per_step  # 1.0
    )

    return total_loss_per_step, log
```

### Value Loss (No Meta-gradient) - `disco_rl/update_rules/disco.py:296-323`

```python
def agent_loss_no_meta(self, rollout, meta_out, hyper_params):
    td = meta_out['q_td']
    q_a = utils.batch_lookup(rollout.agent_out['q'], rollout.actions)[:-1]

    value_loss_per_step = value_utils.value_loss_from_td(
        value_net_out=q_a,
        td=jax.lax.stop_gradient(td),
        nonlinear_transform=True,
        categorical_value=True,
        max_abs_value=self._max_abs_value,
    )
    loss_per_step = value_loss_per_step * hyper_params['value_cost']  # 0.2

    return loss_per_step, log
```

---

## Question 5: How to calculate gradients of Agent-network?

### Answer

Gradients are calculated using JAX's automatic differentiation in `disco_rl/agent.py:288-297`.

### Gradient Calculation

```python
def learner_step(self, rng, rollout, learner_state, agent_net_state,
                 update_rule_params, is_meta_training):
    # ...

    # Get gradient of the loss function
    dloss_dparams = jax.grad(self._loss, has_aux=True)
    grads, (_, last_agent_net_state, logging_dict) = dloss_dparams(
        learner_state.params,
        agent_net_state=agent_net_state,
        meta_state=learner_state.meta_state,
        rollout=rollout,
        meta_out=meta_out,
        is_meta_training=is_meta_training,
    )

    # Average gradients across devices (if distributed)
    if self._batch_axis_name is not None:
        grads = jax.lax.pmean(grads, axis_name=self._batch_axis_name)
```

### Loss Function for Gradient (`disco_rl/agent.py:200-246`)

```python
def _loss(self, agent_net_params, agent_net_state, meta_state, rollout,
          meta_out, is_meta_training):
    # Extract rewards and masks
    reward = rollout.rewards[1:]
    masks = rollout.discounts[:-1] > 0

    # Unroll the network
    agent_out, new_agent_net_state = self.unroll_net(
        agent_net_params, agent_net_state, rollout
    )

    # Get losses from update rule
    loss_per_step, log = self.update_rule.agent_loss(
        eta_inputs, meta_out, hyper_params, backprop=is_meta_training
    )
    loss_per_step_no_meta, log_no_meta = self.update_rule.agent_loss_no_meta(
        eta_inputs, meta_out, hyper_params
    )

    # Combine and mask
    total_loss_per_step = loss_per_step + loss_per_step_no_meta
    total_loss = (total_loss_per_step * masks).sum() / (masks.sum() + 1e-8)

    return total_loss, (meta_state, new_agent_net_state, log_dict)
```

### Key Points

1. **`jax.grad`**: Computes gradient with respect to first argument (agent_net_params)
2. **`has_aux=True`**: Returns auxiliary outputs along with gradients
3. **`jax.lax.pmean`**: Averages gradients across distributed devices
4. **`is_meta_training`**: Controls whether meta-gradient flows through targets

---

## Question 6: How to update the parameters of Agent-network?

### Answer

Parameters are updated using the Adam optimizer with gradient clipping in `disco_rl/agent.py:301-306`.

### Parameter Update Process

```python
def learner_step(self, ...):
    # ... (gradient computation)

    # Use the optimizer to apply gradient transformation
    updates, new_opt_state = self._optimizer.update(
        grads, learner_state.opt_state, learner_state.params
    )

    # Update parameters
    new_params = optax.apply_updates(learner_state.params, updates)

    # Return updated learner state
    learner_state = LearnerState(
        params=new_params,
        opt_state=new_opt_state,
        meta_state=new_meta_state
    )
    return learner_state, last_agent_net_state, logging_dict
```

### Optimizer Definition (`disco_rl/agent.py:101-106`)

```python
self._optimizer = optax.chain(
    optimizers.scale_by_adam_sg_denom(),  # Adam with stop-gradient on 2nd moment
    optax.clip(max_delta=self.settings.max_abs_update),  # Clip to 1.0
    optax.scale(-self.settings.learning_rate),  # Scale by -0.0003
)
```

### Custom Adam Implementation (`disco_rl/optimizers.py`)

The `scale_by_adam_sg_denom` function implements Adam with stop-gradient on the second moment statistics for meta-optimization stability (as mentioned in the paper's supplementary material).

### Update Rule

The standard Adam update:
$$\theta_{t+1} = \theta_t - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

Where:
- $\hat{m}_t$ = bias-corrected first moment estimate
- $\hat{v}_t$ = bias-corrected second moment estimate (with stop_gradient in meta-training)
- $\alpha$ = learning rate (0.0003)
- Gradient clipping applied before scaling

---

## Question 7: What is the neural network architecture of the 'Meta-network'? Where is it defined? How is it defined?

### Answer

The **Meta-Network** (parameterized by η) produces targets towards which the agent updates its predictions.

### Architecture Overview

The Meta-Network consists of:  
1. **Input embedding networks** - process agent outputs and environment signals.  
2. **Backward LSTM** - processes trajectory in reverse for bootstrapping.  
3. **Meta-RNN** - processes across agent updates for lifetime adaptation.  
4. **Output decoders** - produce targets (π̂, ŷ, ẑ)

### Definition Location

Main class: `disco_rl/networks/meta_nets.py:45-158`

```python
class LSTM(MetaNet):
    """Meta network with LSTMs."""

    def __init__(
        self,
        hidden_size: int,                    # 256
        embedding_size: Sequence[int],       # (16, 1)
        prediction_size: int,                # 600
        meta_rnn_kwargs: Mapping[str, Any],  # Meta-RNN config
        input_option: types.MetaNetInputOption,
        policy_channels: Sequence[int] = (16, 2),
        policy_target_channels: Sequence[int] = (16,),
        # ... stddev initializers
    ):
        self._hidden_size = hidden_size
        self._embedding_size = embedding_size
        self._prediction_size = prediction_size
        self._meta_rnn_core = MetaLSTM(**meta_rnn_kwargs, input_option=input_option)
```

### Forward Pass (`disco_rl/networks/meta_nets.py:78-157`)

```python
def __call__(self, inputs: types.UpdateRuleInputs, axis_name: str | None):
    # Get dimensions
    _, batch_size, num_actions = inputs.agent_out['logits'].shape

    # Step 1: Construct input embeddings
    y_net = _batch_mlp(self._embedding_size, num_dims=2)
    z_net = _batch_mlp(self._embedding_size, num_dims=3)
    policy_net = _conv1d_net(self._policy_channels)
    x, policy_emb = _construct_input(inputs, y_net, z_net, policy_net, ...)

    # Step 2: Backward LSTM for bootstrapping
    per_trajectory_rnn_core = hk.ResetCore(hk.LSTM(self._hidden_size))
    should_reset_bwd = inputs.should_reset_mask_bwd[:-1]
    x, _ = hk.dynamic_unroll(
        per_trajectory_rnn_core,
        (x, should_reset_bwd),
        per_trajectory_rnn_core.initial_state(batch_size=batch_size),
        reverse=True  # BACKWARD unrolling
    )

    # Step 3: Multiplicative interaction with meta-RNN
    x = _multiplicative_interaction(
        x=x,
        y=self._meta_rnn_core.output(meta_rnn_state),
        initializer=self._state_init,
    )

    # Step 4: Compute targets
    y_hat = hk.BatchApply(hk.Linear(self._prediction_size))(x)
    z_hat = hk.BatchApply(hk.Linear(self._prediction_size))(x)

    # Step 5: Policy target with action-conditional processing
    w = jnp.repeat(jnp.expand_dims(x, 2), num_actions, axis=2)
    w = jnp.concatenate([w, policy_emb], axis=-1)
    w = _conv1d_net(self._policy_target_channels)(w)
    pi_hat = jnp.squeeze(hk.BatchApply(hk.Linear(1))(w), -1)

    # Step 6: Update meta-RNN state
    new_meta_rnn_state = self._meta_rnn_core.unroll(inputs, meta_out, meta_rnn_state)

    return dict(pi=pi_hat, y=y_hat, z=z_hat), new_rnn_state
```

### Meta-RNN (`disco_rl/networks/meta_nets.py:160-227`)

```python
class MetaLSTM(hk.Module):
    """Meta LSTM that processes trajectories throughout agent's lifetime."""

    def __init__(self, input_option, policy_channels, embedding_size,
                 pred_embedding_size, hidden_size):
        self._hidden_size = hidden_size  # 128
        self._core_constructor = lambda: hk.LSTM(self._hidden_size)

    def unroll(self, inputs, meta_out, state, axis_name):
        # Embed inputs
        x = jnp.concatenate(input_list, axis=-1)
        x = _batch_mlp(self._embedding_size)(x)

        # Average over batch-time dimensions
        x_avg = x.mean(axis=(0, 1))
        if axis_name is not None:
            x_avg = jax.lax.pmean(x_avg, axis_name=axis_name)

        # Single LSTM step
        _, new_state = self._core_constructor()(x_avg, state)
        return new_state
```

### Configuration (`disco_rl/agent.py:335-356`)

```python
update_rule=dict(
    net=config_dict.ConfigDict(dict(
        name='lstm',
        prediction_size=600,
        hidden_size=256,
        embedding_size=(16, 1),
        policy_target_channels=(16,),
        policy_channels=(16, 2),
        output_stddev=0.3,
        meta_rnn_kwargs=dict(
            policy_channels=(16, 2),
            embedding_size=(16,),
            pred_embedding_size=(16, 1),
            hidden_size=128,
        ),
    )),
)
```

---

## Question 8: How to train the Meta-network?

### Answer

The Meta-Network is trained through **meta-optimization** using meta-gradients that backpropagate through the agent's update process.

### Meta-Training Overview

From the paper (page 314):
> "Our goal is to discover an RL rule, represented by the meta-network with meta-parameters η, that allows agents to maximize rewards in a variety of training environments."

### Meta-Gradient Computation

The meta-gradient is:
$$\nabla_\eta J(\eta) \approx \mathbb{E}_{\mathcal{E}} \mathbb{E}_\theta [\nabla_\eta \theta \nabla_\theta J(\theta)]$$

This is computed by:  
1. Instantiating a population of agents.  
2. Having each agent learn according to the meta-network.  
3. Backpropagating through the entire update procedure.  

### Implementation in Agent Training

The key is the `is_meta_training` flag in `disco_rl/agent.py:248-256`:

```python
def learner_step(self, rng, rollout, learner_state, agent_net_state,
                 update_rule_params, is_meta_training):
    # ...

    # When is_meta_training=True, gradients flow through meta_out
    grads, (_, last_agent_net_state, logging_dict) = dloss_dparams(
        learner_state.params,
        agent_net_state=agent_net_state,
        meta_state=learner_state.meta_state,
        rollout=rollout,
        meta_out=meta_out,
        is_meta_training=is_meta_training,  # Controls gradient flow
    )
```

### In Agent Loss (`disco_rl/update_rules/disco.py:238-239`)

```python
def agent_loss(self, rollout, meta_out, hyper_params, backprop):
    # ...
    if not backprop:
        pi_hat, y_hat, z_hat = jax.lax.stop_gradient((pi_hat, y_hat, z_hat))
```

When `backprop=True` (meta-training), gradients flow through the targets, enabling the meta-gradient.

### Meta-Optimization Details (from paper)

1. **Sliding window**: Backpropagate over 20 agent updates.  
2. **Per-agent Adam normalization**: `η ← η + (1/n) Σ ADAM(gᵢ)`.  
3. **Meta-regularization losses**:  
   - Entropy regularization: Prevents predictions from converging prematurely.  
   - KL divergence regularization: Prevents excessively aggressive updates.  

### Hyperparameters for Meta-Training (from paper page 320)

- Meta learning rate: 0.001
- Gradient clipping: 1.0
- Batch size per agent: 96 trajectories × 29 timesteps
- Replay ratio: 90%
- Meta-gradient steps: 20 (sliding window)

---

## Question 9: How to produce the targets (π̂, ŷ, ẑ) from Meta-network? What are the inputs? What are the outputs?

### Answer

### Inputs to Meta-Network

The inputs are constructed in `disco_rl/update_rules/disco.py:326-428` and `disco_rl/networks/meta_nets.py:256-331`.

**Input Categories:**

1. **Policy inputs** (current, behavior, target):
   ```python
   types.TransformConfig(
       source='agent_out/logits',
       transforms=('drop_last', 'softmax', 'stop_grad', 'select_a'),
   )
   ```

2. **Rewards and termination**:
   ```python
   types.TransformConfig(source='rewards', transforms=('sign_log',))
   types.TransformConfig(source='is_terminal', transforms=('masks_to_discounts',))
   ```

3. **Value estimates**:
   ```python
   types.TransformConfig(
       source='extra_from_rule/v_scalar',
       transforms=('sign_log', 'td_pair', 'stop_grad'),
   )
   types.TransformConfig(source='extra_from_rule/adv', transforms=('sign_log', 'stop_grad'))
   types.TransformConfig(source='extra_from_rule/normalized_adv', transforms=('stop_grad',))
   ```

4. **Prediction embeddings (y)**:
   ```python
   types.TransformConfig(
       source='agent_out/y',
       transforms=('softmax', 'y_net', 'td_pair')
   )
   ```

5. **Action-conditioned predictions (z)**:
   ```python
   types.TransformConfig(
       source='agent_out/z',
       transforms=('drop_last', 'softmax', 'z_net', 'select_a'),
   )
   ```

### Input Construction (`disco_rl/networks/meta_nets.py:256-331`)

```python
def _construct_input(inputs, input_option, y_net, z_net, policy_net, axis_name):
    # Process base inputs (observation-conditioned)
    inputs_t = preprocess_from_config(
        inputs, input_option.base, prefix_shape=(unroll_len, batch_size)
    )

    # Process action-conditional inputs
    if input_option.action_conditional:
        act_cond_inputs = preprocess_from_config(...)
        act_cond_emb = policy_net(act_cond_inputs)  # [T, B, A, C]
        act_cond_emb_avg = jnp.mean(act_cond_emb, axis=2)  # [T, B, C]
        act_cond_emb_a = utils.batch_lookup(act_cond_emb, actions)  # [T, B, C]
        inputs_t += [act_cond_emb_avg, act_cond_emb_a]

    return jnp.concatenate(inputs_t, axis=-1), act_cond_emb
```

### Target Production Process

In `disco_rl/networks/meta_nets.py:78-157`:

```python
def __call__(self, inputs, axis_name):
    # 1. Construct input embeddings
    x, policy_emb = _construct_input(inputs, ...)  # [T, B, E]

    # 2. Backward LSTM (for bootstrapping from future)
    x, _ = hk.dynamic_unroll(
        per_trajectory_rnn_core, (x, should_reset_bwd),
        initial_state, reverse=True
    )  # [T, B, H]

    # 3. Multiplicative interaction with meta-RNN
    x = x * Linear(meta_rnn_output)  # [T, B, H]

    # 4. Compute y_hat and z_hat
    y_hat = BatchApply(Linear(self._prediction_size))(x)  # [T, B, Y]
    z_hat = BatchApply(Linear(self._prediction_size))(x)  # [T, B, Z]

    # 5. Compute pi_hat with action-conditional processing
    w = repeat(expand_dims(x, 2), num_actions, axis=2)  # [T, B, A, H]
    w = concatenate([w, policy_emb], axis=-1)           # [T, B, A, H+C]
    w = conv1d_net(w)                                   # [T, B, A, O]
    pi_hat = squeeze(BatchApply(Linear(1))(w), -1)      # [T, B, A]

    return dict(pi=pi_hat, y=y_hat, z=z_hat)
```

### Outputs

| Output | Shape | Description |
|--------|-------|-------------|
| π̂ (pi_hat) | [T, B, A] | Policy target (logits) |
| ŷ (y_hat) | [T, B, 600] | Observation-conditioned prediction target |
| ẑ (z_hat) | [T, B, 600] | Action-conditioned prediction target |

---

## Question 10: How to calculate loss of Meta-network?

### Answer

The Meta-Network doesn't have a direct loss function like supervised learning. Instead, it's optimized through **meta-gradients** that maximize the expected return of agents trained using its targets.

### Meta-Objective

From the paper (page 314):
$$J(\eta) = \mathbb{E}_{\mathcal{E}} \mathbb{E}_\theta [J(\theta)]$$

Where $J(\theta) = \mathbb{E}[\sum_t \gamma^t r_t]$ is the expected discounted return.

### Indirect Loss Through Agent Performance

The meta-network is evaluated by how well agents perform when trained using its targets:

1. **Agent trains using meta-network targets** (inner loop)
2. **Agent's return is measured** (meta-objective)
3. **Meta-gradient flows back through agent updates** to meta-network

### Meta-Regularization Losses (from paper page 320)

To stabilize meta-optimization, additional regularization is added:

```python
# Entropy regularization (prevents premature convergence of predictions)
L_ent(θ) = -E[H(y_θ(s)) + H(z_θ(s,a))]

# KL divergence regularization (prevents aggressive policy updates)
L_KL(θ) = D_KL(π_θ- || π̂)
```

Where:
- $H(·)$ is entropy
- $π_{θ^-}$ is the policy from target network (EMA params)
- $\hat{\pi}$ is the policy target from meta-network

### Effective "Loss" for Meta-Network

The effective objective being minimized is:
$$-J(\eta) + \lambda_{ent} L_{ent} + \lambda_{KL} L_{KL}$$

This is computed implicitly through backpropagation, not as an explicit loss function.

---

## Question 11: How to calculate gradient of Meta-network?

### Answer

The meta-gradient is computed by backpropagating through the agent's update procedure using the chain rule.

### Meta-Gradient Formula

From the paper (page 314):  
$$\nabla_\eta J(\eta) \approx \mathbb{E}_{\mathcal{E}} \mathbb{E}_\theta [\nabla_\eta \theta \nabla_\theta J(\theta)]$$

This decomposes into two terms:  
1. **$\nabla_\eta \theta$**: Gradient of agent parameters with respect to meta-parameters (through the update procedure).  
2. **$\nabla_\theta J(\theta)$**: Gradient of RL objective with respect to agent parameters (standard policy gradient).  

### Computation Process

The meta-gradient is computed by:

1. **Iteratively update agent N times** while tracking computation graph:
   ```
   θ₀ → θ₁ → θ₂ → ... → θ_N
   ```

2. **Backpropagate through entire update sequence**:
   ```
   ∇_η J = ∇_η θ_N · ∇_{θ_N} J(θ_N)
   ```

### Implementation Detail

In the agent training, when `is_meta_training=True`:

```python
# disco_rl/update_rules/disco.py:238-239
if not backprop:
    pi_hat, y_hat, z_hat = jax.lax.stop_gradient((pi_hat, y_hat, z_hat))
```

When `backprop=True`, the targets are **not** stopped, allowing gradients to flow:
- From agent loss → through targets → to meta-network parameters

### Sliding Window for Tractability

From the paper (page 314):
> "To make it tractable, we backpropagate over 20 agent updates using a sliding window."

This limits the computational graph to 20 update steps rather than the entire agent lifetime.

### Advantage Actor-Critic for $\nabla_\theta J(\theta)$

The second term $\nabla_\theta J(\theta)$ is estimated using advantage actor-critic:
- A **meta-value function** is trained to estimate the advantage
- This is used only for discovery, not for the agent itself

---

## Question 12: How to update the parameters of Meta-network?

### Answer

Meta-network parameters are updated using gradient ascent with the Adam optimizer.

### Update Rule

From the paper (page 314):
$$\eta \leftarrow \eta + \nabla_\eta J(\eta)$$

### Implementation Details (from paper page 320)

1. **Per-agent Adam normalization**:
   ```python
   η ← η + (1/n) Σᵢ ADAM(gᵢ)
   ```
   Where $g_i$ is the meta-gradient from the i-th agent. This helps balance gradient magnitudes across different environments.

2. **Optimizer settings**:
   - Learning rate: 0.001
   - Gradient clipping: 1.0

### Aggregation Across Agents

```python
# Pseudocode for meta-parameter update
meta_grads = []
for agent in population:
    agent_meta_grad = compute_meta_gradient(agent, meta_params)
    normalized_grad = adam_normalize(agent_meta_grad)
    meta_grads.append(normalized_grad)

# Average and update
mean_grad = mean(meta_grads)
meta_params = meta_params + learning_rate * mean_grad
```

### Meta-Regularization

Additional regularization terms are added to the objective:
```python
total_meta_grad = meta_grad - λ_ent * ∇L_ent - λ_KL * ∇L_KL
```

This prevents:
- Predictions from converging prematurely (entropy regularization)
- Excessively aggressive policy updates (KL regularization)

---

## Question 13: How to calculate 'Advantage estimates'?

### Answer

Advantage estimates are calculated using **Retrace** (for Q-values) or **V-trace** (for state values) in `disco_rl/value_fns/value_utils.py`.

### Retrace for Q-values (`disco_rl/value_fns/value_utils.py:323-421`)

```python
def estimate_q_values(rewards, actions, env_discounts, rho, values,
                      target_values, q_values, target_q_values, discount, lambda_):
    # Get Q-values for taken actions
    q_a = batch_lookup(target_q_values[:-1], actions)

    # Compute Retrace targets
    discounts = env_discounts * discount
    clipped_rho = jnp.minimum(rho, 1.0)
    c_t = lambda_ * clipped_rho

    q_target = batch_retrace_fn(q_a, target_values, rewards, discounts, c_t)

    # Advantage = Q_target - V
    adv = q_target - target_values[:-1]

    # Q-V advantage for all actions
    qv_adv = target_q_values - expand_dims(target_values, axis=2)

    return ValueOuts(adv=adv, qv_adv=qv_adv, ...)
```

### V-trace for State Values (`disco_rl/value_fns/value_utils.py:252-320`)

```python
def estimate_values(rewards, actions, env_discounts, rho, values,
                    target_values, discount, lambda_):
    # V-trace computation
    batch_vtrace_fn = jax.vmap(
        functools.partial(rlax.vtrace_td_error_and_advantage, lambda_=lambda_),
        in_axes=1, out_axes=1,
    )

    discounts = env_discounts * discount
    vtrace_return = batch_vtrace_fn(
        target_values[:-1], target_values[1:], rewards, discounts, rho
    )

    value_target = vtrace_return.errors + target_values[:-1]
    adv = vtrace_return.pg_advantage

    return ValueOuts(adv=adv, value_target=value_target, ...)
```

### Importance Weights (`disco_rl/value_fns/value_utils.py:495-509`)

```python
def importance_weight(pi_logits, mu_logits, actions):
    log_prob_fn = lambda t, a: distrax.Softmax(t).log_prob(a)
    log_pi_a = log_prob_fn(pi_logits, actions)
    log_mu_a = log_prob_fn(mu_logits, actions)
    rho = jax.lax.stop_gradient(jnp.exp(log_pi_a - log_mu_a))
    return rho
```

### Normalization (`disco_rl/value_fns/value_utils.py:202-248`)

Advantages are normalized using exponential moving averages:

```python
if adv_ema_state is not None and adv_ema_fn is not None:
    new_adv_ema_state = adv_ema_fn.update_state(advantages, adv_ema_state, axis_name)
    normalized_adv = adv_ema_fn.normalize(advantages, new_adv_ema_state)
```

The `MovingAverage` class (`disco_rl/utils.py:173-270`) tracks mean and variance:
```python
def normalize(self, value, state, subtract_mean=True):
    mean, variance = self._compute_moments(state)
    if subtract_mean:
        return (val - mean) / (sqrt(var + eps_root) + eps)
    else:
        return val / (sqrt(var + eps_root) + eps)
```

---

## Question 14: What is the purpose of 'Advantage estimates'? Where are they used? When are they used?

### Answer

### Purpose

Advantage estimates serve multiple purposes in DiscoRL:

1. **Policy gradient baseline**: Reduce variance in policy updates
2. **Meta-network input**: Provide learning signals to the meta-network
3. **Value function training**: Compute TD targets for Q-value updates
4. **Meta-gradient estimation**: Used in computing the meta-objective

### Where Advantages Are Used

#### 1. As Meta-Network Input (`disco_rl/update_rules/disco.py:353-358`)

```python
types.TransformConfig(
    source='extra_from_rule/adv', transforms=('sign_log', 'stop_grad')
)
types.TransformConfig(
    source='extra_from_rule/normalized_adv', transforms=('stop_grad',)
)
```

The meta-network sees:
- Raw advantage (sign-log transformed)
- Normalized advantage

This allows the meta-network to learn how to use advantage information to construct appropriate targets.

#### 2. Computing Value Outputs (`disco_rl/update_rules/disco.py:161-170`)

```python
rollout.extra_from_rule = dict(
    v_scalar=value_outs.value,
    adv=value_outs.adv,
    normalized_adv=value_outs.normalized_adv,
    q=value_outs.target_q_value,
    qv_adv=value_outs.qv_adv,
    normalized_qv_adv=value_outs.normalized_qv_adv,
    target_out=target_out,
)
```

#### 3. Meta-Network Output Enrichment (`disco_rl/update_rules/disco.py:183-191`)

```python
meta_out['adv'] = value_outs.adv
meta_out['normalized_adv'] = value_outs.normalized_adv
meta_out['qv_adv'] = value_outs.qv_adv
meta_out['normalized_qv_adv'] = value_outs.normalized_qv_adv
```

#### 4. Meta-Optimization (from paper)

In the meta-gradient computation, advantage actor-critic is used:
> "To estimate the second term, we use the advantage actor–critic method. To estimate the advantage, we train a meta-value function, which is a value function used only for discovery."

### When Advantages Are Computed

1. **Every agent update**: `unroll_meta_net` computes advantages from the rollout (`disco_rl/update_rules/disco.py:141-159`)

2. **Before meta-network forward pass**: Advantages must be computed first as they're inputs to the meta-network

3. **For meta-gradient estimation**: A separate meta-value function estimates advantages for the meta-objective

---

## Question 15: What is the function of the 'Value Function' in file value_fn.py? Why is a value function necessary? Why does the value function change? When the value function changes, what exactly is being modified?

### Answer

### Function of Value Function (`disco_rl/value_fns/value_fn.py`)

The `ValueFunction` class is a **domain value function approximator used only in meta-training**. It serves as the **meta-value function** for estimating advantages in the meta-gradient computation.

From `disco_rl/value_fns/value_fn.py:31-40`:
```python
class ValueFunction:
    """Domain value function approximator.

    Used only in meta-training.
    """
```

### Why a Value Function is Necessary

1. **Meta-gradient estimation**: The meta-objective $J(\eta)$ requires estimating $\nabla_\theta J(\theta)$. This is done using advantage actor-critic, which needs value estimates.

2. **Variance reduction**: Using a baseline (value function) reduces variance in policy gradient estimates.

3. **Off-policy correction**: The value function enables V-trace corrections for off-policy data.

From the paper (page 314):
> "To estimate the second term, we use the advantage actor–critic method. To estimate the advantage, we train a meta-value function, which is a value function used only for discovery."

### Why the Value Function Changes

The value function changes because:

1. **Agent policies evolve**: As agents learn, their policies change, so the value of states changes
2. **Environment distribution**: Different environments have different value scales
3. **Tracking true values**: The value function must track the changing expected returns

### What Gets Modified

When the value function is updated (`disco_rl/value_fns/value_fn.py:141-198`):

```python
def update(self, value_state, rollout, target_logits):
    def value_loss_fn(v_params, value_state, rollout, target_logits):
        # Forward pass
        value_outs, net_out, adv_ema_state, td_ema_state = self.get_value_outs(...)

        # Loss: minimize TD error
        value_losses = value_utils.value_loss_from_td(
            net_out[:-1], jax.lax.stop_gradient(value_outs.normalized_td)
        )
        value_loss = (self._outer_value_cost * value_losses).mean()
        return value_loss, (value_outs, adv_ema_state, td_ema_state)

    # Compute gradients
    (value_loss, aux), dv_dparams = jax.value_and_grad(value_loss_fn, has_aux=True)(
        value_state.params, value_state, rollout, target_logits
    )

    # Update parameters
    update, new_opt_state = self._value_opt.update(dv_dparams, value_state.opt_state, ...)
    new_params = optax.apply_updates(value_state.params, update)

    new_state = types.ValueState(
        params=new_params,           # Network weights
        state=value_state.state,     # Haiku state (if any)
        opt_state=new_opt_state,     # Optimizer state (Adam moments)
        adv_ema_state=adv_ema_state, # EMA for advantage normalization
        td_ema_state=td_ema_state,   # EMA for TD normalization
    )
```

### Components Being Modified

1. **`params`**: Neural network weights for value prediction
2. **`opt_state`**: Adam optimizer state (first and second moments)
3. **`adv_ema_state`**: Exponential moving average for advantage normalization
4. **`td_ema_state`**: Exponential moving average for TD normalization

### Value State Structure (`disco_rl/types.py:86-93`)

```python
@chex.dataclass
class ValueState:
    params: AgentParams      # Network parameters
    state: HaikuState        # Haiku state (usually empty for MLP)
    opt_state: OptState      # Optimizer state
    adv_ema_state: EmaState  # Advantage normalization statistics
    td_ema_state: EmaState   # TD normalization statistics
```

### Value Function Configuration (`disco_rl/value_fns/value_fn.py:44-75`)

```python
def __init__(self, config, axis_name):
    self._discount_factor = config.discount_factor
    self._td_lambda = config.td_lambda
    self._outer_value_cost = config.outer_value_cost

    # Build value function network
    self._value_fn = nets.get_network(
        config.net,
        out_spec={'value': types.ArraySpec([1], jnp.float32)},
        ...
    )

    # Build optimizer
    self._value_opt = optax.chain(
        optimizers.scale_by_adam_sg_denom(),
        optax.clip(max_delta=config.max_abs_update),
        optax.scale(-config.learning_rate),
    )

    # Build EMA trackers
    self._adv_ema = utils.MovingAverage(jnp.zeros(()), decay=config.ema_decay)
    self._td_ema = utils.MovingAverage(jnp.zeros(()), decay=config.ema_decay)
```

---

## Summary

DiscoRL represents a significant advance in meta-learning for RL by:

1. **Meta-learning targets** instead of loss functions (more expressive, includes semi-gradients)
2. **Discovering new prediction semantics** (y, z) beyond pre-defined concepts like value functions
3. **Using backward LSTM** for n-step bootstrapping
4. **Meta-RNN** for adaptation across agent updates
5. **Large-scale discovery** from complex environments (Atari, ProcGen, DMLab)

The discovered rule (Disco103) achieves state-of-the-art performance on Atari (IQM 13.86) and generalizes to unseen benchmarks without any environment-specific tuning.
