

--- FILE: ./github_DiscoRL/.DS_Store ---



--- FILE: ./github_DiscoRL/LICENSE ---


                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.


--- FILE: ./github_DiscoRL/pyproject.toml ---

[build-system]
requires = ["setuptools>=77.0"]
build-backend = "setuptools.build_meta"

[project]
name = "disco_rl"
version = "1.0.0"
description = "DiscoRL: Discovering State-of-the-art Reinforcement Learning Algorithms."
readme = "README.md"
requires-python = ">=3.11"
license-files = ["LICENSE"]
authors = [{name = "DiscoRL team", email="disco_rl@google.com"}]
classifiers = [
    "Programming Language :: Python :: 3",
    "Intended Audience :: Science/Research",
]
keywords = [
  "reinforcement learning",
  "rl",
  "meta learning",
  "meta",
  "update rule",
  "automated discovery",
  "automl",
  "algorithm discovery",
  "data driven research",
]

# pip dependencies of the project
# Installed locally with `pip install -e .`
dependencies = [
  "jax",
  "distrax",
  "optax",
  "jmp",
  "rlax",
  "dm-haiku",
  "immutabledict",
  "ml_collections",
  "matplotlib",
  "seaborn",
  "pandas",
  "tqdm"
]

[project.urls]
repository = "https://github.com/google-deepmind/disco_rl"

[tool.setuptools.packages.find]
where = ["."]  # list of folders that contain the packages (["."] by default)
include = ["*"]  # package names should match these glob patterns (["*"] by default)


--- FILE: ./github_DiscoRL/README.md ---

# DiscoRL: Discovering State-of-the-art Reinforcement Learning Algorithms

This repository contains accompanying code for the *"Discovering
 State-of-the-art Reinforcement Learning Algorithms"* Nature publication.

It provides a minimal JAX harness for the DiscoRL setup together with the
 original meta-learned weights for the *Disco103* discovered update rule.

The harness supports both:

-   **Meta-evaluation**: training an agent using the *Disco103* discovered RL
    update rule, using the `colabs/eval.ipynb` notebook [![Open In](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/disco_rl/blob/master/colabs/eval.ipynb) and

-   **Meta-training**: meta-learning a RL update rule from scratch or from a
    pre-existing checkpoint, using the `colabs/meta_train.ipynb` notebook [![Open In](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/disco_rl/blob/master/colabs/meta_train.ipynb)

Note that it will not be actively maintained moving forward.

## Installation

Set up a Python virtual environment and install the package:

```bash
python3 -m venv disco_rl_venv
source disco_rl_venv/bin/activate
pip install git+https://github.com/google-deepmind/disco_rl.git
```

The package can also be installed from colab:

```bash
!pip install git+https://github.com/google-deepmind/disco_rl.git
```

## Usage

The code is structured as follows:

* `environments/` contains the general interface for the environments that can
  be used with the provided harness, and two implementations of `Catch`:
  a CPU-based one and jittable;

* `networks/` includes a simple MLP network and LSTM-based components of the
  DiscoRL models, all implemented in Haiku;

* `update_rules/` has implementations of the discovered rules, actor-critic, and
  policy gradient;

* `value_fns/` contains value-function related utilities;

* `types.py`, `utils.py`, `optimizers.py` implement a basic functionality for
  the harness;

* `agent.py` is a generic implementation of an RL agent which uses the update
  rule's API for training, hence it is compatible with all the rules from
  `update_rules/`.

Detailed examples of usage can be found in the colabs above.

## Citation

Please cite the original Nature paper:

```
@Article{DiscoRL2025,
  author  = {Oh, Junhyuk and Farquhar, Greg and Kemaev, Iurii and Calian, Dan A. and Hessel, Matteo and Zintgraf, Luisa and Singh, Satinder and van Hasselt, Hado and Silver, David},
  journal = {Nature},
  title   = {Discovering State-of-the-art Reinforcement Learning Algorithms},
  year    = {2025},
  doi     = {10.1038/s41586-025-09761-x}
}
```

## License and disclaimer

Copyright 2025 Google LLC

All software is licensed under the Apache License, Version 2.0 (Apache 2.0);
you may not use this file except in compliance with the Apache 2.0 license.
You may obtain a copy of the Apache 2.0 license at:
https://www.apache.org/licenses/LICENSE-2.0

All other materials are licensed under the Creative Commons Attribution 4.0
International License (CC-BY). You may obtain a copy of the CC-BY license at:
https://creativecommons.org/licenses/by/4.0/legalcode

Unless required by applicable law or agreed to in writing, all software and
materials distributed here under the Apache 2.0 or CC-BY licenses are
distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
either express or implied. See the licenses for the specific language governing
permissions and limitations under those licenses.

This is not an official Google product.


--- FILE: ./github_DiscoRL/.gitignore ---

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST


--- FILE: ./github_DiscoRL/CONTRIBUTING.md ---

# How to Contribute

## Contributor License Agreement

Contributions to this project must be accompanied by a Contributor License
Agreement. You (or your employer) retain the copyright to your contribution,
this simply gives us permission to use and redistribute your contributions as
part of the project. Head over to <https://cla.developers.google.com/> to see
your current agreements on file or to sign a new one.

You generally only need to submit a CLA once, so if you've already submitted one
(even if it was for a different project), you probably don't need to do it
again.

## Code reviews

All submissions, including submissions by project members, require review. We
use GitHub pull requests for this purpose. Consult
[GitHub Help](https://help.github.com/articles/about-pull-requests/) for more
information on using pull requests.

## Community Guidelines

This project follows [Google's Open Source Community
Guidelines](https://opensource.google/conduct/).


--- FILE: ./github_DiscoRL/disco_rl/.DS_Store ---



--- FILE: ./github_DiscoRL/disco_rl/__init__.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""DiscoRL."""

__version__ = "1.0.0"


--- FILE: ./github_DiscoRL/disco_rl/types.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Types."""

from typing import Any, Callable, Mapping, Sequence

import chex
import dm_env
from dm_env import specs
import haiku as hk
import jax
import jax.numpy as jnp
import numpy as np

Array = jnp.ndarray
ArraySpec = jax.ShapeDtypeStruct
ActionSpec = specs.BoundedArray
Specs = dict[str, specs.Array | ArraySpec]
SpecsTree = ArraySpec | Sequence['SpecsTree'] | dict[str, 'SpecsTree']
MetaState = dict[str, chex.ArrayTree | None]
UpdateRuleLog = dict[str, chex.ArrayTree]

HaikuState = hk.State
AgentOuts = dict[str, chex.ArrayTree]
UpdateRuleOuts = dict[str, chex.ArrayTree]
HyperParams = dict[str, chex.Array | float]

OptState = chex.ArrayTree
AgentParams = chex.ArrayTree
MetaParams = chex.ArrayTree
MetaParamsEMA = dict[float, MetaParams]  # {decay: params}
# State that can be directly updated by update rule.
MetaState = dict[str, chex.ArrayTree | None]
LogDict = dict[str, chex.Array]
RNNState = chex.ArrayTree


# (params, state, observations, should_reset) -> (agent_out, new_state)
AgentStepFn = Callable[
    [AgentParams, HaikuState, chex.ArrayTree, chex.Array | None],
    tuple[AgentOuts, HaikuState],
]
AgentUnrollFn = Callable[
    [
        AgentParams,
        HaikuState,
        chex.ArrayTree,
        chex.Array | None,
    ],
    tuple[AgentOuts, HaikuState],
]
SampleActionFn = Callable[
    [chex.PRNGKey, MetaParams, AgentOuts],
    tuple[chex.ArrayTree, chex.ArrayTree, chex.ArrayTree],
]


@chex.dataclass
class ValueFnConfig:
  """Value function config."""

  net: str
  net_args: dict[str, Any]
  learning_rate: float
  max_abs_update: float
  discount_factor: float
  td_lambda: float
  outer_value_cost: float
  ema_decay: float = 0.99
  ema_eps: float = 1e-6


@chex.dataclass
class ValueState:
  params: AgentParams
  state: HaikuState
  opt_state: OptState
  adv_ema_state: 'EmaState'
  td_ema_state: 'EmaState'


@chex.dataclass
class TransformConfig:
  source: str
  transforms: Sequence[str | Callable[[Any], chex.Array]]


@chex.dataclass
class MetaNetInputOption:
  """Meta network input options."""

  base: Sequence[TransformConfig]
  action_conditional: Sequence[TransformConfig]


@chex.dataclass(mappable_dataclass=False, frozen=True)
class PolicyNetwork:
  """Collects useful callable transformations of underlying agent network."""

  # hk-transformed functions.
  init: Callable[
      [
          chex.PRNGKey,  # rng for params
          chex.ArrayTree,  # obs
          chex.Array | None,  # should_reset
      ],
      tuple[AgentParams, HaikuState],
  ]
  one_step: AgentStepFn
  unroll: AgentUnrollFn


@chex.dataclass
class EmaState:
  # The tree of first moments.
  moment1: chex.ArrayTree
  # The tree of second moments.
  moment2: chex.ArrayTree
  # The product of the all decays from the start of accumulating.
  decay_product: float


@chex.dataclass
class EnvironmentTimestep:
  observation: Mapping[str, chex.ArrayTree]
  step_type: chex.Array
  reward: chex.Array


@chex.dataclass
class ActorTimestep:
  """Actor timestep."""

  observations: chex.ArrayTree
  actions: Any
  rewards: Any
  discounts: Any
  agent_outs: AgentOuts
  states: HaikuState
  logits: Any

  @classmethod
  def from_rollout(cls, rollout: 'ActorRollout') -> 'ActorTimestep':
    return cls(
        observations=rollout.observations,
        actions=rollout.actions,
        rewards=rollout.rewards,
        discounts=rollout.discounts,
        agent_outs=rollout.agent_outs,
        states=rollout.states,
        logits=rollout.logits,
    )

  def to_env_timestep(self) -> 'EnvironmentTimestep':
    return EnvironmentTimestep(
        observation=self.observations,
        step_type=jnp.where(
            self.discounts > 0, dm_env.StepType.MID, dm_env.StepType.LAST
        ),
        reward=self.rewards,
    )


@chex.dataclass
class ActorRollout(ActorTimestep):
  """Stacked actor timesteps.

  Shapes: [D, O, T, B, ...] (by default; can be changed in the code).

  where:
    D: number of learner devices
    O: outer rollout length (aka, meta rollout length)
    T: trajectory length
    B: batch size
  """

  @classmethod
  def from_timestep(cls, timestep: ActorTimestep) -> 'ActorRollout':
    return cls(**timestep)

  def first_state(self, time_axis: int) -> HaikuState:
    index = tuple([np.s_[:]] * (time_axis - 1) + [0])
    return jax.tree.map(lambda x: x[index], self.states)


@chex.dataclass
class ValueOuts:
  """Value function outputs."""

  value: jax.typing.ArrayLike = 0.0  # Scalar value
  target_value: jax.typing.ArrayLike = 0.0  # Scalar target value
  rho: jax.typing.ArrayLike = 0.0  # Importance weight
  adv: jax.typing.ArrayLike = 0.0  # Advantage
  normalized_adv: jax.typing.ArrayLike = 0.0  # Normalised advantage
  value_target: jax.typing.ArrayLike = 0.0  # Value target
  td: jax.typing.ArrayLike = 0.0  # value_target - value
  normalized_td: jax.typing.ArrayLike = 0.0  # Normalised TD
  qv_adv: chex.ArrayTree | None = None  # Q - V
  normalized_qv_adv: chex.ArrayTree | None = None  # Normalised Q - V
  q_value: chex.ArrayTree | None = None  # Scalar Q-value
  target_q_value: chex.ArrayTree | None = None  # Scalar target Q-value
  q_target: chex.ArrayTree | None = None  # Q-value target
  q_td: chex.ArrayTree | None = None  # q_target - q_a
  normalized_q_td: chex.ArrayTree | None = None  # Normalised q_td


@chex.dataclass
class UpdateRuleInputs:
  """Update rule inputs."""

  observations: chex.ArrayTree
  actions: chex.Array
  rewards: chex.Array
  is_terminal: chex.Array  # whether the action was terminal
  agent_out: chex.ArrayTree
  behaviour_agent_out: AgentOuts | None = None
  value_out: ValueOuts | None = None
  # Inputs with pre-processing in update rule before meta-net (e.g. advantages)
  extra_from_rule: chex.ArrayTree | None = None

  @property
  def should_reset_mask_fwd(self) -> chex.Array:
    """Returns `should_reset` mask for forward RNNs."""
    # Shifts is_terminal to the right by a step, mimicking step_type.is_first().
    prepend_non_terminal = jnp.zeros_like(self.is_terminal[:1])
    return jnp.concatenate(
        (prepend_non_terminal, self.is_terminal),
        axis=0,
        dtype=self.is_terminal.dtype,
    )

  @property
  def should_reset_mask_bwd(self) -> chex.Array:
    """Returns `should_reset` mask for backward RNNs."""
    # Appends one non-terminal step, for bootstrapping.
    append_non_terminal = jnp.zeros_like(self.is_terminal[:1])
    return jnp.concatenate(
        (self.is_terminal, append_non_terminal),
        axis=0,
        dtype=self.is_terminal.dtype,
    )


--- FILE: ./github_DiscoRL/disco_rl/optimizers.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Optimizers."""

import jax
from jax import numpy as jnp
import optax


def scale_by_adam_sg_denom(
    b1: float = 0.9,
    b2: float = 0.999,
    eps: float = 1e-8,
) -> optax.GradientTransformation:
  """Adam grad rescaling; but denominator does not receive (meta-)gradients.

  References:
    [Kingma et al, 2014](https://arxiv.org/abs/1412.6980)

  Args:
    b1: decay rate for the exponentially weighted average of grads.
    b2: decay rate for the exponentially weighted average of squared grads.
    eps: term added to the denominator to improve numerical stability.

  Returns:
    An (init_fn, update_fn) tuple.
  """

  def init_fn(params):
    mu = jax.tree.map(jnp.zeros_like, params)  # First moment.
    nu = jax.tree.map(jnp.zeros_like, params)  # Second moment.
    return optax.ScaleByAdamState(count=jnp.zeros([], jnp.int32), mu=mu, nu=nu)

  def update_fn(updates, state, params=None):
    del params
    mu = optax.update_moment(updates, state.mu, b1, 1)
    nu = optax.update_moment(updates, state.nu, b2, 2)
    count_inc = optax.safe_int32_increment(state.count)
    mu_hat = optax.bias_correction(mu, b1, count_inc)
    nu_hat = optax.bias_correction(nu, b2, count_inc)
    updates = jax.tree.map(
        lambda m, v: m / (jnp.sqrt(v) + eps),
        mu_hat,
        jax.lax.stop_gradient(nu_hat),  # NOTE: stop_gradient on nu_hat here
    )
    return updates, optax.ScaleByAdamState(count=count_inc, mu=mu, nu=nu)

  return optax.GradientTransformation(init_fn, update_fn)


--- FILE: ./github_DiscoRL/disco_rl/utils.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Utility functions."""

import functools
from typing import Any, Sequence, TypeVar

import chex
import haiku as hk
import jax
import jax.numpy as jnp
import jmp
import numpy as np
import rlax

from disco_rl import types

_T = TypeVar('_T')
_SpecsT = TypeVar('_SpecsT')


def shard_across_devices(data: _T, devices: Sequence[jax.Device]) -> _T:
  num_shards = len(devices)
  leaves, treedef = jax.tree.flatten(data)
  split_leaves = [np.split(leaf, num_shards, axis=0) for leaf in leaves]
  flat_shards = ((leaf[i] for leaf in split_leaves) for i in range(num_shards))
  data_shards = [jax.tree.unflatten(treedef, shard) for shard in flat_shards]
  return jax.device_put_sharded(data_shards, devices)


def gather_from_devices(data: _T) -> _T:
  return jax.tree.map(
      lambda x: x.reshape((-1, *x.shape[2:])), jax.device_get(data)
  )


def batch_lookup(
    table: chex.Array, index: chex.Array, num_dims: int = 2
) -> chex.Array:

  def _lookup(table: chex.Array, index: chex.Array) -> chex.Array:
    return jax.vmap(lambda x, i: x[i])(table, index)

  if index is not None:
    index = index.astype(jnp.int32)
  return hk.BatchApply(_lookup, num_dims=num_dims)(table, index)


def broadcast_specs(specs: _SpecsT, n: int, replace: bool = False) -> _SpecsT:
  """Prepends `n` to the specs' shapes.

  Args:
    specs: specs to broadcast.
    n: a value to prepend to shapes.
    replace: whether to replace or prepend the first dimension.

  Returns:
    Broadcasted specs.
  """
  f_i = 1 if replace else 0

  def _prepend(s: types.ArraySpec | types.specs.Array):
    if isinstance(s, types.specs.Array):
      return s.replace(shape=(n,) + s.shape[f_i:])
    elif isinstance(s, types.ArraySpec):
      return types.ArraySpec(shape=(n,) + s.shape[f_i:], dtype=s.dtype)
    else:
      raise ValueError(f'Unsupported spec type: {type(s)}')

  return jax.tree.map(_prepend, specs)


def tree_stack(
    elems: Sequence[chex.ArrayTree], axis: int = 0
) -> chex.ArrayTree:
  """Stacks a sequence of trees into a single tree."""
  return jax.tree.map(lambda *xs: jnp.stack(xs, axis=axis), *elems)


def cast_to_single_precision(
    tree_like: _T, cast_ints: bool = True, host_data: bool = False
) -> _T:
  """Casts the data to full precision."""
  if host_data:

    def conditional_cast(x):
      if isinstance(x, (np.ndarray, jnp.ndarray)):
        if np.issubdtype(x.dtype, np.floating) or jnp.issubdtype(
            x.dtype, jnp.floating
        ):
          if x.dtype != np.float32:
            x = x.astype(np.float32)
        elif cast_ints and x.dtype == np.int64:
          x = x.astype(np.int32)
      return x

    return jax.tree.map(conditional_cast, tree_like)
  else:
    return jmp.cast_to_full(tree_like)


def get_num_actions_from_spec(spec: types.ActionSpec) -> int:
  """Returns the number of actions from the action spec."""
  return spec.maximum - spec.minimum + 1


def get_logits_specs(
    spec: types.ActionSpec, with_batch_dim: bool = False
) -> types.ArraySpec:
  """Extracts a tree of shapes for logits for the provided spec."""
  if with_batch_dim:
    spec = spec.replace(shape=spec.shape[1:])

  return types.ArraySpec((get_num_actions_from_spec(spec),), np.float32)


def zeros_like_spec(spec: Any, prepend_shape: tuple[int, ...] = ()):
  """Returns a tree of zeros from `spec`.

  Args:
    spec: a tree of `array_like`s or specs.
    prepend_shape: a tuple of integers to prepend to the shapes.

  Returns:
    A tree of zero arrays.
  """
  return jax.tree.map(
      lambda spec: np.zeros(shape=prepend_shape + spec.shape, dtype=spec.dtype),
      spec,
  )


def differentiable_policy_gradient_loss(
    logits_t: chex.Array, a_t: chex.Array, adv_t: chex.Array, backprop: bool
) -> chex.Array:
  """Calculates the policy gradient loss with differentiable advantage.

  An optimised version of `rlax.policy_gradient_loss()`.

  Args:
    logits_t: a sequence of unnormalized action preferences (shape: [..., |A|]).
    a_t: a sequence of actions sampled from the preferences `logits_t`.
    adv_t: the observed or estimated advantages from executing actions `a_t`.
    backprop: whether to make the loss differentiable.

  Returns:
    Loss (per step) whose gradient corresponds to a policy gradient update.
  """

  chex.assert_type([logits_t, a_t, adv_t], [float, int, float])

  log_pi_a = rlax.batched_index(jax.nn.log_softmax(logits_t), a_t)
  if backprop:
    loss_per_step = -log_pi_a * adv_t
  else:
    loss_per_step = -log_pi_a * jax.lax.stop_gradient(adv_t)
  return loss_per_step


class MovingAverage:
  """Functions to track EMAs and use them for normalization."""

  def __init__(
      self,
      example_tree: chex.ArrayTree,
      decay: float = 0.999,
      eps: float = 1e-6,
  ):
    """Initialize moving average parameters.

    Args:
      example_tree: An example of the structure later passed to `update_state`.
      decay: The decay of the moments. I.e., the learning rate is `1 - decay`.
      eps: Epsilon used for normalization.
    """
    self._example_tree = example_tree
    self._decay = decay
    self._eps = eps

  def init_state(self) -> types.EmaState:
    zeros = jax.tree.map(
        lambda x: jnp.zeros((), jnp.float32), self._example_tree
    )
    return types.EmaState(  # pytype: disable=wrong-arg-types  # jnp-type
        moment1=zeros,
        moment2=zeros,
        decay_product=jnp.ones([], dtype=jnp.float32),
    )

  def update_state(
      self,
      tree_like: chex.ArrayTree,
      state: types.EmaState,
      pmean_axis_name: str | None,
  ) -> types.EmaState:
    """Update moving average stats."""
    squared_tree = jax.tree.map(jnp.square, tree_like)

    def _update(
        moment: chex.Array,
        value: chex.Array,
        pmean_axis_name: str | None = None,
    ) -> chex.Array:
      mean = jnp.mean(value)
      # Compute the mean across all learner devices involved in the `pmap`.
      if pmean_axis_name is not None:
        mean = jax.lax.pmean(mean, axis_name=pmean_axis_name)
      return self._decay * moment + (1.0 - self._decay) * mean

    update_fn = functools.partial(_update, pmean_axis_name=pmean_axis_name)
    moment1 = jax.tree.map(update_fn, state.moment1, tree_like)
    moment2 = jax.tree.map(update_fn, state.moment2, squared_tree)
    return types.EmaState(
        moment1=moment1,
        moment2=moment2,
        decay_product=state.decay_product * self._decay,
    )

  def _compute_moments(
      self, state: types.EmaState
  ) -> tuple[chex.ArrayTree, chex.ArrayTree]:
    """Computes moments, applying 0-debiasing as in the Adam optimizer."""

    # Factor to account for initializing moments with 0s.
    debias = 1.0 / (1 - state.decay_product)

    # Debias mean.
    mean = jax.tree.map(lambda m1: m1 * debias, state.moment1)

    # Estimate zero-centered debiased variance; clip negative values to
    # safeguard against numerical errors.
    variance = jax.tree.map(
        lambda m2, m: jnp.maximum(0.0, m2 * debias - jnp.square(m)),
        state.moment2,
        mean,
    )
    return mean, variance

  def normalize(
      self,
      value: chex.ArrayTree,
      state: types.EmaState,
      subtract_mean: bool = True,
      root_eps: float = 1e-12,
  ) -> chex.ArrayTree:
    """Normalize by dividing by second moment and subtracting by mean."""

    def _normalize(mean, var, val):
      # Two epsilons, instead of one, are used for numerical stability when
      # backpropagating through the normalization (as in optax.scale_by_adam).
      if subtract_mean:
        return (val - mean) / (jnp.sqrt(var + root_eps) + self._eps)
      else:
        return val / (jnp.sqrt(var + root_eps) + self._eps)

    mean, variance = self._compute_moments(state)
    return jax.tree.map(_normalize, mean, variance, value)


--- FILE: ./github_DiscoRL/disco_rl/agent.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""An agent that uses update rules to learn from the environment."""

from absl import logging
import chex
import distrax
import dm_env
import haiku as hk
import jax
from jax import numpy as jnp
from ml_collections import config_dict
import numpy as np
import optax

from disco_rl import optimizers
from disco_rl import types
from disco_rl.networks import nets
from disco_rl.update_rules import actor_critic
from disco_rl.update_rules import base as update_rules_base
from disco_rl.update_rules import disco
from disco_rl.update_rules import policy_gradient


@chex.dataclass(frozen=True)
class LearnerState:
  """Dataclass for the learner's state: params, and optimizer/update's state."""

  params: hk.Params
  opt_state: optax.OptState
  meta_state: types.MetaState


class Agent:
  """A generic agent with an update rule.

  Its API supports both evaluation and meta-training of the update rule.

  Note that for evaluation/inference only, the update rule's application can be
  simplified and encapsulated by using its __call__ method.
  """

  update_rule: update_rules_base.UpdateRule

  def __init__(
      self,
      *,
      single_observation_spec: types.Specs,
      single_action_spec: types.ActionSpec,
      agent_settings: config_dict.ConfigDict,
      batch_axis_name: str | None,
  ):
    self.settings = agent_settings
    self.single_observation_spec = single_observation_spec
    self.single_action_spec = single_action_spec
    self._batch_axis_name = batch_axis_name

    # This agent only supports scalar discrete action specs.
    assert single_action_spec.dtype == np.int32
    assert not single_action_spec.shape

    # Create the update rule.
    if agent_settings.update_rule_name == 'disco':
      self.update_rule = disco.DiscoUpdateRule(**agent_settings.update_rule)
    elif agent_settings.update_rule_name == 'actor_critic':
      self.update_rule = actor_critic.ActorCritic(**agent_settings.update_rule)
    elif agent_settings.update_rule_name == 'policy_gradient':
      self.update_rule = policy_gradient.PolicyGradientUpdate(
          **agent_settings.update_rule
      )
    else:
      raise ValueError(
          f'Unsupported update rule: {agent_settings.update_rule_name}'
      )
    logging.info('Update rule config %r', agent_settings.update_rule)

    # Define the agent's neural network.
    flat_out_spec = self.update_rule.flat_output_spec(self.single_action_spec)
    model_out_spec = self.update_rule.model_output_spec(self.single_action_spec)
    self._network = nets.get_network(
        name=agent_settings.net_settings.name,
        action_spec=self.single_action_spec,
        out_spec=flat_out_spec,
        model_out_spec=model_out_spec,
        **agent_settings.net_settings.net_args,
    )

    # Define the optimiser.
    self._optimizer = optax.chain(
        optimizers.scale_by_adam_sg_denom(),
        optax.clip(max_delta=self.settings.max_abs_update),
        optax.scale(-self.settings.learning_rate),
    )

  def _dummy_obs(self, batch_size: int) -> chex.ArrayTree:
    """Create dummy observation for params and actor state initialisation."""
    return jax.tree.map(
        lambda v: np.zeros((batch_size,) + v.shape, dtype=v.dtype),
        self.single_observation_spec,
    )

  def _dummy_act(self, batch_size: int) -> chex.Array:
    """Create dummy action for params and actor state initialisation."""
    return np.zeros((batch_size,), dtype=self.single_action_spec.dtype)

  def _dummy_should_reset(self, batch_size: int) -> chex.Array:
    """Create dummy should_reset for params and actor state initialisation."""
    return jnp.zeros([batch_size], dtype=bool)

  def initial_actor_state(self, rng: chex.PRNGKey) -> types.HaikuState:
    """Create (potentially empty) initial actor state."""
    dummy_obs = self._dummy_obs(batch_size=1)
    should_reset = self._dummy_should_reset(batch_size=1)
    _, rnn_state = self._network.init(
        rng,  # params init rng
        dummy_obs,
        should_reset,
    )
    return rnn_state

  def initial_learner_state(self, rng_key: chex.PRNGKey) -> LearnerState:
    """Create the initial learner state."""
    net_rng, state_rng = jax.random.split(rng_key)
    dummy_obs = self._dummy_obs(batch_size=1)
    should_reset = self._dummy_should_reset(batch_size=1)
    # Init params, optimiser state and the discovered update's state.
    params, _ = self._network.init(net_rng, dummy_obs, should_reset)
    opt_state = self._optimizer.init(params)
    meta_state = self.update_rule.init_meta_state(rng=state_rng, params=params)
    return LearnerState(
        params=params, opt_state=opt_state, meta_state=meta_state
    )

  def actor_step(
      self,
      actor_params: hk.Params,
      rng: chex.PRNGKey,
      timestep: types.EnvironmentTimestep,
      actor_state: hk.State,
  ) -> tuple[types.ActorTimestep, hk.State]:
    """Compute actions for the provided timestep."""
    # Perform inference on the agent's network.
    should_reset = timestep.step_type == dm_env.StepType.LAST
    agent_outs, next_actor_state = self._network.one_step(
        actor_params, actor_state, timestep.observation, should_reset
    )
    # Sample actions.
    actions = distrax.Softmax(logits=agent_outs['logits']).sample(seed=rng)

    # Return the actor timestep.
    actor_timestep = types.ActorTimestep(
        observations=timestep.observation,
        actions=actions,
        agent_outs=agent_outs,
        rewards=timestep.reward,
        discounts=jnp.logical_not(should_reset),
        states=actor_state,
        logits=agent_outs['logits'],
    )
    return actor_timestep, next_actor_state

  def unroll_net(
      self,
      agent_net_params: hk.Params,
      agent_net_state: hk.State,
      rollout: types.ActorRollout,
  ) -> tuple[types.AgentOuts, hk.State]:
    """Unroll the agent's network."""
    # Mask, with 0s only on timesteps of type LAST.
    masks = rollout.discounts[:-1] > 0

    # Shifts is_terminal to the right by a step.
    prepend_non_terminal = jax.numpy.zeros_like(masks[:1])
    should_reset = jnp.concatenate(
        (prepend_non_terminal, masks), axis=0, dtype=masks.dtype
    )

    # Unroll the network.
    agent_out, new_agent_net_state = self._network.unroll(
        agent_net_params,
        agent_net_state,
        rollout.observations,
        should_reset,
    )
    return agent_out, new_agent_net_state

  def _loss(
      self,
      agent_net_params: hk.Params,
      agent_net_state: hk.State,
      meta_state: types.MetaState,
      rollout: types.ActorRollout,
      meta_out: types.UpdateRuleOuts,
      is_meta_training: bool,
  ) -> tuple[chex.Array, tuple[types.MetaState, hk.State, types.LogDict]]:
    """Computes the loss according to the update rule."""
    # Extract rewards and discounts.
    reward = rollout.rewards[1:]
    masks = rollout.discounts[:-1] > 0

    # Unroll the network.
    agent_out, new_agent_net_state = self.unroll_net(
        agent_net_params, agent_net_state, rollout
    )

    # Construct inputs for the update rule.
    discount = rollout.discounts[1:]
    eta_inputs = types.UpdateRuleInputs(
        observations=rollout.observations,  # [T, ...]
        actions=rollout.actions,  # [T, ...]
        rewards=reward,  # [T-1]
        is_terminal=discount == 0,  # [T-1]
        behaviour_agent_out=rollout.agent_outs,  # [T, ...]
        agent_out=agent_out,  # [T, ...]
        value_out=None,
    )

    # When not meta-training, this can be simplified by calling the update rule
    # directly to get the combined loss. See the class docstring for details.
    hyper_params = self.settings.hyper_params.to_dict()
    loss_per_step, log = self.update_rule.agent_loss(
        eta_inputs, meta_out, hyper_params, backprop=is_meta_training
    )
    loss_per_step_no_meta, log_no_meta = self.update_rule.agent_loss_no_meta(
        eta_inputs, meta_out, hyper_params
    )
    disco_log = log_no_meta | log
    total_loss_per_step = loss_per_step + loss_per_step_no_meta
    total_loss = (total_loss_per_step * masks).sum() / (masks.sum() + 1e-8)

    # Make logs.
    log_dict = dict(total_loss=total_loss, **jax.tree.map(jnp.mean, disco_log))
    return total_loss, (meta_state, new_agent_net_state, log_dict)

  def learner_step(
      self,
      rng: chex.PRNGKey,
      rollout: types.ActorRollout,
      learner_state: LearnerState,
      agent_net_state: hk.State,
      update_rule_params: types.MetaParams,
      is_meta_training: bool,
  ) -> tuple[LearnerState, hk.State, types.LogDict]:
    """Runs a training step across all learner devices for one rollout."""
    reward = rollout.rewards[1:]
    agent_out, _ = self.unroll_net(
        learner_state.params, agent_net_state, rollout
    )

    # Compute the loss using the discovered update(s).
    # Construct inputs for the discovered update.
    eta_inputs = types.UpdateRuleInputs(
        observations=rollout.observations,  # [T, ...]
        actions=rollout.actions,  # [T, ...]
        rewards=reward,  # [T-1]
        is_terminal=rollout.discounts[1:] == 0,  # [T-1]
        behaviour_agent_out=rollout.agent_outs,  # [T, ...]
        agent_out=agent_out,  # [T, ...]
        value_out=None,
    )

    # Apply the update network.
    meta_out, new_meta_state = self.update_rule.unroll_meta_net(
        meta_params=update_rule_params,
        params=learner_state.params,
        state=agent_net_state,
        meta_state=learner_state.meta_state,
        rollout=eta_inputs,
        hyper_params=self.settings.hyper_params.to_dict(),
        unroll_policy_fn=self._network.unroll,
        rng=rng,
        axis_name=self._batch_axis_name,
    )

    # Get gradient of the loss function using the latest rollout and parameters.
    dloss_dparams = jax.grad(self._loss, has_aux=True)
    grads, (_, last_agent_net_state, logging_dict) = dloss_dparams(
        learner_state.params,
        agent_net_state=agent_net_state,
        meta_state=learner_state.meta_state,
        rollout=rollout,
        meta_out=meta_out,
        is_meta_training=is_meta_training,
    )
    # Average gradients across the other learner devices involved in the `pmap`.
    if self._batch_axis_name is not None:
      grads = jax.lax.pmean(grads, axis_name=self._batch_axis_name)
    # Use the optimizer to apply a suitable gradient transformation.
    updates, new_opt_state = self._optimizer.update(
        grads, learner_state.opt_state, learner_state.params
    )
    # Update parameters.
    new_params = optax.apply_updates(learner_state.params, updates)

    # Log the gradient and update norms.
    logging_dict['global_gradient_norm'] = optax.global_norm(grads)
    logging_dict['global_update_norm'] = optax.global_norm(updates)
    logging_dict['meta_out'] = meta_out
    # Return the updated learner state and logging outputs.
    learner_state = LearnerState(
        params=new_params, opt_state=new_opt_state, meta_state=new_meta_state
    )
    return learner_state, last_agent_net_state, logging_dict


def get_settings_disco():
  """Disco-103 setting."""
  return config_dict.ConfigDict(
      dict(
          # discovered objective
          hyper_params=dict(
              pi_cost=1.0,
              y_cost=1.0,
              z_cost=1.0,
              value_cost=0.2,
              aux_policy_cost=1.0,
              target_params_coeff=0.9,
              value_fn_td_lambda=0.95,
              discount_factor=0.997,
          ),
          update_rule_name='disco',
          update_rule=dict(
              net=config_dict.ConfigDict(
                  dict(
                      name='lstm',
                      prediction_size=600,
                      hidden_size=256,
                      embedding_size=(16, 1),
                      policy_target_channels=(16,),
                      policy_channels=(16, 2),
                      output_stddev=0.3,
                      aux_stddev=0.3,
                      policy_target_stddev=0.3,
                      state_stddev=1.0,
                      meta_rnn_kwargs=dict(
                          policy_channels=(16, 2),
                          embedding_size=(16,),
                          pred_embedding_size=(16, 1),
                          hidden_size=128,
                      ),
                      input_option=disco.get_input_option(),
                  )
              ),
              value_discount=0.997,
              num_bins=601,
              max_abs_value=300.0,
          ),
          # optimizer.
          learning_rate=0.0003,
          max_abs_update=1.0,
          # agent network.
          net_settings=dict(
              name='mlp',
              net_args=dict(
                  dense=(512, 512),
                  model_arch_name='lstm',
                  head_w_init_std=1e-2,
                  model_kwargs=dict(
                      head_mlp_hiddens=(128,),
                      lstm_size=128,
                  ),
              ),
          ),
      )
  )


def get_settings_actor_critic():
  """Actor-Critic setting."""
  return config_dict.ConfigDict(
      dict(
          hyper_params=dict(
              discount_factor=0.997,
              vtrace_lambda=0.95,
              entropy_cost=0.2,
              pg_cost=1.0,
              value_cost=0.5,
          ),
          update_rule_name='actor_critic',
          update_rule=config_dict.ConfigDict(
              dict(
                  categorical_value=True,
                  num_bins=601,
                  max_abs_value=300.0,
                  nonlinear_value_transform=True,
                  normalize_adv=False,
                  normalize_td=False,
              )
          ),
          # optimizer.
          learning_rate=5e-4,
          max_abs_update=1.0,
          # agent network.
          net_settings=dict(
              name='mlp',
              net_args=dict(
                  dense=(64, 32, 32),
                  model_arch_name='lstm',
                  head_w_init_std=1e-2,
                  model_kwargs=dict(
                      head_mlp_hiddens=(64,),
                      lstm_size=64,
                  ),
              ),
          ),
      ),
  )


--- FILE: ./github_DiscoRL/disco_rl/environments/jittable_envs.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Jittable environments."""

import jax
import jax.numpy as jnp
from ml_collections import config_dict

from disco_rl.environments.wrappers import batched_jittable_env


class _SingleStreamCatch:
  """Catch with lifetime reset."""

  def __init__(self, rows: int = 10, columns: int = 5):
    self._rows = rows
    self._columns = columns

  @property
  def num_actions(self) -> int:
    return 3

  def initial_state(self, rng):
    ball_y = 0
    ball_x = jax.random.randint(rng, (), 0, self._columns)
    paddle_y = self._rows - 1
    paddle_x = self._columns // 2
    return jnp.array((ball_y, ball_x, paddle_y, paddle_x), dtype=jnp.int32)

  def episode_reset(self, rng, state):
    del state
    return self.initial_state(rng)

  def step(self, rng, state, action):
    del rng
    paddle_x = jnp.clip(state[3] + action - 1, 0, self._columns - 1)
    return jnp.array([state[0] + 1, state[1], state[2], paddle_x])

  def is_terminal(self, state):
    return state[0] == self._rows - 1

  def render(self, state):
    """Render the screen."""

    def f(y, x):
      return jax.lax.select(
          jnp.bitwise_or(
              jnp.bitwise_and(y == state[0], x == state[1]),
              jnp.bitwise_and(y == state[2], x == state[3]),
          ),
          1.0,
          0.0,
      )

    y_board = jnp.repeat(jnp.arange(self._rows), self._columns)
    x_board = jnp.tile(jnp.arange(self._columns), self._rows)
    return jax.vmap(f)(y_board, x_board).reshape((self._rows, self._columns, 1))

  def reward(self, state):
    return jax.lax.select(
        state[0] == self._rows - 1,
        jax.lax.select(state[1] == state[3], 1.0, -1.0),
        0.0,
    )


class CatchJittableEnvironment(batched_jittable_env.BatchedJittableEnvironment):
  """Catch environment."""

  def __init__(
      self,
      batch_size: int,
      env_settings: config_dict.ConfigDict,
  ) -> None:

    super().__init__(
        _SingleStreamCatch,
        batch_size,
        env_settings,
    )


def get_config_catch() -> config_dict.ConfigDict:
  """Returns default config for CatchEnvironment."""
  return config_dict.ConfigDict(
      dict(
          rows=8,
          columns=8,
      )
  )


--- FILE: ./github_DiscoRL/disco_rl/environments/__init__.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================




--- FILE: ./github_DiscoRL/disco_rl/environments/catch.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Catch environment."""

import dm_env
from dm_env import specs
from ml_collections import config_dict as configdict
import numpy as np

from disco_rl.environments.wrappers import batched_env
from disco_rl.environments.wrappers import single_stream_env

_ACTIONS = (-1, 0, 1)  # Left, no-op, right.


class SingleStreamCatch:
  """A Catch environment built on the `dm_env.Environment` class."""

  def __init__(self, env_settings: configdict.ConfigDict):
    """Initializes a new Catch environment."""
    self._rows = env_settings.rows
    self._columns = env_settings.columns
    self._rng = np.random.RandomState(env_settings.random_seed)
    self._board = np.zeros((self._rows, self._columns), dtype=np.float32)
    self._ball_x = None
    self._ball_y = None
    self._paddle_x = None
    self._paddle_y = self._rows - 1
    self._reset_next_step = True

  def reset(self) -> dm_env.TimeStep:
    """Returns the first `TimeStep` of a new episode."""
    self._reset_next_step = False
    self._ball_x = self._rng.randint(self._columns)
    self._ball_y = 0
    self._paddle_x = self._columns // 2
    return dm_env.restart(self._observation())

  def step(self, action: int) -> dm_env.TimeStep:
    """Updates the environment according to the action."""
    if self._reset_next_step:
      return self.reset()

    # Move the paddle.
    dx = _ACTIONS[action]
    self._paddle_x = np.clip(self._paddle_x + dx, 0, self._columns - 1)

    # Drop the ball.
    self._ball_y += 1

    # Check for termination.
    if self._ball_y == self._paddle_y:
      reward = 1.0 if self._paddle_x == self._ball_x else -1.0
      self._reset_next_step = True
      return dm_env.termination(reward=reward, observation=self._observation())
    else:
      return dm_env.transition(reward=0.0, observation=self._observation())

  def observation_spec(self) -> specs.Array:
    """Returns the observation spec."""
    return specs.Array(
        shape=self._board.shape,
        dtype=self._board.dtype,
        name="board",
    )

  def action_spec(self) -> specs.BoundedArray:
    """Returns the action spec."""
    return specs.BoundedArray((), np.int32, 0, len(_ACTIONS) - 1)

  def _observation(self) -> np.ndarray:
    self._board.fill(0.0)
    self._board[self._ball_y, self._ball_x] = 1.0
    self._board[self._paddle_y, self._paddle_x] = 1.0
    return self._board.copy()


class CatchEnvironment(batched_env.BatchedSingleStreamEnvironment):
  """A batched Catch environment."""

  def __init__(
      self,
      batch_size: int,
      env_settings: configdict.ConfigDict,
  ) -> None:

    def _single_stream_catch(
        env_settings: configdict.ConfigDict,
    ) -> single_stream_env.SingleStreamEnv:
      return single_stream_env.SingleStreamEnv(
          env=SingleStreamCatch(env_settings)
      )

    super().__init__(
        _single_stream_catch,
        batch_size,
        env_settings,
    )


def get_config() -> configdict.ConfigDict:
  """Returns default config for CatchEnvironment."""
  return configdict.ConfigDict(
      dict(
          rows=8,
          columns=8,
          random_seed=1,
      )
  )


--- FILE: ./github_DiscoRL/disco_rl/environments/base.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Base interface for environments."""

import abc
from typing import Any, TypeVar

import chex

from disco_rl import types
from disco_rl import utils

_EnvState = TypeVar('_EnvState')


class Environment(abc.ABC):
  """Interface for environments.

  All environments are supposed to be batched.
  """

  batch_size: int

  @abc.abstractmethod
  def step(
      self, states: _EnvState, actions: chex.ArrayTree
  ) -> tuple[_EnvState, types.EnvironmentTimestep]:
    pass

  @abc.abstractmethod
  def reset(
      self, rng_key: chex.PRNGKey
  ) -> tuple[Any, types.EnvironmentTimestep]:
    """Resets episodes."""
    pass

  @abc.abstractmethod
  def single_observation_spec(self) -> types.Specs:
    pass

  @abc.abstractmethod
  def single_action_spec(self) -> types.ActionSpec:
    pass

  def batched_action_spec(self) -> types.ActionSpec:
    return utils.broadcast_specs(self.single_action_spec(), self.batch_size)

  def batched_observation_spec(self) -> types.Specs:
    return utils.broadcast_specs(
        self.single_observation_spec(), self.batch_size
    )


--- FILE: ./github_DiscoRL/disco_rl/environments/wrappers/batched_jittable_env.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Wrapper for batched jittable environments."""

import chex
import dm_env
from dm_env import specs as dm_specs
import jax
import jax.numpy as jnp
from ml_collections import config_dict as configdict
import numpy as np

from disco_rl import types
from disco_rl.environments import base


def _to_env_timestep(
    obs: chex.Array, reward: chex.Array, is_terminal: chex.Array
) -> types.EnvironmentTimestep:
  return types.EnvironmentTimestep(
      step_type=jax.lax.select(
          is_terminal, dm_env.StepType.LAST, dm_env.StepType.MID
      ),
      reward=reward.astype(jnp.float32),
      observation=obs.astype(jnp.float32),
  )


@chex.dataclass(mappable_dataclass=False)
class EnvState:
  state: chex.ArrayTree
  rng: chex.PRNGKey


class BatchedJittableEnvironment(base.Environment):
  """Wrapper for making a batched jitted disco env."""

  def __init__(
      self,
      env_class,
      batch_size: int,
      env_settings: configdict.ConfigDict,
  ):
    self.batch_size = batch_size
    self._env = env_class(**env_settings.to_dict())
    self._single_action_spec = dm_specs.BoundedArray(
        (), np.int32, 0, self._env.num_actions - 1
    )

    dummy_state = self._env.initial_state(jax.random.PRNGKey(0))
    obs = self._env.render(dummy_state)
    self._single_observation_spec = {
        'observation': dm_specs.Array(shape=obs.shape, dtype=obs.dtype)
    }

    self._batched_env_step = jax.vmap(self._single_env_step)
    self._batched_env_reset = jax.vmap(self._single_env_reset)

  def _single_env_step(
      self, env_state: EnvState, action: chex.Array
  ) -> tuple[EnvState, types.EnvironmentTimestep]:
    new_rng, rng_step = jax.random.split(env_state.rng)
    new_state = self._env.step(rng_step, env_state.state, action)
    is_terminal = self._env.is_terminal(new_state)
    reward = self._env.reward(new_state)

    # Use initial states for terminated episodes while keeping other data.
    init_state = self._env.episode_reset(rng_step, new_state)
    next_state = jax.tree.map(
        lambda reset_x, x: jax.lax.select(is_terminal, reset_x, x),
        init_state,
        new_state,
    )

    return EnvState(state=next_state, rng=new_rng), _to_env_timestep(
        self._env.render(next_state), reward, is_terminal
    )

  def _single_env_reset(
      self, rng_key: chex.PRNGKey
  ) -> tuple[EnvState, types.EnvironmentTimestep]:
    new_rng, reset_rng = jax.random.split(rng_key)
    state = self._env.initial_state(reset_rng)
    return EnvState(state=state, rng=new_rng), _to_env_timestep(
        self._env.render(state),
        self._env.reward(state),
        self._env.is_terminal(state),
    )

  def step(  # pytype: disable=signature-mismatch  # numpy-scalars
      self, state: EnvState, actions: chex.Array
  ) -> tuple[EnvState, types.EnvironmentTimestep]:
    return self._batched_env_step(state, actions)

  def reset(self, rng_key: chex.PRNGKey) -> tuple[EnvState, types.EnvironmentTimestep]:  # pytype: disable=signature-mismatch  # numpy-scalars
    rngs = jax.random.split(rng_key, self.batch_size)
    return self._batched_env_reset(rngs)

  def single_action_spec(self) -> types.ActionSpec:
    return self._single_action_spec

  def single_observation_spec(self) -> types.Specs:
    return self._single_observation_spec


--- FILE: ./github_DiscoRL/disco_rl/environments/wrappers/__init__.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================




--- FILE: ./github_DiscoRL/disco_rl/environments/wrappers/batched_env.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Wrapper for batching a single-stream environment."""

import copy

import chex
import dm_env
import jax
from ml_collections import config_dict as configdict
import numpy as np
import rlax

from disco_rl import types
from disco_rl import utils
from disco_rl.environments import base


class UnusedEnvState:
  pass


def _to_env_timestep(timestep: dm_env.TimeStep) -> types.EnvironmentTimestep:
  return types.EnvironmentTimestep(
      step_type=np.array(timestep.step_type, dtype=np.int32),
      reward=np.array(timestep.reward or 0.0),
      observation=timestep.observation,
  )


class BatchedSingleStreamEnvironment(base.Environment):
  """Wrapper for making a single-stream environment batched.

  All operations are sequentially executed for all instances from the batch.

  Attributes:
    batch_size: a batch size.
  """

  def __init__(
      self,
      env_class,
      batch_size: int,
      env_settings: configdict.ConfigDict,
  ) -> None:
    self.batch_size = batch_size
    self._num_envs = batch_size
    self._shape_prefix = (batch_size,)

    if 'random_seed' in env_settings:
      seed = env_settings.random_seed
      max_seed = np.iinfo(np.int32).max
      seeds = jax.random.randint(
          jax.random.PRNGKey(seed), (self._num_envs,), 0, max_seed
      )
      envs = []
      for i in range(self._num_envs):
        settings = copy.deepcopy(env_settings)
        settings.random_seed = int(seeds[i])
        envs.append(env_class(env_settings=settings))
      self._envs = tuple(envs)
    else:
      self._envs = tuple(
          env_class(env_settings=env_settings) for _ in range(self._num_envs)
      )

    self._single_action_spec = self._envs[0].single_action_spec()
    self._single_observation_spec = self._envs[0].single_observation_spec()
    self._states = [env.reset(jax.random.PRNGKey(0))[1] for env in self._envs]

  def _stack_states(self) -> types.EnvironmentTimestep:
    states = jax.tree.map(
        lambda *xs: np.stack(xs).reshape(self._shape_prefix + xs[0].shape),
        *self._states,
    )
    return utils.cast_to_single_precision(states, host_data=True)

  def step(
      self, state: UnusedEnvState | None, actions: chex.ArrayTree
  ) -> tuple[UnusedEnvState | None, types.EnvironmentTimestep]:
    del state
    chex.assert_tree_shape_prefix(actions, self._shape_prefix)
    actions = jax.tree.map(
        lambda a: a.reshape((self._num_envs,) + a.shape[2:]), actions
    )
    actions_list = rlax.tree_split_leaves(actions)
    self._states = [
        env.step(None, a)[1] for env, a in zip(self._envs, actions_list)
    ]
    return UnusedEnvState(), self._stack_states()

  def reset(
      self, rng_key: chex.PRNGKey
  ) -> tuple[UnusedEnvState | None, types.EnvironmentTimestep]:
    del rng_key
    self._states = [env.reset(jax.random.PRNGKey(0))[1] for env in self._envs]
    return UnusedEnvState(), self._stack_states()

  def single_action_spec(self) -> types.ActionSpec:
    return self._single_action_spec

  def single_observation_spec(self) -> types.Specs:
    return self._single_observation_spec


--- FILE: ./github_DiscoRL/disco_rl/environments/wrappers/single_stream_env.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Single-stream environment."""

from typing import Any

import chex
import dm_env
import numpy as np

from disco_rl import types
from disco_rl.environments import base


def _to_env_timestep(timestep: dm_env.TimeStep) -> types.EnvironmentTimestep:
  return types.EnvironmentTimestep(
      step_type=np.array(timestep.step_type, dtype=np.int32),
      reward=np.array(timestep.reward or 0.0),
      observation=timestep.observation,
  )


class UnusedEnvState:
  pass


class SingleStreamEnv(base.Environment):
  """Wrapper for a single-stream environment."""

  def __init__(self, env: Any):
    self._env = env

  def reset(
      self, rng_key: chex.PRNGKey
  ) -> tuple[UnusedEnvState | None, types.EnvironmentTimestep]:
    """Resets the environment."""
    del rng_key
    return UnusedEnvState(), _to_env_timestep(self._env.reset())

  def step(
      self, state: UnusedEnvState | None, action: int
  ) -> tuple[UnusedEnvState | None, types.EnvironmentTimestep]:
    """Steps the environment."""
    del state
    ts = _to_env_timestep(self._env.step(action))

    # Reset terminal episodes.
    if ts.step_type == dm_env.StepType.LAST:
      # Step the terminated episodes once again to switch to the next episodes.
      ts_start = _to_env_timestep(self._env.step(action))

      # Recover step_type and rewards from the terminal states.
      ts_start.step_type = ts.step_type
      ts_start.reward = ts.reward

      return UnusedEnvState(), ts_start
    else:
      return UnusedEnvState(), ts

  def single_observation_spec(self) -> types.Specs:
    """Returns the observation spec."""
    return self._env.observation_spec()

  def single_action_spec(self) -> types.ActionSpec:
    """Returns the action spec."""
    return self._env.action_spec()


--- FILE: ./github_DiscoRL/disco_rl/networks/__init__.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================




--- FILE: ./github_DiscoRL/disco_rl/networks/action_models.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Models for conditioning on actions."""

import chex
import haiku as hk
import jax
from jax import numpy as jnp
import numpy as np

from disco_rl import types
from disco_rl import utils


def get_action_model(name: str, *args, **kwargs):
  if name == 'lstm':
    net = LSTMModel(*args, **kwargs)
  else:
    raise ValueError(f'Invalid model network name {name}.')
  return net


class LSTMModel:
  """LSTM-based action-conditional model inspried by Muesli/MuZero."""

  def __init__(
      self,
      action_spec: types.ActionSpec,
      out_spec: types.Specs,
      head_mlp_hiddens: tuple[int, ...],
      lstm_size: int,
  ) -> None:
    self._out_spec = out_spec
    self._action_spec = action_spec
    self._head_mlp_hiddens = head_mlp_hiddens
    self._lstm_size = lstm_size

  def _model_transition_all_actions(
      self, embedding: hk.LSTMState
  ) -> chex.Array:
    """Performs a model transition pass for all actions."""
    num_actions = utils.get_num_actions_from_spec(self._action_spec)
    batch_size = embedding.cell.shape[0]

    # Enumerate all action embeddings.
    one_hot_actions = jnp.eye(num_actions).astype(
        embedding.cell.dtype
    )  # [A, A]
    batched_one_hot_actions = jnp.tile(
        one_hot_actions, [batch_size, 1]
    )  # [BA, A]

    all_actions_embed = jax.tree.map(
        lambda x: jnp.repeat(x, repeats=num_actions, axis=0), embedding
    )  # [BA, *H]

    lstm_output, _ = hk.LSTM(self._lstm_size, name='action_cond')(
        batched_one_hot_actions, all_actions_embed
    )
    return lstm_output

  def _model_head_pass(
      self, transition_output: chex.Array
  ) -> dict[str, chex.Array]:
    """Gets outputs from MLPs with shapes according to out spec."""
    # transition_output has shape [BA, ...]
    num_actions = utils.get_num_actions_from_spec(self._action_spec)
    batch_size = transition_output.shape[0] // num_actions

    model_outputs = dict()
    for key, pred_spec in self._out_spec.items():
      pred = hk.nets.MLP(self._head_mlp_hiddens + (np.prod(pred_spec.shape),))(
          transition_output
      )
      model_outputs[key] = pred.reshape(
          (batch_size, num_actions, *pred_spec.shape)
      )

    return model_outputs

  def model_step(self, embedding: hk.LSTMState) -> dict[str, chex.Array]:
    """Steps model."""
    transition_output = self._model_transition_all_actions(embedding)
    model_outputs = self._model_head_pass(transition_output)
    return model_outputs

  def root_embedding(self, state: chex.Array) -> hk.LSTMState:
    """Constructs a root node from agent's state."""
    flat_state = hk.Flatten()(state)
    cell = hk.Linear(self._lstm_size)(flat_state)
    return hk.LSTMState(hidden=jnp.tanh(cell), cell=cell)


--- FILE: ./github_DiscoRL/disco_rl/networks/meta_nets.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Meta networks used by update rules."""

import functools
from typing import Any, Callable, Mapping, Sequence

import chex
import haiku as hk
from haiku import initializers as hk_init
import jax
from jax import lax
from jax import numpy as jnp

from disco_rl import types
from disco_rl import utils
from disco_rl.update_rules import input_transforms


class MetaNet(hk.Module):
  """Meta Network base class."""

  def __call__(
      self,
      inputs: types.UpdateRuleInputs,
      axis_name: str | None,
  ) -> types.UpdateRuleOuts:
    """Produces outputs needed for update rule's agent loss."""
    raise NotImplementedError


class LSTM(MetaNet):
  """Meta network with LSTMs."""

  def __init__(
      self,
      hidden_size: int,
      embedding_size: Sequence[int],
      prediction_size: int,
      meta_rnn_kwargs: Mapping[str, Any],
      input_option: types.MetaNetInputOption,
      policy_channels: Sequence[int] = (16, 2),
      policy_target_channels: Sequence[int] = (128,),
      policy_target_stddev: float | None = None,
      output_stddev: float | None = None,
      aux_stddev: float | None = None,
      state_stddev: float | None = None,
      name: str | None = None,
  ) -> None:
    super().__init__(name=name)
    self._hidden_size = hidden_size
    self._embedding_size = embedding_size
    self._prediction_size = prediction_size
    self._input_option = input_option
    self._output_init = _maybe_get_initializer(output_stddev)
    self._aux_init = _maybe_get_initializer(aux_stddev)
    self._state_init = _maybe_get_initializer(state_stddev)
    self._policy_target_init = _maybe_get_initializer(policy_target_stddev)
    self._policy_channels = policy_channels
    self._policy_target_channels = policy_target_channels
    self._meta_rnn_core: 'MetaLSTM' = MetaLSTM(
        **meta_rnn_kwargs, input_option=input_option
    )

  def __call__(
      self, inputs: types.UpdateRuleInputs, axis_name: str | None
  ) -> types.UpdateRuleOuts:
    # Initialize or extract the meta RNN core state.
    initial_meta_rnn_state = self._meta_rnn_core.initial_state()
    meta_rnn_state = hk.get_state(
        'meta_rnn_state',
        shape=jax.tree.map(lambda t: t.shape, initial_meta_rnn_state),
        dtype=jax.tree.map(lambda t: t.dtype, initial_meta_rnn_state),
        init=lambda *_: initial_meta_rnn_state,
    )
    assert isinstance(meta_rnn_state, hk.LSTMState)

    # Inputs have shapes of [T+1, B, ...]
    logits = inputs.agent_out['logits']
    assert isinstance(logits, chex.Array)
    _, batch_size, num_actions = logits.shape

    # Construct inputs for the meta network.
    y_net = _batch_mlp(self._embedding_size, num_dims=2)
    z_net = _batch_mlp(self._embedding_size, num_dims=3)
    policy_net = _conv1d_net(self._policy_channels)
    x, policy_emb = _construct_input(
        inputs,
        y_net=y_net,
        z_net=z_net,
        policy_net=policy_net,
        input_option=self._input_option,
        axis_name=axis_name,
    )

    # Unroll the per-trajectory RNN core in reverse direction for bootstrapping.
    per_trajectory_rnn_core = hk.ResetCore(hk.LSTM(self._hidden_size))
    should_reset_bwd = inputs.should_reset_mask_bwd[:-1]  # [T, B]
    x, _ = hk.dynamic_unroll(  # [T, B, H]
        per_trajectory_rnn_core,
        (x, should_reset_bwd),
        per_trajectory_rnn_core.initial_state(batch_size=batch_size),
        reverse=True,
    )

    # Perform multipl-ve interaction with the (per-lifetime) meta RNN's outputs.
    x = _multiplicative_interaction(
        x=x,
        y=self._meta_rnn_core.output(meta_rnn_state),
        initializer=self._state_init,
    )

    # Compute an additional input embedding for the meta network unrolling.
    meta_input_emb = hk.BatchApply(hk.Linear(1, w_init=self._output_init))(
        x
    )  # [T, B, 1]

    # Compute the y, z targets.
    y_hat = hk.BatchApply(
        hk.Linear(self._prediction_size, w_init=self._aux_init)
    )(x)
    z_hat = hk.BatchApply(
        hk.Linear(self._prediction_size, w_init=self._aux_init)
    )(x)

    # Compute the policy target (pi).
    w = jnp.repeat(jnp.expand_dims(x, 2), num_actions, axis=2)  # [T, B, A, H]
    w = jnp.concatenate([w, policy_emb], axis=-1)  # [T, B, A, H+C]
    w = _conv1d_net(self._policy_target_channels)(w)  # [T, B, A, O]
    w = hk.BatchApply(hk.Linear(1, w_init=self._policy_target_init))(
        w
    )  # [T, B, A, 1]
    pi_hat = jnp.squeeze(w, -1)  # [T, B, A]

    # Set the meta network outputs.
    meta_out = dict(pi=pi_hat, y=y_hat, z=z_hat, meta_input_emb=meta_input_emb)

    # Unroll the meta RNN core and update its state.
    new_meta_rnn_state = self._meta_rnn_core.unroll(
        inputs, meta_out, meta_rnn_state, axis_name=axis_name
    )
    hk.set_state('meta_rnn_state', new_meta_rnn_state)

    return meta_out


class MetaLSTM(hk.Module):
  """A meta LSTM that processes trajectories and meta targets throughout the agent's lifetime."""

  def __init__(
      self,
      input_option: types.MetaNetInputOption,
      policy_channels: Sequence[int],
      embedding_size: Sequence[int],
      pred_embedding_size: Sequence[int],
      hidden_size: int,
  ):
    super().__init__()
    self._input_option = input_option
    self._hidden_size = hidden_size
    self._embedding_size = embedding_size
    self._policy_channels = policy_channels
    self._pred_embedding_size = pred_embedding_size
    self._core_constructor = lambda: hk.LSTM(self._hidden_size)

  def unroll(
      self,
      inputs: types.UpdateRuleInputs,
      meta_out: types.UpdateRuleOuts,
      state: hk.LSTMState,
      axis_name: str | None,
  ) -> hk.LSTMState:
    """Updates meta_state given a rollout and a rnn_state."""

    # Get meta inputs.
    y_net = _batch_mlp(self._pred_embedding_size, num_dims=2)
    z_net = _batch_mlp(self._pred_embedding_size, num_dims=3)
    policy_net = _conv1d_net(self._policy_channels)
    meta_inputs, _ = _construct_input(  # [T, B, ...]
        inputs,
        y_net=y_net,
        z_net=z_net,
        policy_net=policy_net,
        input_option=self._input_option,
        axis_name=axis_name,
    )
    input_list = [
        meta_inputs,
        meta_out['meta_input_emb'],
        y_net(jax.nn.softmax(meta_out['y'])),
    ]

    # Concatenate & embed all inputs.
    x = jnp.concatenate(input_list, axis=-1)  # [T, B, ...]
    x = _batch_mlp(self._embedding_size)(x)  # [T, B, E]

    # Apply average pooling over batch-time dimensions.
    x_avg = x.mean(axis=(0, 1))  # [E]
    if axis_name is not None:
      x_avg = jax.lax.pmean(x_avg, axis_name=axis_name)

    # Unroll the meta RNN core and update its state.
    core = self._core_constructor()
    _, new_state = core(x_avg, state)
    return new_state

  def initial_state(self) -> hk.LSTMState:
    """Returns an initial a rnn_state."""
    return self._core_constructor().initial_state(batch_size=None)

  def output(self, state: hk.LSTMState) -> chex.Array:
    """Extracts an output vector from a rnn_state."""
    return state.hidden  # pytype: disable=attribute-error  # numpy-scalars


def _multi_level_extract_by_attr_or_key(x: Any, keys: str) -> Any:
  """Returns `x[k0][k1]...[kn]` where `keys` is of the form `k0[/k1]`."""

  # Note that the keys can also be attributes of `x`.
  # A simple usage example: assert extract({'a': {'b': {'c': 3}}}, 'a/b/c') == 3

  def _get_attr_or_key(x: Any, key: str, keys: str) -> Any:
    if hasattr(x, key):
      return getattr(x, key)
    else:
      try:
        return x[key]
      except:
        raise KeyError(f'Input {x} has no attr or key {key}. {keys}') from None

  processed_keys = []
  for key in keys.split('/'):
    if x is None:
      raise ValueError(
          f'x/{"/".join(processed_keys)} is `None`, cannot recurse up to'
          f' x/{keys}'
      )
    x = _get_attr_or_key(x, key, keys)
    processed_keys.append(key)
  return x


def _construct_input(
    inputs: types.UpdateRuleInputs,
    input_option: types.MetaNetInputOption,
    y_net: Callable[[chex.Array], chex.Array],
    z_net: Callable[[chex.Array], chex.Array],
    policy_net: Callable[[chex.Array], chex.Array],
    axis_name: str | None = None,
) -> tuple[chex.Array, chex.Array | None]:
  """Maps update rule inputs to a single vector."""
  unroll_len, batch_size = inputs.is_terminal.shape

  actions = jax.tree.map(lambda x: x[:-1], inputs.actions)  # [T, B]
  policy = lax.stop_gradient(
      jax.nn.softmax(inputs.agent_out['logits'])
  )  # [T+1, B, A]
  num_actions = policy.shape[2]

  def preprocess_from_config(inputs, preproc_config, prefix_shape):
    inputs_t = []
    for input_config in preproc_config:
      # Extract inputs according to the config.
      x = _multi_level_extract_by_attr_or_key(inputs, input_config.source)

      # Align extra input dims.
      if (
          input_config.source.startswith('extra_from_rule')
          and 'target_out' not in input_config.source
      ) or input_config.source == 'extra_from_rule/target_out/q':
        x = jnp.expand_dims(x, axis=-1)

      # Apply transforms.
      for tx in input_config.transforms:
        if tx == 'y_net':
          x = y_net(x)
        elif tx == 'z_net':
          x = z_net(x)
        else:
          if tx not in input_transforms.TRANSFORMS:
            raise KeyError(
                f'Transform {tx} was not found in {input_config.transforms}.'
            )
          transform_fn = input_transforms.TRANSFORMS[tx]()
          x = transform_fn(x, actions, policy, axis_name)

      # Flatten to a vector.
      x = jnp.reshape(x, (*prefix_shape, -1))
      inputs_t.append(x)
    return inputs_t

  inputs_t = preprocess_from_config(
      inputs, input_option.base, prefix_shape=(unroll_len, batch_size)
  )

  # Get action-conditional inputs, if required.
  if input_option.action_conditional:
    act_cond_inputs = preprocess_from_config(
        inputs,
        input_option.action_conditional,
        prefix_shape=(unroll_len, batch_size, num_actions),
    )
    act_cond_inputs.append(
        jnp.expand_dims(
            jax.nn.one_hot(actions, num_actions, dtype=jnp.float32), axis=-1
        )
    )
    act_cond_inputs = jnp.concatenate(act_cond_inputs, axis=-1)
    act_cond_emb = policy_net(act_cond_inputs)  # [T, B, A, C]
    act_cond_emb_avg = jnp.mean(act_cond_emb, axis=2)  # [T, B, C]
    act_cond_emb_a = utils.batch_lookup(act_cond_emb, actions)  # [T, B, C]
    inputs_t += [act_cond_emb_avg, act_cond_emb_a]
  else:
    act_cond_emb = None

  chex.assert_rank(inputs_t, 3)
  chex.assert_tree_shape_prefix(inputs_t, (unroll_len, batch_size))
  return jnp.concatenate(inputs_t, axis=-1), act_cond_emb


def _maybe_get_initializer(
    stddev: float | None,
) -> hk.initializers.Initializer | None:
  return hk_init.TruncatedNormal(stddev=stddev) if stddev is not None else None


def _multiplicative_interaction(
    x: chex.Array, y: chex.Array, initializer: hk.initializers.Initializer
) -> chex.Array:
  """Returns out = x * Linear(y) if y is not None. Otherwise, returns x."""
  if isinstance(y, chex.Array) and y.shape:  # not scalar
    # Condition on rnn_state via multiplicative interaction.
    y_embed = hk.Linear(x.shape[-1], w_init=initializer)(y)
    return jnp.multiply(x, y_embed)
  else:
    return x


def _batch_mlp(
    hiddens: Sequence[int], num_dims: int = 2
) -> Callable[[chex.Array], chex.Array]:
  return hk.BatchApply(hk.nets.MLP(hiddens), num_dims=num_dims)


def _conv1d_block(x: chex.Array, n_channels: int) -> chex.Array:
  x_avg = jnp.mean(x, axis=2, keepdims=True)  # [T, B, 1, C]
  x_avg = jnp.repeat(x_avg, x.shape[2], axis=2)  # [T, B, A, C]
  x = jnp.concatenate([x, x_avg], axis=-1)  # [T, B, A, 2C]
  x = hk.BatchApply(hk.Conv1D(output_channels=n_channels, kernel_shape=1))(x)
  x = jax.nn.relu(x)  # [T, B, A, C_new]
  return x


def _conv1d_net(channels: Sequence[int]) -> Callable[[chex.Array], chex.Array]:
  return hk.Sequential(
      [functools.partial(_conv1d_block, n_channels=c) for c in channels]
  )


--- FILE: ./github_DiscoRL/disco_rl/networks/nets.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Network factory."""

from collections.abc import Iterable, Mapping
from typing import Any

import chex
import haiku as hk
from haiku import initializers as hk_init
import jax
from jax import numpy as jnp
import numpy as np

from disco_rl import types
from disco_rl.networks import action_models


def get_network(name: str, *args, **kwargs) -> types.PolicyNetwork:
  """Constructs a network."""

  def _get_net():
    if name == 'mlp':
      return MLP(*args, **kwargs)
    else:
      raise ValueError(f'Unknown network: {name}')

  def _agent_step(*call_args, **call_kwargs):
    return _get_net()(*call_args, **call_kwargs)

  def _unroll(*call_args, **call_kwargs):
    return _get_net().unroll(*call_args, **call_kwargs)

  module_init_fn, one_step_fn = hk.without_apply_rng(
      hk.transform_with_state(_agent_step)
  )
  _, unroll_fn = hk.without_apply_rng(hk.transform_with_state(_unroll))

  return types.PolicyNetwork(
      init=module_init_fn,
      one_step=one_step_fn,
      unroll=unroll_fn,
  )


class MLPHeadNet(hk.Module):
  """MLP heads according to the update rules' out_spec."""

  def __init__(
      self,
      out_spec: chex.ArrayTree,
      action_spec: types.Specs,
      head_w_init_std: float | None,
      model_out_spec: chex.ArrayTree | None = None,
      model_arch_name: str | None = None,
      model_kwargs: Mapping[str, Any] | None = None,
      module_name: str | None = None,
  ):
    super().__init__(name=module_name)
    self._out_spec = out_spec
    self._action_spec = action_spec
    if model_out_spec:
      self._model = action_models.get_action_model(
          model_arch_name,
          action_spec=action_spec,
          out_spec=model_out_spec,
          **model_kwargs,
      )
    else:
      self._model = None
    self._head_w_init = (
        hk_init.TruncatedNormal(head_w_init_std) if head_w_init_std else None
    )

  def _embedding_pass(
      self, inputs: chex.ArrayTree, should_reset: chex.Array | None = None
  ) -> chex.Array:
    """Compute embedding from agent inputs."""
    raise NotImplementedError

  def _head_pass(self, embedding: chex.Array) -> dict[str, chex.Array]:
    """Compute outputs as linear functions of embedding."""
    embedding = hk.Flatten()(embedding)

    def _infer(spec: types.ArraySpec) -> chex.Array:
      output = hk.nets.MLP(
          output_sizes=(np.prod(spec.shape),),
          w_init=self._head_w_init,
          name='torso_head',
      )(embedding)
      output = output.reshape((embedding.shape[0], *spec.shape))
      return output

    return jax.tree.map(_infer, self._out_spec)

  def unroll(
      self, inputs: chex.ArrayTree, should_reset: chex.Array | None = None
  ) -> dict[str, chex.Array]:
    """Assumes there is a time dimension in the inputs."""
    return hk.BatchApply(self.__call__)(inputs, should_reset)

  def __call__(
      self, inputs: chex.ArrayTree, should_reset: chex.Array | None = None
  ) -> dict[str, chex.Array]:
    torso = self._embedding_pass(inputs)
    out = self._head_pass(torso)
    if self._model:
      root = self._model.root_embedding(torso)
      model_out = self._model.model_step(root)
      out.update(model_out)
    return out


class MLP(MLPHeadNet):
  """Simple MLP network."""

  def __init__(
      self,
      out_spec: chex.ArrayTree,
      dense: Iterable[int],
      action_spec: types.Specs,
      head_w_init_std: float | None,
      model_out_spec: chex.ArrayTree | None = None,
      model_arch_name: str | None = None,
      model_kwargs: Mapping[str, Any] | None = None,
      module_name: str | None = None,
  ) -> None:
    super().__init__(
        out_spec,
        action_spec=action_spec,
        model_out_spec=model_out_spec,
        head_w_init_std=head_w_init_std,
        model_arch_name=model_arch_name,
        model_kwargs=model_kwargs,
        module_name=module_name,
    )
    self._dense = dense

  def _embedding_pass(
      self, inputs: chex.ArrayTree, should_reset: chex.Array | None = None
  ) -> chex.Array:
    del should_reset
    inputs = [hk.Flatten()(x) for x in jax.tree_util.tree_leaves(inputs)]
    inputs = jnp.concatenate(inputs, axis=-1)
    return hk.nets.MLP(self._dense, name='torso')(inputs)


--- FILE: ./github_DiscoRL/disco_rl/value_fns/value_utils.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Utils for value functions."""

import functools

import chex
import distrax
import jax
import jax.numpy as jnp
from optax import losses
import rlax

from disco_rl import types
from disco_rl import utils


DEFAULT_DISCOUNT = 0.995
DEFAULT_TD_LAMBDA = 0.95


def get_value_outs(
    value_net_out: chex.Array | None,
    q_net_out: chex.ArrayTree | None,
    target_value_net_out: chex.Array | None,
    target_q_net_out: chex.ArrayTree | None,
    rollout: types.ActorRollout | types.UpdateRuleInputs,
    pi_logits: chex.Array,
    discount: jax.typing.ArrayLike,
    lambda_: jax.typing.ArrayLike = DEFAULT_TD_LAMBDA,
    nonlinear_transform: bool = False,
    categorical_value: bool = False,
    max_abs_value: float | None = None,
    drop_last: bool = True,
    adv_ema_state: types.EmaState | chex.ArrayTree | None = None,
    adv_ema_fn: utils.MovingAverage | None = None,
    td_ema_state: types.EmaState | chex.ArrayTree | None = None,
    td_ema_fn: utils.MovingAverage | None = None,
    axis_name: str | None = None,
) -> tuple[types.ValueOuts, types.EmaState | None, types.EmaState | None]:
  """Gets value outs and updates EMA state, handling multi-discount values.

  Calculates V-trace targets when only state-values are given or Retrace
  targets when action-values (or both) are given.

  Args:
    value_net_out: a scalar value or logits, possibly with extra final dim
      corresponding to multiple discount factors. [T, B, 1 or num_bins,
      (num_gamma)]
    q_net_out: a tree of scalar value or logits per action dim, possibly with
      extra final dim corresponding to multiple discount factors. [T, B, A, 1 or
      num_bins, (num_gamma)]
    target_value_net_out: a target value output for bootstrapping. [T, B, 1 or
      num_bins, (num_gamma)]
    target_q_net_out: a target q-value output for bootstrapping. [T, B, A, 1 or
      num_bins, (num_gamma)]
    rollout: rollout from actor or preprocessed for update rule.
    pi_logits: logits of online policy.
    discount: scalar or vector of discount factors.
    lambda_: trace parameter.
    nonlinear_transform: apply signed hyperbolic transform to value if true.
    categorical_value: expect value net outputs are logits if true.
    max_abs_value: the maximum absolute value representable by the categorical
      value function; unused if `categorical_value` is False; must be set when
      `categorical_value` is True.
    drop_last: drop final reward, terminal, action if true.
    adv_ema_state: EMA state of advantages for normalization.
    adv_ema_fn: EMA state for adv normalization.
    td_ema_state: EMA state of TD for normalization.
    td_ema_fn: EMA state for TD normalization.
    axis_name: reduction axis, used for EMA statistics aggregation.

  Returns:
    A tuple of:
      value_outs (containing value and advantage estimates, value loss)
      an updated adv EMA state
      an updated TD EMA state
  """
  if adv_ema_state is not None:
    assert isinstance(adv_ema_state, types.EmaState)  # for pytypes
  if td_ema_state is not None:
    assert isinstance(td_ema_state, types.EmaState)  # for pytypes

  ema_arg_names = ('adv_ema_state', 'adv_ema_fn')
  ema_arg_undef = [name for name in ema_arg_names if name not in locals()]
  if len(ema_arg_undef) not in (0, len(ema_arg_names)):
    raise ValueError(
        f'Received some but not all ema args, undef: {ema_arg_undef}.'
    )

  if value_net_out is None and q_net_out is None:
    raise ValueError('Both value_net and q_value_net are None.')

  if categorical_value and max_abs_value is None:
    raise ValueError(
        'The max absolute value (`max_abs_value`) representable'
        'by the categorical value fn must be set when using the '
        'categorical value fn (`categorical_value` = True).'
    )

  if value_net_out is not None:
    t, b = value_net_out.shape[:2]
  else:
    t, b = jax.tree_util.tree_leaves(q_net_out)[0].shape[:2]

  rewards, actions = rollout.rewards, rollout.actions
  if isinstance(rollout, types.ActorRollout):
    env_discounts, mu_logits = rollout.discounts, rollout.logits
  else:
    assert isinstance(rollout, types.UpdateRuleInputs)
    env_discounts = 1.0 - rollout.is_terminal
    mu_logits = rollout.behaviour_agent_out['logits']

  # Check shapes.
  chex.assert_tree_shape_prefix((pi_logits, actions), (t, b))
  if drop_last:
    chex.assert_tree_shape_prefix((rewards, env_discounts, actions), (t, b))
  else:
    chex.assert_tree_shape_prefix((rewards, env_discounts), (t - 1, b))

  # Extract scalar values from network outputs
  values, q_values = extract_scalar_values_from_net_out(
      value_net_out=value_net_out,
      q_net_out=q_net_out,
      pi_logits=pi_logits,
      categorical_value=categorical_value,
      max_abs_value=max_abs_value,
      nonlinear_transform=nonlinear_transform,
  )

  if target_value_net_out is None and target_q_net_out is None:
    target_values = values
    target_q_values = q_values
  else:
    target_values, target_q_values = extract_scalar_values_from_net_out(
        value_net_out=target_value_net_out,
        q_net_out=target_q_net_out,
        pi_logits=pi_logits,
        categorical_value=categorical_value,
        max_abs_value=max_abs_value,
        nonlinear_transform=nonlinear_transform,
    )

  # Preprocess rollouts.
  if drop_last:
    rewards = rewards[:-1]
    env_discounts = env_discounts[:-1]

  # Always drop the last action (update rule and value fn both get all actions.)
  actions = jax.tree.map(lambda x: x[:-1], actions)

  # Actor rollouts are from mu.
  rho = importance_weight(
      jax.tree.map(lambda x: x[:-1], pi_logits),
      jax.tree.map(lambda x: x[:-1], mu_logits),
      actions,
  )

  # Get value targets and value outs
  if q_values is not None:
    # Estimate state-action values.
    chex.assert_rank(q_values, 3)
    value_outs = estimate_q_values(
        rewards,
        actions,
        env_discounts,
        rho,
        values,
        target_values,
        q_values,
        target_q_values,
        discount,
        lambda_,
    )
  else:
    # Estimate state values, returning dummy q values.
    assert value_net_out is not None
    value_outs = estimate_values(
        rewards,
        actions,
        env_discounts,
        rho,
        values,
        target_values,
        discount,
        lambda_,
    )

  # Calculate normalized advantages.
  advantages = value_outs.adv
  qv_advantages = value_outs.qv_adv
  tds = value_outs.td
  q_tds = value_outs.q_td

  if adv_ema_state is not None and adv_ema_fn is not None:
    new_adv_ema_state = adv_ema_fn.update_state(
        advantages, adv_ema_state, axis_name
    )
    normalized_adv = adv_ema_fn.normalize(advantages, new_adv_ema_state)
    # Traverse only up to actions to avoid going inside adv_to_dict structure.
    normalized_qv_adv = jax.tree.map(
        lambda _, x: adv_ema_fn.normalize(x, new_adv_ema_state),  # pytype: disable=wrong-arg-types
        rollout.actions,
        qv_advantages,
    )
  else:
    new_adv_ema_state = None
    normalized_adv = jax.tree.map(jnp.zeros_like, tds)
    normalized_qv_adv = jax.tree.map(jnp.zeros_like, qv_advantages)

  if td_ema_state is not None and td_ema_fn is not None:
    # Update stats using q_td if q_net is provided. Otherwise, use value td.
    if q_net_out is None:
      # Normalize TD for state values.
      new_td_ema_state = td_ema_fn.update_state(tds, td_ema_state, axis_name)
      normalized_td = td_ema_fn.normalize(
          tds, new_td_ema_state, subtract_mean=False
      )
      normalized_q_td = jax.tree.map(jnp.zeros_like, q_tds)
    else:
      # Normalize TD for state action-values.
      new_td_ema_state = td_ema_fn.update_state(q_tds, td_ema_state, axis_name)
      normalized_q_td = td_ema_fn.normalize(
          q_tds, new_td_ema_state, subtract_mean=False
      )
      normalized_td = jax.tree.map(jnp.zeros_like, tds)
  else:
    new_td_ema_state = None
    normalized_td = jax.tree.map(jnp.zeros_like, tds)
    normalized_q_td = jax.tree.map(jnp.zeros_like, q_tds)

  value_outs.normalized_adv = normalized_adv
  value_outs.normalized_qv_adv = normalized_qv_adv
  value_outs.normalized_td = normalized_td
  value_outs.normalized_q_td = normalized_q_td
  return value_outs, new_adv_ema_state, new_td_ema_state


def estimate_values(
    rewards: chex.Array,
    actions: chex.ArrayTree,
    env_discounts: chex.Array,
    rho: chex.Array,
    values: chex.Array,
    target_values: chex.Array,
    discount: float = DEFAULT_DISCOUNT,
    lambda_: float = DEFAULT_TD_LAMBDA,
) -> types.ValueOuts:
  """Get value estimates from state-values and rollouts with v-trace.

  Args:
    rewards: a [T, B] array with rewards.
    actions: an array tree of [T, B] shapes with actions.
    env_discounts: a [T, B] array with episode termination flags.
    rho: a [T, B] array with importance weights.
    values: a [T+1, B] array with state values.
    target_values: a [T+1, B] array with state target values.
    discount: a scalar discount factor.
    lambda_: trace parameter for v-trace.

  Returns:
    value_outs containing value and (unnorm) advantage estimates
  """
  chex.assert_rank([values, target_values], [2, 2])  # [T, B]
  chex.assert_equal_shape(
      [values[:-1], target_values[:-1], rho, rewards, env_discounts]
  )
  # Vmap for batch dimension.
  batch_vtrace_fn = jax.vmap(
      functools.partial(rlax.vtrace_td_error_and_advantage, lambda_=lambda_),
      in_axes=1,
      out_axes=1,
  )

  discounts = env_discounts * discount
  chex.assert_equal_shape([discounts, values[1:], target_values[1:]])

  vtrace_return = batch_vtrace_fn(
      target_values[:-1],
      target_values[1:],
      rewards,
      discounts,
      rho,
  )
  value_target = vtrace_return.errors + target_values[:-1]

  # Assign dummy qv_adv/q_target due to the lack of Q-values
  dummy_q_value = jax.tree.map(jnp.zeros_like, actions)
  dummy_qv_adv = jax.tree.map(jnp.zeros_like, actions)
  dummy_q_target = jax.tree.map(lambda _: jnp.zeros_like(value_target), actions)
  dummy_q_td = jax.tree.map(lambda _: jnp.zeros_like(value_target), actions)

  value_out = types.ValueOuts(
      adv=vtrace_return.pg_advantage,
      value=values,
      target_value=target_values,
      rho=rho,
      value_target=value_target,
      td=value_target - values[:-1],
      qv_adv=dummy_qv_adv,
      q_target=dummy_q_target,
      q_value=dummy_q_value,
      target_q_value=dummy_q_value,
      q_td=dummy_q_td,
  )

  return value_out


def estimate_q_values(
    rewards: chex.Array,
    actions: chex.ArrayTree,
    env_discounts: chex.Array,
    rho: chex.Array,
    values: chex.Array,
    target_values: chex.Array,
    q_values: chex.ArrayTree,
    target_q_values: chex.ArrayTree,
    discount: float = DEFAULT_DISCOUNT,
    lambda_: float = DEFAULT_TD_LAMBDA,
) -> types.ValueOuts:
  """Get action-value estimates from values and rollouts with Retrace.

  Args:
    rewards: a [T, B] array with rewards.
    actions: an array tree of [T, B] shapes with actions.
    env_discounts: a [T, B] array with episode termination flags.
    rho: a [T, B] array with importance weights.
    values: a [T+1, B] array with state values.
    target_values: a [T+1, B] array with state values.
    q_values: an array tree of [T+1, B, A] with action values.
    target_q_values: an array tree of [T+1, B, A] with target action values.
    discount: a scalar discount factor.
    lambda_: trace parameter for Retrace.

  Returns:
    value_outs containing value and (unnorm) advantage estimates
  """

  chex.assert_rank([values], [2])  # [T, B]
  chex.assert_rank([jax.tree_util.tree_leaves(q_values)[0]], [3])  # [T, B, A]
  chex.assert_equal_shape([values[:-1], rho, rewards, env_discounts])
  chex.assert_equal_shape_prefix(
      (values, jax.tree_util.tree_leaves(q_values)[0]), 2
  )
  q_a = jax.tree.map(lambda x: utils.batch_lookup(x[:-1], actions), q_values)
  target_q_a = jax.tree.map(
      lambda x: utils.batch_lookup(x[:-1], actions), target_q_values
  )

  # Vmap for batch dimension.
  batch_retrace_fn = jax.vmap(
      functools.partial(
          rlax.general_off_policy_returns_from_q_and_v,
          stop_target_gradients=True,
      ),
      in_axes=1,
      out_axes=1,
  )

  discounts = env_discounts * discount
  chex.assert_equal_shape([discounts, values[1:]])

  def _add_first(x):
    return jnp.concatenate([jnp.zeros_like(x[:1]), x], axis=0)

  # Add a dummy first step to format correctly for Retrace.
  r = _add_first(rewards)
  d = _add_first(discounts)
  clipped_rho = jnp.minimum(rho, 1.0)
  lambda_rho = lambda_ * clipped_rho
  c_t = lambda_rho
  q_target = jax.tree.map(
      lambda q: batch_retrace_fn(q, target_values, r, d, c_t),
      target_q_a,
  )

  # Drop the dummy first step.
  q_target = jax.tree.map(lambda x: x[1:], q_target)
  chex.assert_equal_shape([q_a, q_target])

  # Calculate advantage from Q-values
  qv_adv = jax.tree.map(
      lambda x: x - jnp.expand_dims(target_values, axis=2), target_q_values
  )

  v_target = target_values[:-1] + clipped_rho * (
      jax.tree_util.tree_leaves(q_target)[0] - target_values[:-1]
  )
  adv = jax.tree_util.tree_leaves(q_target)[0] - target_values[:-1]

  q_td = jax.tree.map(lambda target, q: target - q, q_target, q_a)

  value_out = types.ValueOuts(
      adv=adv,
      value=values,
      target_value=target_values,
      rho=rho,
      value_target=v_target,
      td=v_target - values[:-1],
      qv_adv=qv_adv,
      q_target=q_target,
      q_value=q_values,
      target_q_value=target_q_values,
      q_td=q_td,
  )

  return value_out


def get_values_from_net_outs(
    x: chex.Array,
    categorical_value: bool,
    max_abs_value: float | None,
    nonlinear_transform: bool,
) -> chex.Array:
  """Extract scalar values from network outputs."""
  chex.assert_rank(x, 3)  # [T, B, 1 or num_bins]
  if categorical_value and max_abs_value is None:
    raise ValueError(
        'The max absolute value (`max_abs_value`) representable'
        'by the categorical value fn must be set when using the '
        'categorical value fn (`categorical_value` = True).'
    )

  if categorical_value:
    v = value_logits_to_scalar(x, max_abs_value)
  else:
    v = jnp.squeeze(x, axis=2)
  if nonlinear_transform:
    return rlax.SIGNED_HYPERBOLIC_PAIR[1](v)
  else:
    return v


def extract_scalar_values_from_net_out(
    value_net_out: chex.Array,
    q_net_out: chex.ArrayTree | None,
    pi_logits: chex.ArrayTree,
    categorical_value: bool,
    max_abs_value: float | None,
    nonlinear_transform: bool,
) -> tuple[chex.Array, chex.ArrayTree | None]:
  """Extract scalar values from network outputs."""
  if categorical_value and max_abs_value is None:
    raise ValueError(
        'The max absolute value (`max_abs_value`) representable'
        'by the categorical value fn must be set when using the '
        'categorical value fn (`categorical_value` = True).'
    )

  get_value_fn = functools.partial(
      get_values_from_net_outs,
      categorical_value=categorical_value,
      max_abs_value=max_abs_value,
      nonlinear_transform=nonlinear_transform,
  )

  if value_net_out is not None:
    values = get_value_fn(value_net_out)
  else:
    values = None

  if q_net_out is not None:
    # vmap over action dims [T, B, "A", ...]
    chex.assert_rank(jax.tree.leaves(q_net_out), 4)
    get_q_values_from_net_outs = jax.vmap(get_value_fn, in_axes=2, out_axes=2)
    q_values = jax.tree.map(get_q_values_from_net_outs, q_net_out)
  else:
    q_values = None

  if values is None:
    # Get state values from Q-values if values are not explicitly given
    pi_tree = jax.tree.map(jax.nn.softmax, pi_logits)
    values = jax.tree.map(
        lambda p, q: jnp.sum(p * q, axis=2), pi_tree, q_values
    )

  return values, q_values


def importance_weight(
    pi_logits: chex.ArrayTree,
    mu_logits: chex.ArrayTree,
    actions: chex.ArrayTree,
) -> chex.Array:
  """Calculate importance weights from logits."""
  log_prob_fn = lambda t, a: distrax.Softmax(t).log_prob(a)
  log_pi_a_tree = jax.tree.map(log_prob_fn, pi_logits, actions)
  log_mu_a_tree = jax.tree.map(log_prob_fn, mu_logits, actions)

  # Joint probs.
  log_pi_a = sum(jax.tree_util.tree_leaves(log_pi_a_tree))
  log_mu_a = sum(jax.tree_util.tree_leaves(log_mu_a_tree))
  rho = jax.lax.stop_gradient(jnp.exp(log_pi_a - log_mu_a))
  return rho


def value_logits_to_scalar(
    value_logits: chex.Array, max_abs_value: float
) -> chex.Array:
  """Converts logits to scalar assuming integer bins centred on zero."""
  if max_abs_value <= 0.0:
    raise ValueError(
        f'Max abs value must be greater than 0: {max_abs_value} <= 0.'
    )
  num_bins = value_logits.shape[-1]
  expected_values = rlax.transform_from_2hot(
      jax.nn.softmax(value_logits),
      min_value=-max_abs_value,
      max_value=max_abs_value,
      num_bins=num_bins,
  )
  return expected_values


def scalar_to_categorical_value(
    num_bins: int, value: chex.Array, max_abs_value: float
) -> chex.Array:
  """Converts scalar to 2-hot probs assuming integer bins centred on zero."""
  if max_abs_value <= 0.0:
    raise ValueError(
        f'Maximum abs value must be greater than 0: {max_abs_value} <= 0.'
    )
  value_probs = rlax.transform_to_2hot(
      value,
      min_value=-max_abs_value,
      max_value=max_abs_value,
      num_bins=num_bins,
  )
  return value_probs


def value_loss_from_target(
    value_net_out: chex.Array,
    value_target: chex.Array,
    nonlinear_transform: bool = False,
    categorical_value: bool = False,
    max_abs_value: float | None = None,
) -> chex.Array:
  """Compute per-step value loss for a scalar per-step target."""
  if categorical_value and max_abs_value is None:
    raise ValueError(
        'The max absolute value (`max_abs_value`) representable'
        'by the categorical value fn must be set when using the '
        'categorical value fn (`categorical_value` = True).'
    )

  # No stop-gradient on target here, add before call if necessary.
  if nonlinear_transform:
    down_fn, _ = rlax.SIGNED_HYPERBOLIC_PAIR
    value_target = down_fn(value_target)

  if categorical_value:
    num_bins = value_net_out.shape[-1]
    value_target_probs = scalar_to_categorical_value(
        num_bins, value_target, max_abs_value
    )
    chex.assert_equal_shape([value_net_out, value_target_probs])
    loss_per_step = losses.softmax_cross_entropy(
        value_net_out, value_target_probs
    )
  else:
    values = jnp.squeeze(value_net_out, axis=-1)
    loss_per_step = 0.5 * jnp.square(values - value_target)
  return loss_per_step


def value_loss_from_td(
    value_net_out: chex.Array,
    td: chex.Array,
    nonlinear_transform: bool = False,
    categorical_value: bool = False,
    max_abs_value: float | None = None,
) -> chex.Array:
  """Compute per-step value loss for a scalar per-step TD."""
  if categorical_value and max_abs_value is None:
    raise ValueError(
        'The max absolute value (`max_abs_value`) representable'
        'by the categorical value fn must be set when using the '
        'categorical value fn (`categorical_value` = True).'
    )

  values = get_values_from_net_outs(
      value_net_out,
      categorical_value=categorical_value,
      max_abs_value=max_abs_value,
      nonlinear_transform=nonlinear_transform,
  )

  chex.assert_rank(values, 2)  # [T, B]
  chex.assert_equal_shape([values, td])  # [T, B]

  # Construct a value target from (potentially normalized) TD.
  # loss = 0.5 * (value - stop_grad[value + TD])^2
  # Equivalently, loss = - value * stop_grad[TD]
  value_target = jax.lax.stop_gradient(values + td)
  return value_loss_from_target(
      value_net_out,
      value_target,
      nonlinear_transform=nonlinear_transform,
      categorical_value=categorical_value,
      max_abs_value=max_abs_value,
  )


--- FILE: ./github_DiscoRL/disco_rl/value_fns/value_fn.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Value function for meta-learning."""

import chex
import haiku as hk
import jax
import jax.numpy as jnp
import optax

from disco_rl import optimizers
from disco_rl import types
from disco_rl import utils
from disco_rl.networks import nets
from disco_rl.value_fns import value_utils


class ValueFunction:
  """Domain value function approximator.

  Used only in meta-training.

  Attributes:
    config: a value function's config.
    axis_name: (optional) an axis name to average over, if runs under `pmap`.
  """

  _value_fn: types.PolicyNetwork
  _value_opt: optax.GradientTransformation

  def __init__(
      self,
      config: types.ValueFnConfig,
      axis_name: str | None,
  ) -> None:
    self.config = config
    self.axis_name = axis_name

    self._discount_factor = config.discount_factor
    self._td_lambda = config.td_lambda
    self._outer_value_cost = config.outer_value_cost

    # Build value function network.
    self._value_fn = nets.get_network(
        config.net,
        out_spec={'value': types.ArraySpec([1], jnp.float32)},
        module_name='value_fn',
        **config.net_args,
    )
    self._value_opt = optax.chain(
        optimizers.scale_by_adam_sg_denom(),
        optax.clip(max_delta=config.max_abs_update),
        optax.scale(-config.learning_rate),
    )

    # Build exponential moving average of advantages.
    self._adv_ema = utils.MovingAverage(
        jnp.zeros(()), decay=config.ema_decay, eps=self.config.ema_eps
    )
    self._td_ema = utils.MovingAverage(
        jnp.zeros(()), decay=config.ema_decay, eps=self.config.ema_eps
    )

  def initial_state(
      self, rng: chex.PRNGKey, dummy_observation: chex.ArrayTree
  ) -> types.ValueState:
    """Initializes a value function state: parameters and optimizer state.

    Args:
      rng: a JAX random number key.
      dummy_observation: dummy observations to use for the networks init.

    Returns:
      A value function state.
    """
    params, state = self._value_fn.init(rng, dummy_observation, None)
    if state:
      raise ValueError(
          'Value functions do not support stateful networks, but the state is'
          ' not empty: {state}.'
      )

    return types.ValueState(
        params=params,
        state=state,
        opt_state=self._value_opt.init(params),
        adv_ema_state=self._adv_ema.init_state(),
        td_ema_state=self._td_ema.init_state(),
    )

  def get_value_outs(
      self,
      value_state: types.ValueState,
      rollout: types.ActorRollout,
      target_logits: chex.Array,
  ) -> tuple[
      types.ValueOuts,
      chex.Array,
      types.EmaState | None,
      types.EmaState | None,
  ]:
    """Runs forward pass of value network and gets value estimates."""
    value_net_outputs, _ = hk.BatchApply(
        lambda x: self._value_fn.one_step(
            value_state.params, value_state.state, x, None
        )
    )(rollout.observations)
    value_net_outputs = value_net_outputs['value']
    value_outs, adv_ema_state, td_ema_state = value_utils.get_value_outs(
        value_net_out=value_net_outputs,
        target_value_net_out=None,
        q_net_out=None,
        target_q_net_out=None,
        rollout=rollout,
        pi_logits=target_logits,
        discount=self._discount_factor,
        lambda_=self._td_lambda,
        nonlinear_transform=True,
        categorical_value=False,
        adv_ema_state=value_state.adv_ema_state,
        adv_ema_fn=self._adv_ema,
        td_ema_state=value_state.td_ema_state,
        td_ema_fn=self._td_ema,
        axis_name=self.axis_name,
    )
    return value_outs, value_net_outputs, adv_ema_state, td_ema_state

  def update(
      self,
      value_state: types.ValueState,
      rollout: types.ActorRollout,
      target_logits: chex.Array,
  ) -> tuple[types.ValueState, types.ValueOuts, types.LogDict]:
    """Updates value function state.

    Args:
      value_state: a value state to update.
      rollout: a rollout to use in the update.
      target_logits: target logits for the given rollout.

    Returns:
      A tuple of an updated state, value outputs, and log.
    """

    def value_loss_fn(v_params, value_state, rollout, target_logits):
      """Compute value functions losses."""
      v_params_no_tracer = value_state.params
      value_state.params = v_params
      value_outs, net_out, adv_ema_state, td_ema_state = self.get_value_outs(
          value_state, rollout, target_logits
      )
      value_losses = value_utils.value_loss_from_td(
          net_out[:-1], jax.lax.stop_gradient(value_outs.normalized_td)
      )
      value_loss = (self._outer_value_cost * value_losses).mean()
      value_state.params = v_params_no_tracer
      return value_loss, (value_outs, adv_ema_state, td_ema_state)

    (value_loss, (value_outs, adv_ema_state, td_ema_state)), dv_dparams = (
        jax.value_and_grad(value_loss_fn, has_aux=True)(
            value_state.params, value_state, rollout, target_logits
        )
    )
    if self.axis_name is not None:
      dv_dparams = jax.lax.pmean(dv_dparams, axis_name=self.axis_name)
    update, new_opt_state = self._value_opt.update(
        dv_dparams, value_state.opt_state, value_state.params
    )

    new_params = optax.apply_updates(value_state.params, update)

    new_state = types.ValueState(
        params=new_params,
        state=value_state.state,
        opt_state=new_opt_state,
        adv_ema_state=adv_ema_state,
        td_ema_state=td_ema_state,
    )

    log = dict(
        value_loss=value_loss,
        value_td=value_outs.td,
        value_normalized_td=value_outs.normalized_td,
    )
    return new_state, value_outs, log


--- FILE: ./github_DiscoRL/disco_rl/value_fns/__init__.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================




--- FILE: ./github_DiscoRL/disco_rl/update_rules/actor_critic.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Actor-critic update rule.

This is an example of the well-known Actor-Critic algorithm implemented as an
update rule.
"""

import chex
import distrax
import jax
from jax import numpy as jnp
import numpy as np

from disco_rl import types
from disco_rl import utils
from disco_rl.update_rules import base
from disco_rl.value_fns import value_utils


class ActorCritic(base.UpdateRule):
  """Various actor-critic baselines."""

  def __init__(
      self,
      categorical_value: bool = False,
      num_bins: int = 601,
      max_abs_value: float = 300.0,
      nonlinear_value_transform: bool = False,
      normalize_adv: bool = False,
      normalize_td: bool = False,
      moving_average_decay: float = 0.99,
      moving_average_eps: float = 1e-6,
  ) -> None:
    """Configure the actor-critic module.

    Args:
      categorical_value: use categorical value function if true.
      num_bins: number of bins for categorical value. This is ignored when
        categorical_value is false.
      max_abs_value: maximum absolute value representable by the categorical
        value. This is ignored when categorical_value is False; must be set when
        categorical_value is True.
      nonlinear_value_transform: apply non-linear transform to value if true.
      normalize_adv: normalize advantange for policy gradient if true.
      normalize_td: normalize TD for value loss if true.
      moving_average_decay: moving average decay for advantage and TD.
      moving_average_eps: moving average epsilon for advantage and TD.
    """
    if normalize_td and categorical_value:
      raise ValueError(
          'normalize_td and categorical_value should not be used together.'
      )
    self._normalize_adv = normalize_adv
    self._normalize_td = normalize_td
    self._num_bins = num_bins
    self._max_abs_value = max_abs_value
    self._categorical_value = categorical_value
    self._nonlinear_value_transform = nonlinear_value_transform

    # Moving average advantage.
    self._adv_ema = utils.MovingAverage(
        np.zeros(()), decay=moving_average_decay, eps=moving_average_eps
    )
    self._td_ema = utils.MovingAverage(
        np.zeros(()), decay=moving_average_decay, eps=moving_average_eps
    )

  def init_params(
      self, rng: chex.PRNGKey
  ) -> tuple[types.MetaParams, chex.ArrayTree]:
    del rng
    return {'dummy': jnp.array(0.0)}, {}

  def flat_output_spec(
      self, single_action_spec: types.ActionSpec
  ) -> types.Specs:
    """Returns the agent's unconditional output specs.

    Args:
      single_action_spec: An action spec.

    Returns:
      A nested dict with tuples specifying output specs.
    """
    return dict(
        logits=utils.get_logits_specs(single_action_spec),
        v=types.ArraySpec(
            (self._num_bins if self._categorical_value else 1,), np.float32
        ),
    )

  def model_output_spec(
      self, single_action_spec: types.ActionSpec
  ) -> types.Specs:
    """Returns the agent's action-conditional output specs.

    Args:
      single_action_spec: An action spec.

    Returns:
      A nested dict with tuples specifying model output specs.
    """
    del single_action_spec
    return dict()

  def init_meta_state(
      self,
      rng: chex.PRNGKey,
      params: types.AgentParams,
  ) -> types.MetaState:
    del rng
    meta_state = dict()
    meta_state['adv_ema_state'] = self._adv_ema.init_state()
    meta_state['td_ema_state'] = self._td_ema.init_state()
    return meta_state

  def unroll_meta_net(
      self,
      meta_params: types.MetaParams,
      params: types.AgentParams,
      state: types.HaikuState,
      meta_state: types.MetaState,
      rollout: types.UpdateRuleInputs,
      hyper_params: types.HyperParams,
      unroll_policy_fn: types.AgentUnrollFn,
      rng: chex.PRNGKey,
      axis_name: str | None = None,
  ) -> tuple[types.UpdateRuleOuts, types.MetaState]:
    """Prepare quantities for the loss (no meta network)."""
    del meta_params

    value_outs, adv_ema_state, td_ema_state = value_utils.get_value_outs(
        value_net_out=rollout.agent_out['v'],
        target_value_net_out=None,
        q_net_out=None,
        target_q_net_out=None,
        rollout=rollout,
        pi_logits=rollout.agent_out['logits'],
        discount=hyper_params['discount_factor'],
        lambda_=hyper_params['vtrace_lambda'],
        nonlinear_transform=self._nonlinear_value_transform,
        categorical_value=self._categorical_value,
        max_abs_value=self._max_abs_value,
        drop_last=False,
        adv_ema_state=meta_state['adv_ema_state'],
        adv_ema_fn=self._adv_ema,
        td_ema_state=meta_state['td_ema_state'],
        td_ema_fn=self._td_ema,
        axis_name=axis_name,
    )

    meta_out = dict(
        raw_advs=value_outs.adv,
        normalized_advs=value_outs.normalized_adv,
        value_target=value_outs.value_target,
        values=value_outs.value,
        normalized_tds=value_outs.normalized_td,
        tds=value_outs.td,
    )
    new_meta_state = meta_state
    meta_state['adv_ema_state'] = adv_ema_state
    meta_state['td_ema_state'] = td_ema_state

    return meta_out, new_meta_state

  def agent_loss(
      self,
      rollout: types.UpdateRuleInputs,
      meta_out: types.UpdateRuleOuts,
      hyper_params: types.HyperParams,
      backprop: bool,
  ) -> tuple[chex.Array, types.UpdateRuleLog]:
    """Construct policy and value loss."""
    del backprop
    actions = rollout.actions[:-1]
    logits = rollout.agent_out['logits'][:-1]

    pg_advs = (
        meta_out['normalized_advs']
        if self._normalize_adv
        else meta_out['raw_advs']
    )
    value_tds = (
        meta_out['normalized_tds'] if self._normalize_td else meta_out['tds']
    )

    value_loss_per_step = value_utils.value_loss_from_td(
        rollout.agent_out['v'][:-1],
        jax.lax.stop_gradient(value_tds),
        nonlinear_transform=self._nonlinear_value_transform,
        categorical_value=self._categorical_value,
        max_abs_value=self._max_abs_value,
    )

    # Entropy loss.
    entropy_loss_per_step = -distrax.Softmax(logits).entropy()

    # PG loss: R * log(prod(p_i)) = R * Sum(log(p_i)).
    pg_loss_per_step = utils.differentiable_policy_gradient_loss(
        logits, actions, adv_t=pg_advs, backprop=False
    )

    # Compute total loss.
    chex.assert_rank(  # [T, B]
        (pg_loss_per_step, value_loss_per_step, entropy_loss_per_step), 2
    )
    total_loss_per_step = (
        hyper_params['pg_cost'] * pg_loss_per_step
        + hyper_params['value_cost'] * value_loss_per_step
        + hyper_params['entropy_cost'] * entropy_loss_per_step
    )

    logs = dict(
        logits=jnp.mean(logits),
        entropy=-jnp.mean(entropy_loss_per_step),
        pg_advs=jnp.mean(pg_advs),
        raw_advs=jnp.mean(meta_out['raw_advs']),
        avg_value=jnp.mean(meta_out['values']),
    )

    return total_loss_per_step, logs


--- FILE: ./github_DiscoRL/disco_rl/update_rules/disco.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""DiscoRL update rule."""

import chex
import distrax
import haiku as hk
import jax
import jax.numpy as jnp
from ml_collections import config_dict
import rlax

from disco_rl import types
from disco_rl import utils
from disco_rl.networks import meta_nets
from disco_rl.update_rules import base
from disco_rl.value_fns import value_utils


class DiscoUpdateRule(base.UpdateRule):
  """Discoved update rule, as described in the paper."""

  def __init__(
      self,
      net: config_dict.ConfigDict,
      value_discount: float,
      max_abs_value: float,
      num_bins: int,
      moving_average_decay: float = 0.99,
      moving_average_eps: float = 1e-6,
  ) -> None:
    """The meta-network constructor."""
    super().__init__()
    self._prediction_size = net.prediction_size
    self._value_discount = value_discount
    self._max_abs_value = max_abs_value
    self._num_bins = num_bins

    # Exponential moving averages.
    self._adv_ema = utils.MovingAverage(
        jnp.zeros(()), decay=moving_average_decay, eps=moving_average_eps
    )
    self._td_ema = utils.MovingAverage(
        jnp.zeros(()), decay=moving_average_decay, eps=moving_average_eps
    )

    # Meta-network.
    def meta_net_fn(*args, **kwargs):
      if net['name'] == 'lstm':
        return meta_nets.LSTM(**net)(*args, **kwargs)
      else:
        raise ValueError(f'Invalid model network name: {net["name"]}.')

    self._eta_init_fn, self._eta_apply = hk.transform_with_state(meta_net_fn)

  def init_params(
      self, rng: chex.PRNGKey
  ) -> tuple[types.MetaParams, chex.ArrayTree]:
    dummy_input = self._get_dummy_input(
        include_behaviour_out=True, include_agent_adv=True
    )
    meta_params, meta_rnn_state = self._eta_init_fn(
        rng, dummy_input, axis_name=None
    )
    return meta_params, meta_rnn_state

  def init_meta_state(
      self,
      rng: chex.PRNGKey,
      params: types.AgentParams,
  ) -> types.MetaState:
    """Create meta_state."""
    _, meta_rnn_state = self.init_params(rng)
    meta_state = dict(
        rnn_state=meta_rnn_state,
        adv_ema_state=self._adv_ema.init_state(),
        td_ema_state=self._td_ema.init_state(),
        target_params=params,
    )
    return meta_state

  def flat_output_spec(
      self, single_action_spec: types.ActionSpec
  ) -> types.Specs:
    return dict(
        logits=utils.get_logits_specs(single_action_spec),
        y=types.ArraySpec((self._prediction_size,), jnp.float32),
    )

  def model_output_spec(
      self, single_action_spec: types.ActionSpec
  ) -> types.Specs:
    return dict(
        z=types.ArraySpec((self._prediction_size,), jnp.float32),
        aux_pi=utils.get_logits_specs(single_action_spec),
        q=types.ArraySpec((self._num_bins,), jnp.float32),
    )

  def unroll_meta_net(
      self,
      meta_params: types.MetaParams,
      params: types.AgentParams,
      state: types.HaikuState,
      meta_state: types.MetaState,
      rollout: types.UpdateRuleInputs,
      hyper_params: types.HyperParams,
      unroll_policy_fn: types.AgentUnrollFn,
      rng: chex.PRNGKey,
      axis_name: str | None = None,
  ) -> tuple[types.UpdateRuleOuts, types.MetaState]:
    del rng
    t, b = rollout.rewards.shape

    chex.assert_shape((rollout.rewards, rollout.is_terminal), (t, b))
    chex.assert_tree_shape_prefix(
        (rollout.agent_out, rollout.actions), (t + 1, b)
    )

    # Unroll the target policy.
    target_out, _ = unroll_policy_fn(
        meta_state['target_params'],
        state,
        rollout.observations,
        rollout.should_reset_mask_fwd,
    )

    # TD-value targets.
    value_outs, adv_ema_state, td_ema_state = value_utils.get_value_outs(
        value_net_out=None,
        target_value_net_out=None,
        q_net_out=rollout.agent_out['q'],
        target_q_net_out=target_out['q'],
        rollout=rollout,
        pi_logits=rollout.agent_out['logits'],
        discount=self._value_discount,
        lambda_=hyper_params['value_fn_td_lambda'],
        nonlinear_transform=True,
        categorical_value=True,
        max_abs_value=self._max_abs_value,
        drop_last=False,
        adv_ema_state=meta_state['adv_ema_state'],
        adv_ema_fn=self._adv_ema,
        td_ema_state=meta_state['td_ema_state'],
        td_ema_fn=self._td_ema,
        axis_name=axis_name,
    )

    # Apply the meta-network.
    rollout.extra_from_rule = dict(
        v_scalar=value_outs.value,
        adv=value_outs.adv,
        normalized_adv=value_outs.normalized_adv,
        q=value_outs.target_q_value,
        qv_adv=value_outs.qv_adv,
        normalized_qv_adv=value_outs.normalized_qv_adv,
        target_out=target_out,
    )
    meta_out, new_rnn_state = self._eta_apply(
        meta_params,
        meta_state['rnn_state'],
        None,  # unused rng
        rollout,
        axis_name=axis_name,
    )
    chex.assert_rank(meta_out['pi'], 3)  # [T, B, A]
    chex.assert_rank(meta_out['y'], 3)  # [T, B, Y]
    chex.assert_rank(meta_out['z'], 3)  # [T, B, Y]

    # Enrich the meta-net's outputs with the value function's outputs.
    meta_out['q_target'] = value_outs.q_target
    meta_out['adv'] = value_outs.adv
    meta_out['normalized_adv'] = value_outs.normalized_adv
    meta_out['qv_adv'] = value_outs.qv_adv
    meta_out['normalized_qv_adv'] = value_outs.normalized_qv_adv
    meta_out['q_value'] = value_outs.q_value
    meta_out['q_td'] = value_outs.q_td
    meta_out['normalized_q_td'] = value_outs.normalized_q_td
    meta_out['target_out'] = target_out

    # Update the meta state.
    new_meta_state = meta_state | dict(
        rnn_state=new_rnn_state,
        adv_ema_state=adv_ema_state,
        td_ema_state=td_ema_state,
    )

    # Update target params.
    coeff = hyper_params['target_params_coeff']
    new_meta_state['target_params'] = jax.tree.map(
        lambda old, new: old * coeff + (1.0 - coeff) * new,
        meta_state['target_params'],
        params,
    )

    return meta_out, new_meta_state

  def agent_loss(
      self,
      rollout: types.UpdateRuleInputs,
      meta_out: types.UpdateRuleOuts,
      hyper_params: types.HyperParams,
      backprop: bool,
  ) -> tuple[chex.Array, types.UpdateRuleLog]:
    """Defines an agent loss."""
    t, b = rollout.rewards.shape

    chex.assert_shape((rollout.rewards, rollout.is_terminal), (t, b))
    chex.assert_tree_shape_prefix(
        (rollout.agent_out, rollout.actions), (t + 1, b)
    )

    # Parse the agent's output.
    agent_out, actions = jax.tree.map(
        lambda x: x[:-1], (rollout.agent_out, rollout.actions)
    )
    logits = agent_out['logits']
    y = agent_out['y']
    z = agent_out['z']
    z_a = utils.batch_lookup(agent_out['z'], actions)

    # Parse the meta-net's output.
    pi_hat = meta_out['pi']
    y_hat = meta_out['y']
    z_hat = meta_out['z']
    if not backprop:
      pi_hat, y_hat, z_hat = jax.lax.stop_gradient((pi_hat, y_hat, z_hat))

    # Compute losses.
    chex.assert_equal_shape([pi_hat, logits])  # [T, B, A]
    chex.assert_equal_shape([y_hat, y])  # [T, B, Y]
    chex.assert_equal_shape([z_hat, z_a])  # [T, B, Z]
    pi_loss_per_step = rlax.categorical_kl_divergence(pi_hat, logits)
    y_loss_per_step = rlax.categorical_kl_divergence(y_hat, y)
    z_loss_per_step = rlax.categorical_kl_divergence(z_hat, z_a)

    # Compute auxiliary 1-step policy prediction loss.
    aux_pi = rollout.agent_out['aux_pi'][:-1]  # [T, B, A, A]
    aux_pi_a = utils.batch_lookup(aux_pi, actions)  # [T, B, A]
    aux_policy_target = rollout.agent_out['logits'][1:]  # [T, B, A]
    aux_policy_loss_per_step = rlax.categorical_kl_divergence(
        jax.lax.stop_gradient(aux_policy_target), aux_pi_a
    )
    # Mask out terminal states.
    aux_policy_loss_per_step *= 1.0 - rollout.is_terminal

    # Compute total loss.
    chex.assert_shape(
        (
            pi_loss_per_step,
            y_loss_per_step,
            z_loss_per_step,
            aux_policy_loss_per_step,
        ),
        (t, b),  # [T, B]
    )
    total_loss_per_step = (
        hyper_params['pi_cost'] * pi_loss_per_step
        + hyper_params['y_cost'] * y_loss_per_step
        + hyper_params['z_cost'] * z_loss_per_step
        + hyper_params['aux_policy_cost'] * aux_policy_loss_per_step
    )

    log = dict(
        logits=jnp.mean(logits),
        y=jnp.mean(y),
        z=jnp.mean(z),
        entropy=jnp.mean(distrax.Softmax(logits).entropy()),
        y_entropy=jnp.mean(distrax.Softmax(y).entropy()),
        z_entropy=jnp.mean(distrax.Softmax(z_a).entropy()),
        pi_loss=jnp.mean(pi_loss_per_step),
        aux_kl_loss=jnp.mean(aux_policy_loss_per_step),
        pi_hat=jnp.mean(pi_hat),
        y_hat=jnp.mean(y_hat),
        z_hat=jnp.mean(z_hat),
        pi_hat_entropy=jnp.mean(distrax.Softmax(pi_hat).entropy()),
        aux_policy_entropy=jnp.mean(distrax.Softmax(aux_pi_a).entropy()),
        aux_target_entropy=jnp.mean(
            distrax.Softmax(aux_policy_target).entropy()
        ),
    )
    return total_loss_per_step, log

  def agent_loss_no_meta(
      self,
      rollout: types.UpdateRuleInputs,
      meta_out: types.UpdateRuleOuts | None,
      hyper_params: types.HyperParams,
  ) -> tuple[chex.Array, types.UpdateRuleLog]:
    """Value losses that do not interfere with meta-gradient."""
    assert meta_out is not None
    td = meta_out['q_td']

    q_a = utils.batch_lookup(rollout.agent_out['q'], rollout.actions)[:-1]
    value_loss_per_step = value_utils.value_loss_from_td(
        value_net_out=q_a,
        td=jax.lax.stop_gradient(td),
        nonlinear_transform=True,
        categorical_value=True,
        max_abs_value=self._max_abs_value,
    )
    loss_per_step = value_loss_per_step * hyper_params['value_cost']

    log = dict(
        q_target=jnp.mean(meta_out['q_target']),
        agent_q_mean=jnp.mean(meta_out['q_value']),
        q_loss=jnp.mean(value_loss_per_step),
        td=jnp.mean(meta_out['q_td']),
        normalized_td=jnp.mean(meta_out['normalized_q_td']),
    )
    return loss_per_step, log


def get_input_option() -> types.MetaNetInputOption:
  """Returns the input option for the meta-network.

  Detailed description can be found in the paper.

  Returns:
    The input option for the meta-network.
  """
  return types.MetaNetInputOption(
      base=(
          types.TransformConfig(
              source='agent_out/logits',
              transforms=('drop_last', 'softmax', 'stop_grad', 'select_a'),
          ),
          types.TransformConfig(
              source='behaviour_agent_out/logits',
              transforms=('drop_last', 'softmax', 'stop_grad', 'select_a'),
          ),
          types.TransformConfig(source='rewards', transforms=('sign_log',)),
          types.TransformConfig(
              source='is_terminal',
              transforms=('masks_to_discounts',),
          ),
          types.TransformConfig(
              source='extra_from_rule/v_scalar',
              transforms=('sign_log', 'td_pair', 'stop_grad'),
          ),
          types.TransformConfig(
              source='extra_from_rule/adv', transforms=('sign_log', 'stop_grad')
          ),
          types.TransformConfig(
              source='extra_from_rule/normalized_adv', transforms=('stop_grad',)
          ),
          types.TransformConfig(
              source='extra_from_rule/target_out/logits',
              transforms=('drop_last', 'softmax', 'stop_grad', 'select_a'),
          ),
          types.TransformConfig(
              source='agent_out/y', transforms=('softmax', 'y_net', 'td_pair')
          ),
          types.TransformConfig(
              source='extra_from_rule/target_out/y',
              transforms=('softmax', 'y_net', 'td_pair'),
          ),
          types.TransformConfig(
              source='agent_out/z',
              transforms=('drop_last', 'softmax', 'z_net', 'select_a'),
          ),
          types.TransformConfig(
              source='agent_out/z',
              transforms=('softmax', 'z_net', 'pi_weighted_avg', 'td_pair'),
          ),
          types.TransformConfig(
              source='agent_out/z',
              transforms=('softmax', 'z_net', 'max_a', 'td_pair'),
          ),
          types.TransformConfig(
              source='extra_from_rule/target_out/z',
              transforms=('drop_last', 'softmax', 'z_net', 'select_a'),
          ),
          types.TransformConfig(
              source='extra_from_rule/target_out/z',
              transforms=('softmax', 'z_net', 'pi_weighted_avg', 'td_pair'),
          ),
          types.TransformConfig(
              source='extra_from_rule/target_out/z',
              transforms=('softmax', 'z_net', 'max_a', 'td_pair'),
          ),
      ),
      action_conditional=(
          types.TransformConfig(
              source='agent_out/logits',
              transforms=('drop_last', 'softmax', 'stop_grad'),
          ),
          types.TransformConfig(
              source='behaviour_agent_out/logits',
              transforms=('drop_last', 'softmax', 'stop_grad'),
          ),
          types.TransformConfig(
              source='extra_from_rule/target_out/logits',
              transforms=('drop_last', 'softmax', 'stop_grad'),
          ),
          types.TransformConfig(
              source='agent_out/z', transforms=('drop_last', 'softmax', 'z_net')
          ),
          types.TransformConfig(
              source='extra_from_rule/target_out/z',
              transforms=('drop_last', 'softmax', 'z_net'),
          ),
          types.TransformConfig(
              source='extra_from_rule/q',
              transforms=('sign_log', 'drop_last', 'stop_grad'),
          ),
          types.TransformConfig(
              source='extra_from_rule/qv_adv',
              transforms=('sign_log', 'drop_last', 'stop_grad'),
          ),
          types.TransformConfig(
              source='extra_from_rule/normalized_qv_adv',
              transforms=('drop_last', 'stop_grad'),
          ),
      ),
  )


--- FILE: ./github_DiscoRL/disco_rl/update_rules/__init__.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================




--- FILE: ./github_DiscoRL/disco_rl/update_rules/policy_gradient.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Policy gradient update rule.

This is an example of the well-known Policy Gradient algorithm implemented
as an update rule.
"""

import chex
import distrax
from jax import numpy as jnp

from disco_rl import types
from disco_rl import utils
from disco_rl.update_rules import base


class PolicyGradientUpdate(base.UpdateRule):
  """Policy gradient update rule (uses value functions)."""

  def __init__(
      self,
      entropy_cost: float = 0.01,
      normalize_adv: bool = True,
      pg_cost: float = 1,
      kl_prior_cost: float = 0.5,
      p_actor_prior: float = 0.03,
      p_uniform_prior: float = 0.003,
      target_params_coeff: float = 0.1,
  ) -> None:
    """Init."""
    self._normalize_adv = normalize_adv
    self._target_params_coeff = target_params_coeff
    self._pg_cost = pg_cost
    self._entropy_cost = entropy_cost
    self._kl_prior_cost = kl_prior_cost
    self._p_actor_prior = p_actor_prior
    self._p_uniform_prior = p_uniform_prior

  def init_params(
      self, rng: chex.PRNGKey
  ) -> tuple[types.MetaParams, chex.ArrayTree]:
    del rng
    return {'dummy': jnp.array(0.0)}, {}

  def flat_output_spec(
      self, single_action_spec: types.ActionSpec
  ) -> types.Specs:
    return dict(logits=utils.get_logits_specs(single_action_spec))

  def model_output_spec(
      self, single_action_spec: types.ActionSpec
  ) -> types.Specs:
    del single_action_spec
    return dict()

  def init_meta_state(
      self,
      rng: chex.PRNGKey,
      params: types.AgentParams,
  ) -> types.MetaState:
    del rng
    return dict()

  def unroll_meta_net(
      self,
      meta_params: types.MetaParams,
      params: types.AgentParams,
      state: types.HaikuState,
      meta_state: types.MetaState,
      rollout: types.UpdateRuleInputs,
      hyper_params: types.HyperParams,
      unroll_policy_fn: types.AgentUnrollFn,
      rng: chex.PRNGKey,
      axis_name: str | None = None,
  ) -> tuple[types.UpdateRuleOuts, types.MetaState]:
    """Prepare quantities for the loss (no meta network)."""
    del meta_params, hyper_params, axis_name, rng
    assert rollout.value_out is not None
    pg_adv = (
        rollout.value_out.normalized_adv
        if self._normalize_adv
        else rollout.value_out.adv
    )
    return dict(pi=pg_adv), meta_state

  def agent_loss(
      self,
      rollout: types.UpdateRuleInputs,
      meta_out: types.UpdateRuleOuts,
      hyper_params: types.HyperParams,
      backprop: bool,
  ) -> tuple[chex.Array, types.UpdateRuleLog]:
    """Construct policy and value loss."""
    del backprop, hyper_params
    actions = rollout.actions[:-1]
    logits = rollout.agent_out['logits'][:-1]

    pg_loss_per_step = utils.differentiable_policy_gradient_loss(
        logits, actions, adv_t=meta_out['pi'], backprop=False
    )
    entropy_loss_per_step = -distrax.Softmax(logits).entropy()

    # Compute total loss.
    chex.assert_rank((pg_loss_per_step, entropy_loss_per_step), 2)  # [T, B]
    total_loss_per_step = (
        self._pg_cost * pg_loss_per_step
        + self._entropy_cost * entropy_loss_per_step
    )

    log = dict(
        entropy=-jnp.mean(entropy_loss_per_step),
        pg_advs=jnp.mean(meta_out['pi']),
    )
    return total_loss_per_step, log


--- FILE: ./github_DiscoRL/disco_rl/update_rules/input_transforms.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Transformations of meta-network inputs."""

import functools
from typing import Callable, Sequence

import chex
import haiku as hk
import immutabledict
import jax
import jax.numpy as jnp
import rlax

from disco_rl import utils


class InputTransform:

  def __call__(
      self, x, actions: chex.Array, policy: chex.Array, axis: str | None
  ) -> chex.Array:
    raise NotImplementedError


class SelectByAction(InputTransform):

  def __call__(self, x, actions, policy, axis):
    del policy, axis
    chex.assert_rank(actions, 2)
    chex.assert_tree_shape_prefix([x, actions], actions.shape)
    return utils.batch_lookup(x, actions)


class PiWeightedAvg(InputTransform):

  def __call__(self, x, actions, policy, axis):
    del actions, axis
    chex.assert_rank(x, 4)
    chex.assert_rank(policy, 3)
    chex.assert_tree_shape_prefix([x, policy], policy.shape)
    return jnp.sum(x * jnp.expand_dims(policy, -1), axis=2)


class Normalize(InputTransform, hk.Module):

  def __call__(self, x, actions, policy, axis):
    del actions, policy
    assert x.ndim >= 2  # Average over B, T
    return EmaNorm(
        decay_rate=0.99, eps=1e-6, axis=(0, 1), cross_replica_axis=axis
    )(x)


class EmaNorm(hk.Module):
  """Normalize by EMA estimate of mean and variance."""

  def __init__(
      self,
      decay_rate: float,
      eps: float = 1e-6,
      eps_root: float = 1e-12,
      axis: Sequence[int] | None = None,
      cross_replica_axis: str | Sequence[str] | None = None,
      cross_replica_axis_index_groups: Sequence[Sequence[int]] | None = None,
      data_format: str = 'channels_last',
      name: str | None = None,
  ):
    """Constructs an EmaNorm module. Based on hk.BatchNorm.

    Args:
      decay_rate: Decay rate for EMA.
      eps: Small epsilon to avoid division by zero variance. Defaults ``1e-6``.
      eps_root: Small epsilon to assist metagrad stability. Defaults ``1e-12``.
      axis: Which axes to reduce over. The default (``None``) signifies that all
        but the channel axis should be normalized. Otherwise this is a list of
        axis indices which will have normalization statistics calculated.
      cross_replica_axis: If not ``None``, it should be a string (or sequence of
        strings) representing the axis name(s) over which this module is being
        run within a jax map (e.g. ``jax.pmap`` or ``jax.vmap``). Supplying this
        argument means that batch statistics are calculated across all replicas
        on the named axes.
      cross_replica_axis_index_groups: Specifies how devices are grouped. Valid
        only within ``jax.pmap`` collectives.
      data_format: The data format of the input. Can be either
        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By
        default it is ``channels_last``. See :func:`get_channel_index`.
      name: The module name.
    """
    super().__init__(name=name)

    self.eps = eps
    self.eps_root = eps_root
    self.axis = axis
    self.cross_replica_axis = cross_replica_axis
    self.cross_replica_axis_index_groups = cross_replica_axis_index_groups
    self.channel_index = hk.get_channel_index(data_format)
    self.m1_ema = hk.ExponentialMovingAverage(decay_rate, name='m1_ema')
    self.m2_ema = hk.ExponentialMovingAverage(decay_rate, name='m2_ema')

  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:
    """Updates EMA state (always in `train` mode), returns normalized input."""

    channel_index = self.channel_index
    if channel_index < 0:
      channel_index += inputs.ndim

    if self.axis is not None:
      axis = self.axis
    else:
      axis = [i for i in range(inputs.ndim) if i != channel_index]

    mean = jnp.mean(inputs, axis, keepdims=True)
    mean_of_squares = jnp.mean(jnp.square(inputs), axis, keepdims=True)

    if self.cross_replica_axis:
      mean = jax.lax.pmean(
          mean,
          axis_name=self.cross_replica_axis,
          axis_index_groups=self.cross_replica_axis_index_groups,
      )
      mean_of_squares = jax.lax.pmean(
          mean_of_squares,
          axis_name=self.cross_replica_axis,
          axis_index_groups=self.cross_replica_axis_index_groups,
      )
    self.m1_ema(mean)
    self.m2_ema(mean_of_squares)

    ema_m1 = self.m1_ema.average.astype(inputs.dtype)
    ema_m2 = self.m2_ema.average.astype(inputs.dtype)

    ema_var = jnp.maximum(ema_m2 - jnp.square(ema_m1), 0.0)

    eps = jax.lax.convert_element_type(self.eps, ema_var.dtype)
    eps_root = jax.lax.convert_element_type(self.eps_root, ema_var.dtype)
    return (inputs - ema_m1) / (jnp.sqrt(ema_var + eps_root) + eps)


def td_pair(x):
  # Concat inputs at t and t+1 to ease calculation of TD-error-like quantities
  return jnp.concatenate([x[:-1], x[1:]], axis=-1)


def tx_factory(tx_call: Callable[[chex.Array], chex.Array]):
  """Wraps single-arg transforms so they all have the same interface."""

  # The dummy and wrap allows the fn to be called with the same interface as the
  # modules: tx()(x, actions, policy, axis)
  def dummy_builder():
    def _wrap_transform(x, actions, policy, axis):
      del actions, policy, axis
      return tx_call(x)

    return _wrap_transform

  return dummy_builder


_TRANSFORM_FNS = immutabledict.immutabledict({
    'identity': lambda x: x,
    'softmax': jax.nn.softmax,
    'max_a': functools.partial(jnp.max, axis=2),
    'stop_grad': jax.lax.stop_gradient,
    'clip': functools.partial(jnp.clip, a_min=-2.0, a_max=2.0),
    'sign': jnp.sign,
    'drop_last': lambda x: x[:-1],
    'td_pair': td_pair,
    'sign_log': rlax.signed_logp1,
    'sign_hyp': rlax.signed_hyperbolic,
    'masks_to_discounts': lambda x: 1.0 - x,
})

TRANSFORMS = immutabledict.immutabledict({
    'select_a': SelectByAction,
    'pi_weighted_avg': PiWeightedAvg,
    'normalize': Normalize,
    **{name: tx_factory(tx) for name, tx in _TRANSFORM_FNS.items()},
})


--- FILE: ./github_DiscoRL/disco_rl/update_rules/base.py ---

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Base class for update rules. See `UpdateRule`'s docstrings."""

import chex
from dm_env import specs as dm_env_specs
import jax
import jax.numpy as jnp

from disco_rl import types
from disco_rl import utils

ArraySpec = types.ArraySpec


def get_agent_out_spec(
    action_spec: types.ActionSpec,
    flat_out_spec: types.Specs,
    model_out_spec: types.Specs,
) -> types.Specs:
  """Constructs a tree of shapes of outputs for the provided specs.

  Example:
      >> action_spec = specs.BoundedArray((), int, minimum=0, maximum=A-1)
      >> unconditional_output_spec = {'logits': (A,),
                                      'y': (Y,),
                                      }
      >> conditional_output_spec = {'z': (Z,),
                                    'aux_pi': (P,),
                                    }
      >> agent_output_spec = {
            'logits': (A,),
            'y': (Y,),

            'z': (A, Z),
            'aux_pi': (A, P),
         }

  Args:
      action_spec: An action spec.
      flat_out_spec: A nested dict of agent's flat output shapes.
      model_out_spec: A nested dict of agent's model output shapes.

  Returns:
      A nested dict with shapes specifying agent's total output shapes.
  """
  if set(flat_out_spec.keys()).intersection(set(model_out_spec.keys())):
    raise ValueError(
        'Keys overlap between flat_out_spec and model_out_spec.'
        f'Given: {flat_out_spec} and {model_out_spec}'
    )

  num_actions = utils.get_num_actions_from_spec(action_spec)
  agent_out_spec = {key: val for key, val in flat_out_spec.items()}
  for key, val in model_out_spec.items():
    agent_out_spec[key] = ArraySpec((num_actions, *val.shape), val.dtype)
  return agent_out_spec


class UpdateRule:
  """Base class for update rule."""

  def _get_dummy_input(
      self,
      include_behaviour_out: bool = True,
      include_value_out: bool = False,
      include_agent_adv: bool = False,
  ) -> types.UpdateRuleInputs:
    """Generate a dummy meta-network input for initialization."""
    b = 1
    t = 2
    unroll_batch_shape = (t, b)
    bootstrapped_shape = (t + 1, b)
    dummy_action_spec = dm_env_specs.BoundedArray(
        shape=(3,), dtype=int, minimum=0, maximum=3
    )
    num_actions = utils.get_num_actions_from_spec(dummy_action_spec)
    dummy_actions = jnp.zeros(bootstrapped_shape, dtype=jnp.int32)
    agent_out_shapes = self.agent_output_spec(dummy_action_spec)

    agent_out = jax.tree.map(
        lambda s: jnp.zeros(bootstrapped_shape + s.shape), agent_out_shapes
    )

    dummy_input = types.UpdateRuleInputs(
        observations=jnp.zeros(bootstrapped_shape),
        actions=dummy_actions,
        rewards=jnp.zeros(unroll_batch_shape),
        is_terminal=jnp.ones(unroll_batch_shape, dtype=jnp.bool_),
        agent_out=agent_out,
    )

    if include_behaviour_out:
      dummy_input.behaviour_agent_out = {}
      dummy_input.behaviour_agent_out.update(agent_out)

    target_out = agent_out
    dummy_input.extra_from_rule = dict(target_out=target_out)

    if include_agent_adv:
      value_unroll_batch_shape = (t, b, 1)
      value_bootstrapped_shape = (t + 1, b, 1)
      q_bootstrapped_shape = (t + 1, b, num_actions, 1)

      dummy_input.extra_from_rule = dict(
          adv=jnp.zeros(value_unroll_batch_shape),
          normalized_adv=jnp.zeros(value_unroll_batch_shape),
          v_scalar=jnp.zeros(value_bootstrapped_shape),
          q=jnp.zeros(q_bootstrapped_shape),
          qv_adv=jnp.zeros(q_bootstrapped_shape),
          normalized_qv_adv=jnp.zeros(q_bootstrapped_shape),
          target_out=target_out,
      )

    if include_value_out:
      num_discounts = 1
      value_unroll_batch_shape = (t, b, num_discounts)
      q_shape = (t, b, num_actions, num_discounts)
      bootstrapped_q_shape = (t + 1, b, num_actions, num_discounts)
      value_bootstrapped_shape = (t + 1, b, num_discounts)
      dummy_input.value_out = types.ValueOuts(
          value=jnp.ones(value_bootstrapped_shape),
          target_value=jnp.ones(value_bootstrapped_shape),
          rho=jnp.ones(unroll_batch_shape),
          adv=jnp.ones(value_unroll_batch_shape),
          normalized_adv=jnp.ones(value_unroll_batch_shape),
          td=jnp.ones(value_unroll_batch_shape),
          normalized_td=jnp.ones(value_unroll_batch_shape),
          value_target=jnp.ones(value_unroll_batch_shape),
          qv_adv=jax.tree.map(jnp.ones, bootstrapped_q_shape),
          normalized_qv_adv=jax.tree.map(jnp.ones, bootstrapped_q_shape),
          q_target=jax.tree.map(jnp.ones, q_shape),
          q_value=jax.tree.map(jnp.ones, q_shape),
          target_q_value=jax.tree.map(jnp.ones, q_shape),
          q_td=jax.tree.map(jnp.ones, q_shape),
          normalized_q_td=jax.tree.map(jnp.ones, q_shape),
      )

    return dummy_input

  def init_params(
      self, rng: chex.PRNGKey
  ) -> tuple[types.MetaParams, chex.ArrayTree]:
    """Initialize meta-parameters.

    Args:
      rng: random key.

    Returns:
      Meta-parameters and initial meta-network state.
    """
    raise NotImplementedError

  def flat_output_spec(self, action_spec: types.ActionSpec) -> types.Specs:
    """Returns the agent's unconditional output specs.

    Args:
      action_spec: An action spec.

    Returns:
      A nested dict with tuples specifying output specs.
    """
    del action_spec
    return dict()

  def model_output_spec(self, action_spec: types.ActionSpec) -> types.Specs:
    """Returns the agent's action-conditional output specs.

    Args:
      action_spec: An action spec.

    Returns:
      A nested dict with tuples specifying model output specs.
    """
    del action_spec
    return dict()

  def agent_output_spec(self, action_spec: types.ActionSpec) -> types.Specs:
    """Returns the agent total outputs' specs.

    Args:
      action_spec: An action spec.

    Returns:
      A pair of dicts with specs specifying agent's total output specs.
    """
    return get_agent_out_spec(
        action_spec=action_spec,
        flat_out_spec=self.flat_output_spec(action_spec),
        model_out_spec=self.model_output_spec(action_spec),
    )

  def init_meta_state(
      self,
      rng: chex.PRNGKey,
      params: types.AgentParams,
  ) -> types.MetaState:
    """The agent initial meta state.

    Args:
      rng: random key.
      params: agent params.

    Returns:
      An array tree with meta state.
    """
    raise NotImplementedError

  def unroll_meta_net(
      self,
      meta_params: types.MetaParams,
      params: types.AgentParams,
      state: types.HaikuState,
      meta_state: types.MetaState,
      rollout: types.UpdateRuleInputs,
      hyper_params: types.HyperParams,
      unroll_policy_fn: types.AgentStepFn,
      rng: chex.PRNGKey,
      axis_name: str | None,
  ) -> tuple[types.UpdateRuleOuts, types.MetaState]:
    """Unroll the meta network to prepare for the agent's loss.

    Args:
      meta_params: meta parameters.
      params: agent parameters.
      state: state of the agent.
      meta_state: meta state of the agent.
      rollout: rollout. [T, B, ...] and [T+1, B, ...] for `agent_out`
      hyper_params: hyper_params for the agent loss.
      unroll_policy_fn: agent's policy unroll function.
      rng: random key.
      axis_name: an axis name to use in collective ops, if runs under `pmap`.

    Returns:
      The output of meta network. [T, B, ...]
      An updated meta state.
    """
    raise NotImplementedError

  def agent_loss(
      self,
      rollout: types.UpdateRuleInputs,
      meta_out: types.UpdateRuleOuts,
      hyper_params: types.HyperParams,
      backprop: bool,
  ) -> tuple[chex.Array, types.UpdateRuleLog]:
    """The agent loss.

    Args:
      rollout: rollout with reward, discount, etc. [T, B, ...]
      meta_out: meta network output along the rollout. [T, B, ...]
      hyper_params: hyper_params for the agent loss.
      backprop: whether to make the loss differentiable or not.

    Returns:
      Loss per step (a tensor) and logs.
    """
    raise NotImplementedError

  def agent_loss_no_meta(
      self,
      rollout: types.UpdateRuleInputs,
      meta_out: types.UpdateRuleOuts | None,
      hyper_params: types.HyperParams,
  ) -> tuple[chex.Array, types.UpdateRuleLog]:
    """An optional part of the agent loss which shouldn't receive metagradients.

    Args:
      rollout: rollout with reward, discount, etc. [T, B, ...]
      meta_out: meta network output along the rollout. [T, B, ...]
      hyper_params: hyper_params for the agent loss.

    Returns:
      Loss per step (a tensor) and logs.
    """
    del meta_out, hyper_params
    return jnp.zeros_like(rollout.rewards), {}

  def __call__(
      self,
      meta_params: types.MetaParams,
      params: types.AgentParams,
      state: types.HaikuState,
      rollout: types.UpdateRuleInputs,
      hyper_params: types.HyperParams,
      meta_state: types.MetaState,
      unroll_policy_fn: types.AgentUnrollFn,
      rng: chex.PRNGKey,
      axis_name: str | None,
      backprop: bool = False,
  ) -> tuple[chex.Array, types.MetaState, types.UpdateRuleLog]:
    """The agent loss from rollout and agent output.

    Args:
      meta_params: meta parameters.
      params: agent parameters.
      state: state of the agent.
      rollout: rollout with reward, discount, etc. [T+1, B, ...]
      hyper_params: scalar hyper_params for the agent loss.
      meta_state: meta state of the agent.
      unroll_policy_fn: agent's policy unroll function.
      rng: random key.
      axis_name: an axis name to use in collective ops, if runs under `pmap`.
      backprop: whether to make the loss differentiable wrt meta_params or not.

    Returns:
      A tuple (per-step loss, meta_state, log).
    """
    meta_out, new_meta_state = self.unroll_meta_net(
        meta_params=meta_params,
        params=params,
        state=state,
        meta_state=meta_state,
        rollout=rollout,
        hyper_params=hyper_params,
        unroll_policy_fn=unroll_policy_fn,
        rng=rng,
        axis_name=axis_name,
    )
    loss_per_step, log_with_meta = self.agent_loss(
        rollout, meta_out, hyper_params, backprop=backprop
    )
    loss_per_step_no_meta, log_no_meta = self.agent_loss_no_meta(
        rollout, meta_out, hyper_params
    )

    loss_per_step = loss_per_step + loss_per_step_no_meta
    logs = log_with_meta | log_no_meta

    return loss_per_step, new_meta_state, logs


--- FILE: ./github_DiscoRL/disco_rl/update_rules/weights/disco_103.npz ---



--- FILE: ./github_DiscoRL/colabs/meta_train.ipynb ---

{
  "cells": [
    {
      "metadata": {
        "id": "I837hfXK8CAd"
      },
      "cell_type": "markdown",
      "source": [
        "# Meta-training / finetuning update rules\n",
        "\n",
        "This colab demonstrates how to finetune the `Disco103` update rule on a jittable version of `Catch` using a population of agents.\n",
        "\n",
        "The repository also contains a CPU version of `Catch`; feel free to explore and repurpose this code for your needs."
      ]
    },
    {
      "metadata": {
        "id": "zkPWk7TR7922"
      },
      "cell_type": "code",
      "source": [
        "# @title Install the package.\n",
        "\n",
        "!pip install git+https://github.com/google-deepmind/disco_rl.git\n",
        "\n",
        "from typing import Any\n",
        "\n",
        "import chex\n",
        "import distrax\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from matplotlib import pyplot as plt\n",
        "from ml_collections import config_dict\n",
        "import numpy as np\n",
        "import optax\n",
        "import pandas as pd\n",
        "import rlax\n",
        "import seaborn as sns\n",
        "import tqdm\n",
        "\n",
        "# Types & utils\n",
        "from disco_rl import types\n",
        "from disco_rl import utils\n",
        "\n",
        "# Environments\n",
        "from disco_rl.environments import base as base_env\n",
        "from disco_rl.environments import jittable_envs\n",
        "\n",
        "# Learning\n",
        "from disco_rl import agent as agent_lib\n",
        "from disco_rl.value_fns import value_fn\n",
        "\n",
        "axis_name = 'i'  # for parallelisation"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Q_QQRMPQaLip"
      },
      "cell_type": "code",
      "source": [
        "# @title Download and unpack `Disco103` weights.\n",
        "\n",
        "def unflatten_params(flat_params: chex.ArrayTree) -> chex.ArrayTree:\n",
        "  params = {}\n",
        "  for key_wb in flat_params:\n",
        "    key = '/'.join(key_wb.split('/')[:-1])\n",
        "    params[key] = {\n",
        "        'b': flat_params[f'{key}/b'],\n",
        "        'w': flat_params[f'{key}/w'],\n",
        "    }\n",
        "  return params\n",
        "\n",
        "\n",
        "disco_103_fname = 'disco_103.npz'\n",
        "disco_103_url = f\"https://raw.githubusercontent.com/google-deepmind/disco_rl/main/disco_rl/update_rules/weights/{disco_103_fname}\"\n",
        "!wget $disco_103_url\n",
        "\n",
        "with open(f'/content/{disco_103_fname}', 'rb') as file:\n",
        "  disco_103_params = unflatten_params(np.load(file))\n",
        "\n",
        "print(f'Loaded {len(disco_103_params) * 2} parameter tensors for Disco103.')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "QhHP6mbdE9Rx"
      },
      "cell_type": "code",
      "source": [
        "# @title Configure a simple MLP agent.\n",
        "\n",
        "\n",
        "def get_env(batch_size: int) -> base_env.Environment:\n",
        "  return jittable_envs.CatchJittableEnvironment(\n",
        "      batch_size=batch_size,\n",
        "      env_settings=config_dict.ConfigDict(dict(rows=5, columns=5)),\n",
        "  )\n",
        "\n",
        "\n",
        "# Create a dummy environment.\n",
        "env = get_env(batch_size=1)\n",
        "\n",
        "# Create settings for an agent.\n",
        "agent_settings = agent_lib.get_settings_disco()\n",
        "agent_settings.net_settings.name = 'mlp'\n",
        "agent_settings.net_settings.net_args = dict(\n",
        "    dense=(512, 512),\n",
        "    model_arch_name='lstm',\n",
        "    head_w_init_std=1e-2,\n",
        "    model_kwargs=dict(\n",
        "        head_mlp_hiddens=(256,),\n",
        "        lstm_size=256,\n",
        "    ),\n",
        ")\n",
        "agent_settings.learning_rate = 5e-4\n",
        "\n",
        "# Create the agent.\n",
        "agent = agent_lib.Agent(\n",
        "    agent_settings=agent_settings,\n",
        "    single_observation_spec=env.single_observation_spec(),\n",
        "    single_action_spec=env.single_action_spec(),\n",
        "    batch_axis_name=axis_name,\n",
        ")\n",
        "\n",
        "# Ensure that the agent's update rule's parameters have the same specs.\n",
        "random_update_rule_params, _ = agent.update_rule.init_params(\n",
        "    jax.random.PRNGKey(1)\n",
        ")\n",
        "if agent_settings.update_rule_name == 'disco':\n",
        "  chex.assert_trees_all_equal_shapes_and_dtypes(\n",
        "      random_update_rule_params, disco_103_params\n",
        "  )\n",
        "  print('Update rule parameters have the same specs.')\n",
        "else:\n",
        "  print('Not using a discovered rule, skipping check.')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "KVaiB7T195gy"
      },
      "cell_type": "code",
      "source": [
        "# @title Helper functions for interacting with environments.\n",
        "\n",
        "def unroll_jittable_actor(\n",
        "    params,\n",
        "    actor_state,\n",
        "    ts,\n",
        "    env_state,\n",
        "    rng,\n",
        "    env,\n",
        "    rollout_len,\n",
        "    actor_step_fn,\n",
        "):\n",
        "\n",
        "  def _single_step(carry, step_rng):\n",
        "    env_state, ts, actor_state = carry\n",
        "    actor_timestep, actor_state = actor_step_fn(\n",
        "        params, step_rng, ts, actor_state\n",
        "    )\n",
        "    env_state, ts = env.step(env_state, actor_timestep.actions)\n",
        "    return (env_state, ts, actor_state), actor_timestep\n",
        "\n",
        "  (env_state, ts, actor_state), actor_rollout = jax.lax.scan(\n",
        "      _single_step,\n",
        "      (env_state, ts, actor_state),\n",
        "      jax.random.split(rng, rollout_len),\n",
        "  )\n",
        "\n",
        "  actor_rollout = types.ActorRollout.from_timestep(actor_rollout)\n",
        "  return actor_rollout, actor_state, ts, env_state"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "u8ZHIAzZgneH"
      },
      "cell_type": "code",
      "source": [
        "# @title Meta training agent\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class MetaTrainState:\n",
        "  learner_state: agent_lib.LearnerState\n",
        "  actor_state: types.HaikuState\n",
        "  value_state: types.ValueState\n",
        "  env_state: Any\n",
        "  env_timestep: types.EnvironmentTimestep\n",
        "\n",
        "\n",
        "class MetaTrainAgent:\n",
        "  \"\"\"Meta training agent bundles together all meta training-related funct-ty.\"\"\"\n",
        "\n",
        "  agent: agent_lib.Agent\n",
        "  value_fn: value_fn.ValueFunction\n",
        "  env: base_env.Environment\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      batch_size_per_device: int,\n",
        "      agent_settings: config_dict.ConfigDict,\n",
        "      value_fn_config: types.ValueFnConfig,\n",
        "      axis_name: str | None = axis_name,\n",
        "  ):\n",
        "    self.env = get_env(batch_size_per_device)\n",
        "    self.agent = agent_lib.Agent(\n",
        "        agent_settings=agent_settings,\n",
        "        single_observation_spec=self.env.single_observation_spec(),\n",
        "        single_action_spec=self.env.single_action_spec(),\n",
        "        batch_axis_name=axis_name,\n",
        "    )\n",
        "    self.value_fn = value_fn.ValueFunction(value_fn_config, axis_name=axis_name)\n",
        "    self._unroll_jittable_actor = jax.jit(\n",
        "        unroll_jittable_actor,\n",
        "        static_argnames=('env', 'rollout_len', 'actor_step_fn'),\n",
        "    )\n",
        "\n",
        "  @property\n",
        "  def learner_step(self):\n",
        "    return self.agent.learner_step\n",
        "\n",
        "  @property\n",
        "  def actor_step(self):\n",
        "    return self.agent.actor_step\n",
        "\n",
        "  @property\n",
        "  def unroll_net(self):\n",
        "    return self.agent.unroll_net\n",
        "\n",
        "  def init_state(self, rng_key: chex.PRNGKey) -> MetaTrainState:\n",
        "    \"\"\"Initialize the meta train state.\"\"\"\n",
        "    dummy_obs = utils.zeros_like_spec(\n",
        "        self.env.single_observation_spec(),\n",
        "        prepend_shape=(batch_size_per_device,),\n",
        "    )\n",
        "    rng_keys = jax.random.split(rng_key, 3)\n",
        "    env_state, env_timestep = self.env.reset(rng_key)\n",
        "    return MetaTrainState(\n",
        "        learner_state=self.agent.initial_learner_state(rng_keys[0]),\n",
        "        actor_state=self.agent.initial_actor_state(rng_keys[1]),\n",
        "        value_state=self.value_fn.initial_state(rng_keys[2], dummy_obs),\n",
        "        env_state=env_state,\n",
        "        env_timestep=env_timestep,\n",
        "    )\n",
        "\n",
        "  def unroll_actor(\n",
        "      self, state: MetaTrainState, rng: chex.PRNGKey, rollout_len: int\n",
        "  ) -> tuple[MetaTrainState, types.ActorRollout]:\n",
        "    \"\"\"Unrolls the actor for the given number of steps and updates the state.\"\"\"\n",
        "    rollout, new_actor_state, new_env_timestep, new_env_state = (\n",
        "        self._unroll_jittable_actor(\n",
        "            state.learner_state.params,\n",
        "            state.actor_state,\n",
        "            state.env_timestep,\n",
        "            state.env_state,\n",
        "            rng,\n",
        "            self.env,\n",
        "            rollout_len,\n",
        "            self.actor_step,\n",
        "        )\n",
        "    )\n",
        "    new_state = MetaTrainState(\n",
        "        learner_state=state.learner_state,\n",
        "        actor_state=new_actor_state,\n",
        "        value_state=state.value_state,\n",
        "        env_state=new_env_state,\n",
        "        env_timestep=new_env_timestep,\n",
        "    )\n",
        "    return new_state, rollout\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "amRhz7rAAeYO"
      },
      "cell_type": "code",
      "source": [
        "# @title Calculate meta gradients for a single agent.\n",
        "\n",
        "\n",
        "def calculate_meta_gradient(\n",
        "    update_rule_params: types.MetaParams,\n",
        "    agent_state: MetaTrainState,\n",
        "    train_rollouts: types.ActorRollout,\n",
        "    valid_rollout: types.ActorRollout,\n",
        "    rng: chex.PRNGKey,\n",
        "    agent: MetaTrainAgent,\n",
        "    axis_name: str | None = axis_name,\n",
        "):\n",
        "  \"\"\"Calculates meta gradients for a single agent.\"\"\"\n",
        "\n",
        "  unroll_len = train_rollouts.rewards.shape[0]\n",
        "\n",
        "  def _inner_step(carry, inputs):\n",
        "    \"\"\"Updates agent's and value fn's params using the current update rule.\"\"\"\n",
        "    update_rule_params, learner_state, actor_state, value_state = carry\n",
        "    actor_rollout, learner_rng = inputs\n",
        "\n",
        "    # Update agent's parameters.\n",
        "    new_learner_state, new_actor_state, metrics = agent.learner_step(\n",
        "        rng=learner_rng,\n",
        "        rollout=actor_rollout,\n",
        "        learner_state=learner_state,\n",
        "        agent_net_state=actor_state,\n",
        "        update_rule_params=update_rule_params,\n",
        "        is_meta_training=True,\n",
        "    )\n",
        "\n",
        "    # Update value function.\n",
        "    agent_out, _ = agent.unroll_net(\n",
        "        learner_state.params, actor_state, actor_rollout\n",
        "    )\n",
        "    new_value_state, _, _ = agent.value_fn.update(\n",
        "        value_state, actor_rollout, agent_out['logits']\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        update_rule_params,\n",
        "        new_learner_state,\n",
        "        new_actor_state,\n",
        "        new_value_state,\n",
        "    ), metrics\n",
        "\n",
        "  def _outer_loss(\n",
        "      update_rule_params: types.MetaParams,\n",
        "      agent_state: MetaTrainState,\n",
        "      train_rollouts: types.ActorRollout,\n",
        "      valid_rollout: types.ActorRollout,\n",
        "      rng: chex.PRNGKey,\n",
        "  ):\n",
        "    \"\"\"Calculates (meta) loss for the update rule.\"\"\"\n",
        "    train_rng, valid_rng = jax.random.split(rng, 2)\n",
        "\n",
        "    # Perform N inner steps (i.e. agent's params' updates).\n",
        "    learner_rngs = jax.random.split(train_rng, unroll_len)\n",
        "    (_, new_learner_state, new_actor_state, new_value_state), train_metrics = (\n",
        "        jax.lax.scan(\n",
        "            _inner_step,\n",
        "            (\n",
        "                update_rule_params,\n",
        "                agent_state.learner_state,\n",
        "                agent_state.actor_state,\n",
        "                agent_state.value_state,\n",
        "            ),\n",
        "            (train_rollouts, learner_rngs),\n",
        "        )\n",
        "    )\n",
        "    train_meta_out = train_metrics.pop('meta_out')\n",
        "\n",
        "    # Run inference on the validation rollout.\n",
        "    agent_rollout_on_valid, _ = hk.BatchApply(\n",
        "        lambda ts: agent.actor_step(\n",
        "            actor_params=new_learner_state.params,\n",
        "            rng=valid_rng,\n",
        "            timestep=ts,\n",
        "            actor_state=valid_rollout.first_state(time_axis=0),\n",
        "        )\n",
        "    )(valid_rollout.to_env_timestep())\n",
        "\n",
        "    # Calculate value_fn on the validation rollout.\n",
        "    value_out, _, _, _ = agent.value_fn.get_value_outs(\n",
        "        new_value_state, valid_rollout, agent_rollout_on_valid['logits']\n",
        "    )\n",
        "    actions_on_valid = valid_rollout.actions[:-1]\n",
        "    logits_on_valid = agent_rollout_on_valid['logits'][:-1]\n",
        "    adv_t = jax.lax.stop_gradient(value_out.normalized_adv)\n",
        "\n",
        "    # Calculate meta loss' components.\n",
        "    # Policy gradient loss.\n",
        "    pg_loss_per_step = utils.differentiable_policy_gradient_loss(\n",
        "        logits_on_valid, actions_on_valid, adv_t=adv_t, backprop=False\n",
        "    )\n",
        "\n",
        "    # Meta regularizers.\n",
        "    reg_loss = 0\n",
        "    reg_loss += -1e-2 * distrax.Softmax(logits_on_valid).entropy().mean() # entr\n",
        "\n",
        "    # Validation regularisers.\n",
        "    agent_out_on_valid = agent_rollout_on_valid.agent_outs\n",
        "    z_a = utils.batch_lookup(agent_out_on_valid['z'][:-1], actions_on_valid)\n",
        "    y_entropy_loss = -jnp.mean(\n",
        "        distrax.Softmax(agent_out_on_valid['y']).entropy()\n",
        "    )\n",
        "    z_entropy_loss = -jnp.mean(distrax.Softmax(z_a).entropy())\n",
        "    reg_loss += 1e-3 * (y_entropy_loss + z_entropy_loss)\n",
        "\n",
        "    # Train regularisers.\n",
        "    dp, dy, dz = train_meta_out['pi'], train_meta_out['y'], train_meta_out['z']\n",
        "    chex.assert_equal_shape_prefix([dp, dy, dz], 3)  # [N, T, B, ...]\n",
        "    reg_loss += 1e-3 * jnp.mean(jnp.square(jnp.mean(dy, axis=(1, 2, 3))))\n",
        "    reg_loss += 1e-3 * jnp.mean(jnp.square(jnp.mean(dz, axis=(1, 2, 3))))\n",
        "    reg_loss += 1e-3 * jnp.mean(jnp.square(jnp.mean(dp, axis=(1, 2, 3))))\n",
        "    logits = train_meta_out['target_out']['logits'][:, :-1]\n",
        "    chex.assert_equal_shape([logits, dp])  # [N, T, B, A]\n",
        "    target_kl_loss = rlax.categorical_kl_divergence(\n",
        "        jax.lax.stop_gradient(logits), dp\n",
        "    )\n",
        "    reg_loss += 1e-2 * jnp.mean(target_kl_loss)\n",
        "\n",
        "    # Meta loss.\n",
        "    meta_loss = pg_loss_per_step.mean() + reg_loss\n",
        "\n",
        "    # Update logs and states.\n",
        "    meta_log = dict(\n",
        "        adv=value_out.adv.mean(),\n",
        "        normalized_adv=value_out.normalized_adv.mean(),\n",
        "        entropy=distrax.Softmax(logits_on_valid).entropy().mean(),\n",
        "        value=value_out.value.mean(),\n",
        "        val_importance_weight=jnp.mean(jnp.minimum(value_out.rho, 1.0)),\n",
        "        meta_loss=meta_loss,\n",
        "        pg_loss=pg_loss_per_step.mean(),\n",
        "        reg_loss=reg_loss,\n",
        "    )\n",
        "    new_agent_state = MetaTrainState(\n",
        "        learner_state=new_learner_state,\n",
        "        actor_state=new_actor_state,\n",
        "        value_state=new_value_state,\n",
        "        env_state=agent_state.env_state,\n",
        "        env_timestep=agent_state.env_timestep,\n",
        "    )\n",
        "\n",
        "    return meta_loss, (new_agent_state, train_metrics, meta_log)\n",
        "\n",
        "  # Calculate meta gradients.\n",
        "  meta_grads, outputs = jax.grad(_outer_loss, has_aux=True)(\n",
        "      update_rule_params, agent_state, train_rollouts, valid_rollout, rng\n",
        "  )\n",
        "  new_agent_state, train_metrics, meta_log = outputs\n",
        "  if axis_name is not None:\n",
        "    (meta_grads, train_metrics, meta_log) = jax.lax.pmean(\n",
        "        (meta_grads, train_metrics, meta_log), axis_name\n",
        "    )\n",
        "\n",
        "  return meta_grads, (new_agent_state, train_metrics, meta_log)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "qLagkSk38Skl"
      },
      "cell_type": "code",
      "source": [
        "# @title Meta update step (requires calculating meta gradients for all agents).\n",
        "\n",
        "\n",
        "def meta_update(\n",
        "    update_rule_params: types.MetaParams,\n",
        "    meta_opt_state: optax.OptState,\n",
        "    agents_states: list[MetaTrainState],\n",
        "    rng: chex.PRNGKey,\n",
        "    axis_name: str | None = axis_name,\n",
        "):\n",
        "  \"\"\"Calculates an update for the meta parameters.\"\"\"\n",
        "\n",
        "  # Generate `num_inner_steps` trajectories for each of `num_agents` agents.\n",
        "  train_rollouts = [None] * num_agents\n",
        "  valid_rollouts = [None] * num_agents\n",
        "  rng_act, rng_upd = jax.random.split(rng)\n",
        "  rngs_per_agent_act = jax.random.split(rng_act, num_agents)\n",
        "  for agent_i in range(num_agents):\n",
        "    agent, state = agents[agent_i], agents_states[agent_i]\n",
        "    rollouts = [None] * num_inner_steps\n",
        "    rngs_per_step = jax.random.split(\n",
        "        rngs_per_agent_act[agent_i], num_inner_steps\n",
        "    )\n",
        "    for step_i in range(num_inner_steps):\n",
        "      state, rollouts[step_i] = agent.unroll_actor(\n",
        "          state, rngs_per_step[step_i], rollout_len\n",
        "      )\n",
        "    train_rollouts[agent_i] = utils.tree_stack(rollouts)\n",
        "\n",
        "    # Generate 2x longer trajectories for validation.\n",
        "    agents_states[agent_i], valid_rollouts[agent_i] = agent.unroll_actor(\n",
        "        state, rngs_per_agent_act[agent_i], 2 * rollout_len\n",
        "    )\n",
        "\n",
        "  # Calculate meta gradients for each agent.\n",
        "  meta_grads = [None] * num_agents\n",
        "  rngs_per_agent_upd = jax.random.split(rng_upd, num_agents)\n",
        "  metrics, meta_log = None, None\n",
        "  for agent_i in range(num_agents):\n",
        "    meta_grads[agent_i], (agents_states[agent_i], metrics, meta_log) = (\n",
        "        calculate_meta_gradient(\n",
        "            update_rule_params=update_rule_params,\n",
        "            agent_state=agents_states[agent_i],\n",
        "            train_rollouts=train_rollouts[agent_i],\n",
        "            valid_rollout=valid_rollouts[agent_i],\n",
        "            rng=rngs_per_agent_upd[agent_i],\n",
        "            agent=agents[agent_i],\n",
        "            axis_name=axis_name,\n",
        "        )\n",
        "    )\n",
        "\n",
        "  # Log rewards and proportion positive rewards.\n",
        "  rewards = [None] * num_agents\n",
        "  pos_rewards = [None] * num_agents\n",
        "  neg_rewards = [None] * num_agents\n",
        "  for agent_i in range(num_agents):\n",
        "    assert train_rollouts[agent_i] is not None\n",
        "    r = train_rollouts[agent_i].rewards\n",
        "    rewards[agent_i] = r.mean()\n",
        "\n",
        "    pos_rewards[agent_i] = (r > 0).sum()\n",
        "    neg_rewards[agent_i] = (r < 0).sum()\n",
        "\n",
        "  # Average meta gradients across all agents and pass through meta optimizer.\n",
        "  # Note that we had a separate meta optimiser for each agent in the original\n",
        "  # work; that logic is omitted here for simplicity.\n",
        "  avg_meta_gradient = jax.tree.map(\n",
        "      lambda x: x.mean(axis=0), utils.tree_stack(meta_grads)\n",
        "  )\n",
        "  meta_update, meta_opt_state = meta_opt.update(\n",
        "      avg_meta_gradient, meta_opt_state\n",
        "  )\n",
        "  update_rule_params = optax.apply_updates(update_rule_params, meta_update)\n",
        "\n",
        "  meta_log['meta_grad_norm'] = optax.global_norm(avg_meta_gradient)\n",
        "  meta_log['meta_up_norm'] = optax.global_norm(meta_update)\n",
        "  meta_log['rewards'] = utils.tree_stack(rewards).mean()\n",
        "  meta_log['pos_rewards'] = utils.tree_stack(pos_rewards).mean()\n",
        "  meta_log['neg_rewards'] = utils.tree_stack(neg_rewards).mean()\n",
        "  return update_rule_params, meta_opt_state, agents_states, metrics, meta_log"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "qA6VT-jaaJCf"
      },
      "cell_type": "code",
      "source": [
        "# @title Meta training config\n",
        "\n",
        "num_steps = 800\n",
        "num_agents = 2\n",
        "rollout_len = 16\n",
        "num_inner_steps = 2\n",
        "batch_size_per_device = 32\n",
        "rng_key = jax.random.PRNGKey(4)\n",
        "\n",
        "value_fn_config = types.ValueFnConfig(\n",
        "    net='mlp',\n",
        "    net_args=dict(\n",
        "        dense=(256, 256),\n",
        "        head_w_init_std=1e-2,\n",
        "        action_spec=(),\n",
        "    ),\n",
        "    learning_rate=1e-3,\n",
        "    max_abs_update=1.0,\n",
        "    discount_factor=0.99,\n",
        "    td_lambda=0.96,\n",
        "    outer_value_cost=1.0,\n",
        ")\n",
        "\n",
        "# Use random params for the update rule.\n",
        "update_rule_params = random_update_rule_params  # can be `disco_103_params`\n",
        "meta_opt = optax.adam(learning_rate=5e-4)\n",
        "\n",
        "meta_opt_state = meta_opt.init(update_rule_params)\n",
        "meta_log = {}\n",
        "metrics = {}\n",
        "\n",
        "# Create multiple agents.\n",
        "agents = []\n",
        "agents_states = []\n",
        "rng, rng_key = jax.random.split(rng_key)\n",
        "for rng_key in jax.random.split(rng_key, num_agents):\n",
        "  agents.append(\n",
        "      MetaTrainAgent(\n",
        "          batch_size_per_device=batch_size_per_device,\n",
        "          agent_settings=agent_settings,\n",
        "          value_fn_config=value_fn_config,\n",
        "      )\n",
        "  )\n",
        "  agents_states.append(agents[-1].init_state(rng_key))\n",
        "\n",
        "# Parallelise using all available devices.\n",
        "devices = jax.devices()\n",
        "jitted_meta_update = jax.pmap(meta_update, axis_name=axis_name, devices=devices)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "jrZ3SwBS7_gG"
      },
      "cell_type": "code",
      "source": [
        "# @title Run meta-training (note that compilation can take time!).\n",
        "\n",
        "# Replicate parameters for each device.\n",
        "step_update_rule_params = jax.device_put_replicated(\n",
        "    update_rule_params, devices\n",
        ")\n",
        "step_meta_opt_state = jax.device_put_replicated(meta_opt_state, devices)\n",
        "step_agents_states = jax.device_put_replicated(agents_states, devices)\n",
        "\n",
        "# Run the meta learning loop.\n",
        "for meta_step in tqdm.tqdm(range(num_steps)):\n",
        "  if meta_step in metrics:  # to support interrupting and continuing\n",
        "    continue\n",
        "\n",
        "  rng, step_rngs = jax.random.split(rng)\n",
        "  step_rngs = jax.random.split(step_rngs, len(devices))\n",
        "  (\n",
        "      step_update_rule_params,\n",
        "      step_meta_opt_state,\n",
        "      step_agents_states,\n",
        "      metrics[meta_step],\n",
        "      meta_log[meta_step],\n",
        "  ) = jitted_meta_update(\n",
        "      update_rule_params=step_update_rule_params,\n",
        "      meta_opt_state=step_meta_opt_state,\n",
        "      agents_states=step_agents_states,\n",
        "      rng=step_rngs,\n",
        "  )\n",
        "  metrics[meta_step], meta_log[meta_step] = jax.device_get(\n",
        "      (metrics[meta_step], meta_log[meta_step])\n",
        "  )\n",
        "\n",
        "# Collect metrics from all devices.\n",
        "metrics, meta_log = utils.gather_from_devices((metrics, meta_log))\n",
        "metrics, meta_log = jax.tree.map(lambda x: x.mean(0), (metrics, meta_log))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "erzgtMrjNP3U"
      },
      "cell_type": "code",
      "source": [
        "# @title Process logs and metrics.\n",
        "\n",
        "meta_log_cpu = jax.device_get(meta_log)\n",
        "steps = np.sort(np.unique(list(meta_log_cpu.keys())))\n",
        "rows = []\n",
        "for i in steps:\n",
        "  for key in (\n",
        "      'meta_grad_norm',\n",
        "      'meta_up_norm',\n",
        "      'meta_loss',\n",
        "      'rewards',\n",
        "      'pos_rewards',\n",
        "      'neg_rewards',\n",
        "  ):\n",
        "    rows.append(dict(step=i, value=float(meta_log_cpu[i][key]), f=key))\n",
        "\n",
        "df = pd.DataFrame(rows)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "tU9x4cEyLpyT"
      },
      "cell_type": "code",
      "source": [
        "sns.relplot(\n",
        "    data=df[df.f.isin(['pos_rewards', 'neg_rewards'])],\n",
        "    x='step',\n",
        "    y='value',\n",
        "    kind='line',\n",
        "    hue='f',\n",
        "    errorbar=None,\n",
        "    aspect=1.5,\n",
        ")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "eTTOZwI5E5KF"
      },
      "cell_type": "code",
      "source": [
        "sns.relplot(\n",
        "    data=df[\n",
        "        df.f.isin(['meta_grad_norm', 'meta_loss', 'meta_up_norm', 'rewards'])\n",
        "    ],\n",
        "    x='step',\n",
        "    y='value',\n",
        "    kind='line',\n",
        "    col='f',\n",
        "    errorbar=None,\n",
        "    aspect=1.5,\n",
        "    facet_kws={'sharey': False, 'sharex': True},\n",
        ")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "o7Rt79q3-2pI"
      },
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}


--- FILE: ./github_DiscoRL/colabs/eval.ipynb ---

{
  "cells": [
    {
      "metadata": {
        "id": "xDyUmhfopb4S"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluating discovered update rules\n",
        "\n",
        "This colab demonstrates how to instantiate the `Disco103` update rule and use it for training an RL agent on a jittable version of `Catch`.\n",
        "\n",
        "The repository also contains `ActorCritic` and `PolicyGradient` update rules and a CPU version of `Catch`; feel free to explore and repurpose this code for your needs."
      ]
    },
    {
      "metadata": {
        "id": "Q_QQRMPQaLip"
      },
      "cell_type": "code",
      "source": [
        "# @title Install the package.\n",
        "\n",
        "!pip install git+https://github.com/google-deepmind/disco_rl.git\n",
        "\n",
        "import collections\n",
        "\n",
        "import chex\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rlax\n",
        "import seaborn as sns\n",
        "import tqdm\n",
        "\n",
        "# Types & utils\n",
        "from disco_rl import types\n",
        "from disco_rl import utils\n",
        "\n",
        "# Environments\n",
        "from disco_rl.environments import base as base_env\n",
        "from disco_rl.environments import jittable_envs\n",
        "\n",
        "# Learning\n",
        "from disco_rl import agent as agent_lib\n",
        "\n",
        "axis_name = 'i'  # for parallelisation"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "hinBYQVJqlm0"
      },
      "cell_type": "code",
      "source": [
        "# @title Download and unpack `Disco103` weights.\n",
        "\n",
        "def unflatten_params(flat_params: chex.ArrayTree) -> chex.ArrayTree:\n",
        "  params = {}\n",
        "  for key_wb in flat_params:\n",
        "    key = '/'.join(key_wb.split('/')[:-1])\n",
        "    params[key] = {\n",
        "        'b': flat_params[f'{key}/b'],\n",
        "        'w': flat_params[f'{key}/w'],\n",
        "    }\n",
        "  return params\n",
        "\n",
        "\n",
        "disco_103_fname = 'disco_103.npz'\n",
        "disco_103_url = f\"https://raw.githubusercontent.com/google-deepmind/disco_rl/main/disco_rl/update_rules/weights/{disco_103_fname}\"\n",
        "!wget $disco_103_url\n",
        "\n",
        "with open(f'/content/{disco_103_fname}', 'rb') as file:\n",
        "  disco_103_params = unflatten_params(np.load(file))\n",
        "\n",
        "print(f'Loaded {len(disco_103_params) * 2} parameter tensors for Disco103.')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "QhHP6mbdE9Rx"
      },
      "cell_type": "code",
      "source": [
        "# @title Instantiate a simple MLP agent.\n",
        "\n",
        "\n",
        "def get_env(batch_size: int) -> base_env.Environment:\n",
        "  return jittable_envs.CatchJittableEnvironment(\n",
        "      batch_size=batch_size, env_settings=jittable_envs.get_config_catch()\n",
        "  )\n",
        "\n",
        "\n",
        "# Create a dummy environment.\n",
        "env = get_env(batch_size=1)\n",
        "\n",
        "# Create settings for an agent.\n",
        "agent_settings = agent_lib.get_settings_disco()\n",
        "agent_settings.net_settings.name = 'mlp'\n",
        "agent_settings.net_settings.net_args = dict(\n",
        "    dense=(512, 512),\n",
        "    model_arch_name='lstm',\n",
        "    head_w_init_std=1e-2,\n",
        "    model_kwargs=dict(\n",
        "        head_mlp_hiddens=(128,),\n",
        "        lstm_size=128,\n",
        "    ),\n",
        ")\n",
        "agent_settings.learning_rate = 1e-2\n",
        "\n",
        "# Create the agent.\n",
        "agent = agent_lib.Agent(\n",
        "    agent_settings=agent_settings,\n",
        "    single_observation_spec=env.single_observation_spec(),\n",
        "    single_action_spec=env.single_action_spec(),\n",
        "    batch_axis_name=axis_name,\n",
        ")\n",
        "\n",
        "# Ensure that the agent's update rule's parameters have the same specs.\n",
        "random_update_rule_params, _ = agent.update_rule.init_params(\n",
        "    jax.random.PRNGKey(0)\n",
        ")\n",
        "if agent_settings.update_rule_name == 'disco':\n",
        "  chex.assert_trees_all_equal_shapes_and_dtypes(\n",
        "      random_update_rule_params, disco_103_params\n",
        "  )\n",
        "  print('Update rule parameters have the same specs.')\n",
        "else:\n",
        "  print('Not using a discovered rule, skipping check.')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "XZnd20Cph9Sl"
      },
      "cell_type": "code",
      "source": [
        "# @title Helper functions for interacting with environments.\n",
        "def unroll_cpu_actor(\n",
        "    params,\n",
        "    actor_state,\n",
        "    ts,\n",
        "    env_state,\n",
        "    rng,\n",
        "    env,\n",
        "    rollout_len,\n",
        "    actor_step_fn,\n",
        "    devices,\n",
        "):\n",
        "  \"\"\"Unrolls the policy for a CPU environments.\"\"\"\n",
        "  actor_timesteps = []\n",
        "  for _ in range(rollout_len):\n",
        "    rng, step_rng = jax.random.split(rng)\n",
        "    step_rng = jax.random.split(step_rng, len(devices))\n",
        "    ts = utils.shard_across_devices(ts, devices)\n",
        "\n",
        "    actor_timestep, actor_state = actor_step_fn(\n",
        "        params, step_rng, ts, actor_state\n",
        "    )\n",
        "    actions = utils.gather_from_devices(actor_timestep.actions)\n",
        "    env_state, ts = env.step(env_state, actions)\n",
        "\n",
        "    actor_timesteps.append(actor_timestep)\n",
        "\n",
        "  actor_rollout = types.ActorRollout.from_timestep(\n",
        "      utils.tree_stack(actor_timesteps, axis=1)\n",
        "  )\n",
        "  return actor_rollout, actor_state, ts, env_state\n",
        "\n",
        "\n",
        "def unroll_jittable_actor(\n",
        "    params,\n",
        "    actor_state,\n",
        "    ts,\n",
        "    env_state,\n",
        "    rng,\n",
        "    env,\n",
        "    rollout_len,\n",
        "    actor_step_fn,\n",
        "    devices,\n",
        "):\n",
        "  \"\"\"Unrolls the policy for a jittable environment.\"\"\"\n",
        "  del actor_step_fn, devices\n",
        "\n",
        "  def _single_step(carry, step_rng):\n",
        "    env_state, ts, actor_state = carry\n",
        "    actor_timestep, actor_state = agent.actor_step(\n",
        "        params, step_rng, ts, actor_state\n",
        "    )\n",
        "    env_state, ts = env.step(env_state, actor_timestep.actions)\n",
        "    return (env_state, ts, actor_state), actor_timestep\n",
        "\n",
        "  (env_state, ts, actor_state), actor_rollout = jax.lax.scan(\n",
        "      _single_step,\n",
        "      (env_state, ts, actor_state),\n",
        "      jax.random.split(rng, rollout_len),\n",
        "  )\n",
        "\n",
        "  actor_rollout = types.ActorRollout.from_timestep(actor_rollout)\n",
        "  return actor_rollout, actor_state, ts, env_state\n",
        "\n",
        "\n",
        "def accumulate_rewards(acc_rewards, x):\n",
        "  rewards, discounts = x\n",
        "\n",
        "  def _step_fn(acc_rewards, x):\n",
        "    rewards, discounts = x\n",
        "    acc_rewards += rewards\n",
        "    return acc_rewards * discounts, acc_rewards\n",
        "\n",
        "  return jax.lax.scan(_step_fn, acc_rewards, (rewards, discounts))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "lJ4HfawLSMWH"
      },
      "cell_type": "code",
      "source": [
        "# @title A simple Replay buffer.\n",
        "\n",
        "\n",
        "class SimpleReplayBuffer:\n",
        "  \"\"\"A simple FIFO replay buffer for JAX arrays.\"\"\"\n",
        "\n",
        "  def __init__(self, capacity: int, seed: int):\n",
        "    \"\"\"Initializes the buffer.\"\"\"\n",
        "    self.buffer = collections.deque(maxlen=capacity)\n",
        "    self.capacity = capacity\n",
        "    self.np_rng = np.random.default_rng(seed)\n",
        "\n",
        "  def add(self, rollout: types.ActorRollout) -> None:\n",
        "    \"\"\"Appends a batch of trajectories to the buffer.\"\"\"\n",
        "    rollout = jax.device_get(rollout)\n",
        "    # split_tree = split_tree_on_dim(rollout, 2)\n",
        "    split_tree = rlax.tree_split_leaves(rollout, axis=2)  # across batch dim\n",
        "    self.buffer.extend(split_tree)\n",
        "\n",
        "  def sample(self, batch_size: int) -> types.ActorRollout | None:\n",
        "    \"\"\"Samples a batch of trajectories from the buffer.\"\"\"\n",
        "    buffer_size = len(self.buffer)\n",
        "    if buffer_size == 0:\n",
        "      print(\"Warning: Trying to sample from an empty buffer.\")\n",
        "      return None\n",
        "\n",
        "    indices = self.np_rng.integers(buffer_size, size=batch_size)\n",
        "    batched_samples = utils.tree_stack(\n",
        "        [self.buffer[i] for i in indices], axis=2\n",
        "    )\n",
        "    return batched_samples\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"Returns the current number of transitions in the buffer.\"\"\"\n",
        "    return len(self.buffer)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "lmpFAJ84HUjS"
      },
      "cell_type": "code",
      "source": [
        "# @title Training loop\n",
        "\n",
        "num_steps = 1000\n",
        "batch_size = 64\n",
        "rollout_len = 29\n",
        "rng_key = jax.random.PRNGKey(0)\n",
        "\n",
        "replay_ratio = 32\n",
        "buffer = SimpleReplayBuffer(capacity=1024, seed=17)\n",
        "min_buffer_size = batch_size\n",
        "\n",
        "num_envs = batch_size // replay_ratio\n",
        "devices = tuple(jax.devices()[:num_envs])\n",
        "env = get_env(num_envs)\n",
        "\n",
        "# Init states.\n",
        "env_state, ts = env.reset(rng_key)\n",
        "acc_rewards = jnp.zeros((num_envs,))\n",
        "learner_state = agent.initial_learner_state(rng_key)\n",
        "actor_state = agent.initial_actor_state(rng_key)\n",
        "update_rule_params = disco_103_params\n",
        "\n",
        "# Parallelise training across all available devices.\n",
        "actor_step_fn = jax.pmap(agent.actor_step, axis_name, devices=devices)\n",
        "learner_step_fn = jax.pmap(\n",
        "    agent.learner_step,\n",
        "    axis_name=axis_name,\n",
        "    devices=devices,\n",
        "    static_broadcasted_argnums=(5,),\n",
        ")\n",
        "jitted_unroll_actor = jax.pmap(\n",
        "    unroll_jittable_actor,\n",
        "    axis_name=axis_name,\n",
        "    devices=devices,\n",
        "    static_broadcasted_argnums=(5, 6, 7, 8),\n",
        ")\n",
        "jitted_accumulate_rewards = jax.pmap(\n",
        "    accumulate_rewards,\n",
        "    axis_name=axis_name,\n",
        "    devices=devices,\n",
        ")\n",
        "\n",
        "# Replicate onto the devices.\n",
        "learner_state = jax.device_put_replicated(learner_state, devices)\n",
        "actor_state = jax.device_put_replicated(actor_state, devices)\n",
        "update_rule_params = jax.device_put_replicated(update_rule_params, devices)\n",
        "\n",
        "is_jittable_actor = isinstance(\n",
        "    env, jittable_envs.batched_jittable_env.BatchedJittableEnvironment\n",
        ")\n",
        "\n",
        "if is_jittable_actor:\n",
        "  unroll_actor = jitted_unroll_actor\n",
        "  acc_rewards_fn = jitted_accumulate_rewards\n",
        "  acc_rewards = jnp.zeros((num_envs,))\n",
        "  (env_state, ts, acc_rewards) = utils.shard_across_devices(\n",
        "      (env_state, ts, acc_rewards), devices\n",
        "  )\n",
        "else:\n",
        "  unroll_actor = unroll_cpu_actor\n",
        "  acc_rewards_fn = accumulate_rewards\n",
        "\n",
        "# Buffers.\n",
        "all_metrics = []\n",
        "all_rewards = []\n",
        "all_discounts = []\n",
        "all_steps = []\n",
        "all_returns = []\n",
        "total_steps = 0\n",
        "\n",
        "# Run the loop.\n",
        "for step in tqdm.tqdm(range(num_steps)):\n",
        "  rng_key, rng_actor, rng_learner = jax.random.split(rng_key, 3)\n",
        "\n",
        "  if is_jittable_actor:\n",
        "    rng_actor = jax.random.split(rng_actor, len(devices))\n",
        "\n",
        "  # Generate new trajectories and add them to the buffer.\n",
        "  actor_rollout, actor_state, ts, env_state = unroll_actor(\n",
        "      learner_state.params,\n",
        "      actor_state,\n",
        "      ts,\n",
        "      env_state,\n",
        "      rng_actor,\n",
        "      env,\n",
        "      rollout_len,\n",
        "      actor_step_fn,\n",
        "      devices,\n",
        "  )\n",
        "  buffer.add(actor_rollout)\n",
        "\n",
        "  # Accumulate statistics.\n",
        "  total_steps += np.prod(actor_rollout.rewards.shape)\n",
        "  acc_rewards, returns = acc_rewards_fn(\n",
        "      acc_rewards,\n",
        "      (actor_rollout.rewards, actor_rollout.discounts),\n",
        "  )\n",
        "  all_steps.append(total_steps)\n",
        "  all_rewards.append(jax.device_get(actor_rollout.rewards))\n",
        "  all_discounts.append(jax.device_get(actor_rollout.discounts))\n",
        "  all_returns.append(jax.device_get(returns))\n",
        "\n",
        "  # Update agent's parameters on the samples from the buffer.\n",
        "  rng_learner = jax.random.split(rng_learner, len(devices))\n",
        "  if len(buffer) >= min_buffer_size:\n",
        "    learner_rollout = buffer.sample(batch_size)\n",
        "    learner_state, _, metrics = learner_step_fn(\n",
        "        rng_learner,\n",
        "        learner_rollout,\n",
        "        learner_state,\n",
        "        actor_state,\n",
        "        update_rule_params,\n",
        "        False,  # is_meta_training\n",
        "    )\n",
        "    all_metrics.append(jax.device_get(metrics))\n",
        "\n",
        "# Collect all logs and statistics.\n",
        "all_metrics, all_rewards, all_discounts, all_returns = (\n",
        "    utils.gather_from_devices(\n",
        "        (all_metrics, all_rewards, all_discounts, all_returns)\n",
        "    )\n",
        ")\n",
        "(all_metrics,) = jax.tree.map(lambda x: x.mean(0), (all_metrics,))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "80kt3EUbIps0"
      },
      "cell_type": "code",
      "source": [
        "# @title Process logs\n",
        "all_returns = np.array(all_returns)\n",
        "all_discounts = np.array(all_discounts)\n",
        "all_steps = np.array(all_steps)\n",
        "total_returns = (all_returns * (1 - all_discounts)).sum(axis=(1, 2))\n",
        "total_episodes = (1 - all_discounts).sum(axis=(1, 2))\n",
        "avg_returns = total_returns / total_episodes\n",
        "\n",
        "padded_metrics = {}\n",
        "pad_width = len(all_steps) - len(all_metrics)\n",
        "for key in all_metrics[0].keys():\n",
        "  values = np.array([m[key] for m in all_metrics])\n",
        "  padded_metrics[key] = np.pad(values, (pad_width, 0), constant_values=np.nan)\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    dict(\n",
        "        steps=all_steps,\n",
        "        avg_returns=avg_returns,\n",
        "        **padded_metrics,\n",
        "    )\n",
        ")\n",
        "\n",
        "df['name'] = agent_settings.update_rule_name"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "se4UE1W93uRt"
      },
      "cell_type": "code",
      "source": [
        "sns.lineplot(data=df, x='steps', y='avg_returns')"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
